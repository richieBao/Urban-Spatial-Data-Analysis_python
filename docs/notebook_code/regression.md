> Created on Mon Dec 18 11/43/44 2017  @author: Richie Bao-caDesignè®¾è®¡(cadesign.cn) __+updated on Sun Jul 19 15/12/45 2020 by Richie Bao

## 1. ç®€å•å›å½’ï¼Œå¤šå…ƒå›å½’
å›å½’æ¶‰åŠçš„å†…å®¹å¼‚å¸¸ä¸°å¯Œï¼Œå›å½’çš„ç±»å‹ä¸°å¯Œå¤šå½©ï¼Œåœ¨å®é™…åº”ç”¨å›å½’æ¥é¢„æµ‹å› å˜é‡æ—¶ï¼Œå¹¶ä¸ä¼šä»æœ€åŸºç¡€çš„å¼€å§‹ä¸€æ­¥æ­¥è®¡ç®—ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨pythonä¸­é›†æˆçš„åº“ï¼Œè¿™æ—¶å°±æ¶‰åŠåˆ°äº†è‘—åçš„æœºå™¨å­¦ä¹ åº“[scikit-learn](https://scikit-learn.org/stable/index.html)ï¼Œè¯¥åº“åŒ…å«äº†æ•°æ®çš„é¢„å¤„ç†ã€é™ç»´ã€åˆ†ç±»æ¨¡å‹ã€èšç±»æ¨¡å‹å’Œå›å½’æ¨¡å‹ï¼Œä»¥åŠæ¨¡å‹é€‰æ‹©ç­‰å†…å®¹ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬å¤„ç†ä¼—å¤šåŸå¸‚ç©ºé—´æ•°æ®åˆ†æé—®é¢˜ã€‚åŒæ—¶ä¹Ÿå¯ä»¥ä½¿ç”¨[statsmodels](https://www.statsmodels.org/stable/index.html)åº“ç­‰ã€‚å½“ç„¶ï¼Œä¸€å¼€å§‹å°±ä½¿ç”¨é›†æˆçš„æ¨¡å‹ï¼Œå¯¹äºæˆ‘ä»¬ç†è§£å›å½’ï¼Œä¹ƒè‡³ä»»ä½•ç»Ÿè®¡å­¦çš„çŸ¥è¯†ç‚¹æˆ–è€…æ•°æ®åˆ†æçš„ç®—æ³•éƒ½æ˜¯ä¸åˆ©çš„ï¼Œå¾€å¾€é€ æˆå›«å›µåæ£ã€ä¸€çŸ¥åŠè§£çš„å¼Šç—…ï¼Œå› æ­¤äº¦ç„¶ä¸€æ­¥æ­¥ï¼Œä»æœ€ä¸ºåŸºç¡€çš„éƒ¨åˆ†ï¼Œå€ŸåŠ©pythonè¯­è¨€æ¥æŠŠè¿™ä¸ªè„‰ç»œæ¢³ç†æ¸…æ¥šã€‚

> è¯¥éƒ¨åˆ†å‚è€ƒæ–‡çŒ®ï¼š
> 1. [æ—¥]é«˜æ¡¥ ä¿¡è‘—ä½œ,Inoue Iroha,æ ªå¼ä¼šç¤¾ TREND-PROæ¼«ç”»åˆ¶ä½œ,å¼ ä»²æ’è¯‘.æ¼«ç”»ç»Ÿè®¡å­¦ä¹‹å›å½’åˆ†æ[M].ç§‘å­¦å‡ºç‰ˆç¤¾.åŒ—äº¬.2009.08ï¼›
> 2. Timothy C.Urdan.Statistics in Plain English(ç™½è¯ç»Ÿè®¡å­¦)[M].ä¸­å›½äººæ°‘å¤§å­¦å‡ºç‰ˆç¤¾.2013,12.ç¬¬3ç‰ˆ.
> 3. Douglas C.Montgomery,Elizabeth A.Peck,G.Geoffrey Viningè‘—.ç‹è¾°å‹‡è¯‘.çº¿æ€§å›å½’åˆ†æå¯¼è®º(Introduction to linear regression analysis).æœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾.2016.04(ç¬¬5ç‰ˆ)

### 1.1 é¢„å¤‡çš„çŸ¥è¯†â€”â€”åå‡½æ•°

#### 1.1.1 åå‡½æ•°ï¼ˆinverse functionï¼‰
å¦‚æœå‡½æ•°$f(x)$æœ‰$y=f(x)$ï¼Œ$f$æ˜¯yå…³äºè‡ªå˜é‡xçš„å‡½æ•°ï¼›å¦‚æœå­˜åœ¨ä¸€ä¸ªå‡½æ•°$g$ï¼Œä½¿å¾—ï¼Œ$g(y)=g(f(x))=x$ï¼Œåˆ™$g$æ˜¯xå…³äºè‡ªå˜é‡yçš„å‡½æ•°ï¼Œç§°$g$ä¸º$f$çš„åå‡½æ•°ã€‚è‹¥ä¸€å‡½æ•°æœ‰åå‡½æ•°ï¼Œæ­¤å‡½æ•°å˜ä¸ºå¯é€†çš„ã€‚å¯è®°ä½œå‡½æ•°$f$å’Œå®ƒçš„åå‡½æ•°$f^{-1} $ã€‚å‡è®¾å‡½æ•°$f=2*x+1$ï¼Œåˆ™$x=(f/2-1/2)$ï¼Œæ›¿æ¢$x$ä¸º$f^{-1}$ï¼Œ$f$ä¸º$x$ï¼Œåˆ™ç»“æœä¸ºåå‡½æ•°$f^{-1}=x/2-1/2$ã€‚åœ¨ä¸‹è¿°ä»£ç è¡¨è¿°å‡è®¾å‡½æ•°åŠå…¶åå‡½æ•°æ—¶ï¼Œä½¿ç”¨äº†[sympy](https://docs.sympy.org/latest/index.html)ä»£æ•°è®¡ç®—åº“ï¼Œè½»é‡å‹çš„SymPyçš„è¯­æ³•æ–¹å¼ä¿æŒäº†pythonè¯­è¨€æœ¬èº«çš„å½¢å¼ï¼Œä½¿å¾—æ•°å­¦å…¬ç¤ºçš„è¡¨è¿°å’Œè®¡ç®—ä¸Šæ¸…æ™°ç®€ä¾¿ã€‚æ‰“å°å›¾è¡¨$f$å’Œå®ƒçš„åå‡½æ•°$f^{-1} $ï¼Œå¯ä»¥çœ‹åˆ°ï¼ŒäºŒè€…çš„ç›¸å¯¹æ¨ªçºµåæ ‡å‘ç”Ÿäº†ç½®æ¢ã€‚æ±‚ä¸€ä¸ªå‡½æ•°çš„åå‡½æ•°ï¼Œå½“å‰æœ‰å¾ˆå¤šåœ¨çº¿è‡ªåŠ¨è½¬æ¢çš„å¹³å°ï¼Œå¯ä»¥æœç´¢è¾“å…¥å…¬å¼è·å¾—å…¶åå‡½æ•°çš„å…¬å¼ã€‚ä¾‹å¦‚[Symbolab](https://www.symbolab.com/)ï¼Œå¦‚æœ$f(x)= \frac{ x^{2} +x+1}{x} $,åˆ™å…¶åå‡½æ•°ä¸º$g(x)= \frac{-1+x+ \sqrt{ x^{2}-2x-3 } }{2} $ï¼ŒåŠ$g(x)= \frac{-1+x- \sqrt{ x^{2}-2x-3 } }{2} $


```python
import sympy
from sympy import init_printing,pprint,sqrt
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

init_printing() #sympyæä¾›æœ‰å¤šç§å…¬å¼æ‰“å°æ¨¡å¼

#ç¤ºä¾‹-A
# å®šä¹‰å­—ç¬¦
x=sympy.Symbol('x')

# å®šä¹‰è¡¨è¾¾å¼
f=2*x+1 #å‡½æ•°fx
g=x/2-1/2 #fxçš„åå‡½æ•°

#è½¬æ¢è¡¨è¾¾å¼ä¸ºç­‰ä»·çš„numpyå‡½æ•°å®ç°æ•°å€¼è®¡ç®—
x_array=np.arange(-5,10)
f_=sympy.lambdify(x,f,"numpy")
g_=sympy.lambdify(x,g,"numpy")

#æ±‚è§£å‡½æ•°å¹¶ç»˜åˆ¶å›¾è¡¨
fig, axs=plt.subplots(1,2,figsize=(16,8))
axs[0].plot(x_array,f_(x_array),'+--',color='tab:blue',label='$f=2*x+1$')
axs[0].plot(f_(x_array),g_(f_(x_array)),'o--',color='tab:red',label='$f^{-1}=x/2-1/2$')

axs[0].set_title('$fx=2*x+1$')
axs[0].legend(loc='upper left', frameon=False)
axs[0].hlines(y=0,xmin=-5,xmax=5,lw=2,color='gray')
axs[0].vlines(x=0,ymin=-5,ymax=5,lw=2,color='gray')


#ç¤ºä¾‹-B
f_B=(x**2+x+1)/x
print("ä½¿ç”¨pprintæ–¹å¼æ‰“å°å…¬å¼ï¼š")
pprint(f_B,use_unicode=True) #ä½¿ç”¨pprintæ–¹å¼æ‰“å°å…¬å¼
g_B_negative=(-1+x+sqrt(x**2-2*x-3))/2
g_B_positive=(-1+x-sqrt(x**2-2*x-3))/2

f_B_=sympy.lambdify(x,f_B,"numpy")
g_B_positive_=sympy.lambdify(x,g_B_positive,"numpy")
g_B_negative_=sympy.lambdify(x,g_B_negative,"numpy")

x_B_array=np.arange(-10,21)
x_B_array_positive=x_B_array[x_B_array>0]
axs[1].plot(x_B_array_positive,f_B_(x_B_array_positive),'+',color='tab:blue',label='$+:f(x)$')
axs[1].plot(g_B_positive_(f_B_(x_B_array_positive)),f_B_(x_B_array_positive),'o',color='tab:red',label='$+:g(x)$')

x_B_array_negative=x_B_array[x_B_array<0]
axs[1].plot(x_B_array_negative,f_B_(x_B_array_negative),'+--',color='tab:blue',label='$-:f(x)$')
axs[1].plot(g_B_negative_(f_B_(x_B_array_negative)),f_B_(x_B_array_negative),'o--',color='tab:red',label='$-:g(x)$')

axs[1].hlines(y=0,xmin=-5,xmax=5,lw=2,color='gray')
axs[1].vlines(x=0,ymin=-5,ymax=5,lw=2,color='gray')
axs[1].legend(loc='upper left', frameon=False)
axs[1].set_title('$fx=(x**2+x+1)/x$')

plt.show()
print("JupyterLabç›´æ¥è¾“å‡ºå…¬å¼:g_B_negative=")
g_B_negative #ç”¨JupyterLabç›´æ¥è¾“å‡ºå…¬å¼
```

    ä½¿ç”¨pprintæ–¹å¼æ‰“å°å…¬å¼ï¼š
     2        
    x  + x + 1
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        x     
    


<a href=""><img src="./imgs/7_1.png" height="auto" width="auto" title="caDesign"></a>


    JupyterLabç›´æ¥è¾“å‡ºå…¬å¼:g_B_negative=
    




$\displaystyle \frac{x}{2} + \frac{\sqrt{x^{2} - 2 x - 3}}{2} - \frac{1}{2}$



#### 1.1.2 æŒ‡æ•°å‡½æ•°ä¸è‡ªç„¶å¯¹æ•°å‡½æ•°
æŒ‡æ•°å‡½æ•°(Exponential function)æ˜¯å½¢å¼ä¸º$b^{x} $çš„æ•°å­¦å‡½æ•°ï¼Œå…¶ä¸­$b$æ˜¯åº•æ•°ï¼ˆæˆ–ç§°åŸºæ•°ï¼Œbaseï¼‰,è€Œ$x$æ˜¯æŒ‡æ•°ï¼ˆindex/exponentï¼‰ã€‚

å¯¹æ•°ï¼ˆlogarithmï¼‰æ˜¯å¹‚è¿ç®—çš„é€†è¿ç®—ï¼Œå‡å¦‚$x=b^{y} $ï¼Œåˆ™æœ‰$y=log_{b}x $ï¼Œå…¶ä¸­$b$æ˜¯å¯¹æ•°çš„åº•ï¼ˆæˆ–ç§°åŸºæ•°ï¼‰ï¼Œè€Œ$y$å°±æ˜¯$x$å¯¹äºåº•æ•°$b$ï¼Œ$x$çš„å¯¹æ•°ã€‚å…¸å‹çš„åº•æ•°æœ‰$e$ã€10æˆ–2

è‡ªç„¶å¯¹æ•°ï¼ˆNatural logarithmï¼‰ä¸ºä»¥æ•°å­¦å¸¸æ•°$e$ä¸ºåº•æ•°çš„å¯¹æ•°å‡½æ•°ï¼Œæ ‡è®°ä¸º$lnx$æˆ–$log_{e}x $ï¼Œå…¶åå‡½æ•°ä¸ºæŒ‡æ•°å‡½æ•°$e^{x}$ã€‚

* æŒ‡æ•°å‡½æ•°ä¸å¯¹æ•°å‡½æ•°çš„æ€§è´¨ï¼š

1. $( e^{a} )^{b} = e^{ab} $
2. $\frac{ e^{a} }{ e^{b} } = e^{a-b} $
3. $a=log( e^{a} )$
4. $log(a^{b} )=b \times (loga)$
5. $loga+logb=log(a \times b)$


```python
import math
from sympy import ln,log,Eq
x=sympy.Symbol('x')
f_exp=2**x
f_exp_=sympy.lambdify(x,f_exp,"numpy")

fig, axs=plt.subplots(1,3,figsize=(25,8))
exp_x=np.arange(-10,10,0.5,dtype=float)
axs[0].plot(exp_x,f_exp_(exp_x),label="f(x)=2**x")
axs[0].legend(loc='upper left', frameon=False)

f_exp_e=math.e**x
f_exp_e_=sympy.lambdify(x,f_exp_e,"numpy")
axs[1].plot(exp_x,f_exp_e_(exp_x),label="f(x)=e**x",color='r')
axs[1].legend(loc='upper left', frameon=False)

log_x=np.arange(1,20,dtype=float)
f_ln=ln(x) 
f_ln_=sympy.lambdify(x,f_ln,"numpy")
axs[2].plot(log_x,f_ln_(log_x),label='base=e')

f_log_2=log(x,2)
f_log_2_=sympy.lambdify(x,f_log_2,"numpy")
axs[2].plot(log_x,f_log_2_(log_x),label="base=2")

f_log_10=log(x,10)
f_log_10_=sympy.lambdify(x,f_log_10,"numpy")
axs[2].plot(log_x,f_log_10_(log_x),label="base=10")
axs[2].legend(loc='upper left', frameon=False)

plt.show()
```


<a href=""><img src="./imgs/7_2.png" height="auto" width="auto" title="caDesign"></a>


#### 1.1.3 å¾®åˆ†
å¾®åˆ†æ˜¯å¯¹å‡½æ•°çš„å±€éƒ¨å˜åŒ–ç‡çš„ä¸€ç§çº¿æ€§æè¿°ã€‚å…¶å¯ä»¥è¿‘ä¼¼çš„æè¿°å½“å‡½æ•°è‡ªå˜é‡çš„å–å€¼è¶³å¤Ÿå°æ—¶çš„æ”¹å˜æ—¶ï¼Œå‡½æ•°çš„å€¼æ˜¯æ€æ ·å˜åŒ–çš„ã€‚é¦–å…ˆæ ¹æ®â€˜æ¼«ç”»ç»Ÿè®¡å­¦ä¹‹å›å½’åˆ†æâ€™ä¸­ç¾ç¾½çš„å¹´é¾„å’Œèº«é«˜æ•°æ®å»ºç«‹æ•°æ®é›†ï¼Œå®ç°è®¡ç®—å¹´é¾„å’Œèº«é«˜çš„ç›¸å…³ç³»æ•°ï¼Œç»“æœp_value<0.05ï¼Œå³pearson's r=0.942çš„ç›¸å…³ç³»æ•°èƒ½å¤Ÿè¯´æ˜å¹´é¾„å’Œèº«é«˜ç›´æ¥å­˜åœ¨å¼ºç›¸å…³å…³ç³»ã€‚æ—¢ç„¶äºŒè€…ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œå°±å¯ä»¥å»ºç«‹å›å½’æ–¹ç¨‹ï¼Œåœ¨ä¸‹è¿°ä»£ç ä¸­ç»™å‡ºäº†ä¸‰ç§å›å½’æ¨¡å‹ï¼ˆæ–¹ç¨‹ï¼‰ï¼Œä¸€ç§æ˜¯â€˜æ¼«ç”»ç»Ÿè®¡å­¦ä¹‹å›å½’åˆ†æâ€™ç»™å‡ºçš„$f(x)=- \frac{326.6}{x}+173.3 $æ–¹ç¨‹ï¼Œå¦å¤–ä¸¤ç§æ˜¯ç›´æ¥ä½¿ç”¨sklearnåº“Linear Modelsçº¿æ€§æ¨¡å‹ä¸­çš„LinearRegressionçº¿æ€§å›å½’ï¼Œå’ŒåŸºäºLinearRegressionçš„Polynomial regressionå¤šé¡¹å¼å›å½’ã€‚å…³äºSklearnçš„è¯­æ³•è§„åˆ™ï¼Œå¯ä»¥å‚è€ƒå®˜æ–¹ç½‘ç«™scikit-learnç»™å‡ºçš„æŒ‡å—ï¼ŒSklearnçš„è¯­æ³•ç»“æ„ç§‰æ‰¿äº†pythonè‡ªèº«çš„ç‰¹ç‚¹ï¼Œå…·æœ‰å¾ˆå¼ºçš„æ˜“è¯»æ€§ï¼Œä»£ç ç¼–å†™æµç•…è‡ªç„¶ã€‚ä¸‰ç§å›å½’æ¨¡å‹ä¸­ï¼Œä»¥å¤šé¡¹å¼å›å½’æ‹Ÿåˆçš„æœ€å¥½ï¼Œâ€˜æ¼«ç”»ç»Ÿè®¡å­¦ä¹‹å›å½’åˆ†æâ€™ä¸­ç»™å‡ºçš„å…¬å¼æ¬¡ä¹‹ï¼Œè€Œç®€å•ç²—æš´çš„ç®€å•çº¿æ€§å›å½’å› ä¸ºå‘ˆç°çº¿æ€§ï¼Œä¸çœŸå®å€¼è¿‘ä¼¼å¯¹æ•°å‡½æ•°æ›²çº¿çš„å½¢çŠ¶ç›¸å¼‚ã€‚

$y=- \frac{326.6}{x}+173.3 $å…³äºxæ±‚å¾®åˆ†ï¼Œå³æ˜¯æ±‚$x$å²åˆ°$x$å²ä¹‹åæçŸ­æ—¶é—´å†…ï¼Œèº«é«˜çš„å¹´å¹³å‡å¢é•¿é‡ï¼ˆè‡ªå˜é‡ä»¥å¹´ä¸ºå•ä½ï¼‰ï¼Œ$\frac{(- \frac{326.6}{x+ \triangle }+173.3 )-(- \frac{326.6}{x} +173.3)}{ \triangle } = \frac{326.6}{ x^{2} } $ï¼Œå¯¹äºå¾®åˆ†çš„è®¡ç®—ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨sympyæä¾›çš„`diff`å·¥å…·è®¡ç®—ï¼Œè®¡ç®—ç»“æœè®°ä½œ$\frac{dy}{dx}= \frac{df}{dx}  = y'= f' = \frac{326.6}{ x^{2} }  $ã€‚

* å¸¸ç”¨å…¬å¼æ±‚å¾®åˆ†ï¼š

1. $y=x$ï¼Œå…³äº$x$è¿›è¡Œå¾®åˆ†ï¼Œ $\frac{dy}{dx}=1$
2. $y= x^{2} $ï¼Œå…³äº$x$è¿›è¡Œå¾®åˆ†ï¼Œ$\frac{dy}{dx}=2x$
3. $y= \frac{1}{x} $ï¼Œå…³äº$x$è¿›è¡Œå¾®åˆ†ï¼Œ$\frac{dy}{dx}=- x^{-2} $
4. $y= \frac{1}{ x^{2} } $ï¼Œå…³äº$x$è¿›è¡Œå¾®åˆ†ï¼Œ$\frac{dy}{dx}=- 2x^{-3} $
5. $y= (5x-7)^{2} $ï¼Œå…³äº$x$è¿›è¡Œå¾®åˆ†ï¼Œ$\frac{dy}{dx}=2(5x-7)\times 5 $
6. $y= (ax+b)^{n} $ï¼Œå…³äº$x$è¿›è¡Œå¾®åˆ†ï¼Œ$\frac{dy}{dx}=n (ax+b)^{n-1} \times a  $
7. $y= e^{x} $ï¼Œå…³äº$x$è¿›è¡Œå¾®åˆ†ï¼Œ$\frac{dy}{dx}=e^{x}$
8. $y=logx$ï¼Œå…³äº$x$è¿›è¡Œå¾®åˆ†ï¼Œ$\frac{dy}{dx}=\frac{1}{x} $
9. $y=log(ax+b)$ï¼Œå…³äº$x$è¿›è¡Œå¾®åˆ†ï¼Œ$\frac{dy}{dx}=\frac{1}{ax+b}  \times a $ 
10. $y=log(1+ ea^{x+b} )$ï¼Œå…³äº$x$è¿›è¡Œå¾®åˆ†ï¼Œ$\frac{dy}{dx}=\frac{1}{1+ e^{ax+b} }  \times a e^{ax+b}  $

åœ¨ä»£ç çš„é¢†åŸŸé‡Œç›´æ¥ç”¨sympyçš„diffæ–¹æ³•ï¼Œæˆ–å…¶å®ƒåº“æä¾›çš„æ–¹æ³•è®¡ç®—ã€‚


```python
import pandas as pd
from scipy import stats
emma_statureAge={"age":list(range(4,20)),"stature":[100.1,107.2,114.1,121.7,126.8,130.9,137.5,143.2,149.4,151.6,154.0,154.6,155.0,155.1,155.3,155.7]}
emma_statureAge_df=pd.DataFrame(emma_statureAge)

r_=stats.pearsonr(emma_statureAge_df.age,emma_statureAge_df.stature)
print(
    "pearson's r:",r_[0],"\n",
    "p_value:",r_[1]
     )

#åŸå§‹æ•°æ®æ•£ç‚¹å›¾
fig, axs=plt.subplots(1,3,figsize=(25,8))
axs[0].plot(emma_statureAge_df.age,emma_statureAge_df.stature,'o',label='ground truth',color='r')

#A - ä½¿ç”¨sklearnåº“sklearn.linear_model.LinearRegression()ï¼ŒOrdinary least squares Linear Regression-æ™®é€šæœ€å°äºŒä¹˜çº¿æ€§å›å½’ï¼Œè·å–å›å½’æ–¹ç¨‹
from sklearn.linear_model import LinearRegression
X=emma_statureAge_df.age.to_numpy().reshape(-1,1)
y=emma_statureAge_df.stature.to_numpy()

#æ‹Ÿåˆæ¨¡å‹
LR=LinearRegression().fit(X,y)

#æ¨¡å‹å‚æ•°
print("slop:%.2f,intercept:%.2f"%(LR.coef_, LR.intercept_))
print(LR.get_params())

#æ¨¡å‹é¢„æµ‹
axs[0].plot(emma_statureAge_df.age,LR.predict(X),'o-',label='linear regression')

#B - å¤šé¡¹å¼å›å½’ Polynomial regression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
model=Pipeline([('poly', PolynomialFeatures(degree=2)),
                ('linear', LinearRegression(fit_intercept=False))])
reg=model.fit(X,y)
axs[0].plot(emma_statureAge_df.age,reg.predict(X),'+-',label='polynomial regression')

#C - ä½¿ç”¨'æ¼«ç”»ç»Ÿè®¡å­¦ä¹‹å›å½’åˆ†æ'ç»™å‡ºçš„å…¬å¼
from sympy import Symbol
x=Symbol('x')
f_emma=-326.6/x+173.3
f_emma_=sympy.lambdify(x,f_emma,"numpy")
axs[0].plot(emma_statureAge_df.age,f_emma_(emma_statureAge_df.age),'o-',label='$-326.6/x+173.3$')


#
axs[1].plot(emma_statureAge_df.age,emma_statureAge_df.stature,'o',label='ground truth',color='r')
axs[1].plot(emma_statureAge_df.age,f_emma_(emma_statureAge_df.age),'o-',label='$-326.6/x+173.3$')

def demo_con_style(a_coordi,b_coordi,ax,connectionstyle):
    '''
    function - åœ¨matplotlibçš„å­å›¾ä¸­ç»˜åˆ¶è¿æ¥çº¿
    reference - matplotlibå®˜ç½‘Connectionstyle Demo
    
    Paras:
    a_coordi - aç‚¹çš„xï¼Œyåæ ‡
    b_coordi - bç‚¹çš„xï¼Œyåæ ‡
    ax - å­å›¾
    connectionstyle - è¿æ¥çº¿çš„å½¢å¼
    '''
    x1, y1=a_coordi[0],a_coordi[1]
    x2, y2=b_coordi[0],b_coordi[1]

    ax.plot([x1, x2], [y1, y2], ".")
    ax.annotate("",
                xy=(x1, y1), xycoords='data',
                xytext=(x2, y2), textcoords='data',
                arrowprops=dict(arrowstyle="->", color="0.5",
                                shrinkA=5, shrinkB=5,
                                patchA=None, patchB=None,
                                connectionstyle=connectionstyle,
                                ),
                )

    ax.text(.05, .95, connectionstyle.replace(",", ",\n"),
            transform=ax.transAxes, ha="left", va="top")

dx=3
demo_con_style((6,f_emma.evalf(subs={x:6})),(6+dx,f_emma.evalf(subs={x:6+dx})),axs[1],"angle,angleA=-90,angleB=180,rad=0")    
axs[1].text(7, f_emma.evalf(subs={x:6})-3, "â–³ x", family="monospace",size=20)
axs[1].text(9.3, f_emma.evalf(subs={x:9.3})-10, "â–³ y", family="monospace",size=20)

#ç”¨sympyæä¾›çš„diffæ–¹æ³•æ±‚å¾®åˆ†
from sympy import diff
print("f_emma=-326.6/x+173.3å…³äºxæ±‚å¾®åˆ†ï¼š")
pprint(diff(f_emma),use_unicode=True) 
diff_f_emma_=sympy.lambdify(x,diff(f_emma),"numpy")
axs[2].plot(emma_statureAge_df.age,diff_f_emma_(emma_statureAge_df.age),'+--',label='annual growth',color='r')

axs[2].legend(loc='upper right', frameon=False)
axs[1].legend(loc='lower right', frameon=False)
axs[0].legend(loc='upper left', frameon=False)
plt.show()
```

    pearson's r: 0.9422225583501309 
     p_value: 4.943118398567093e-08
    slop:3.78,intercept:94.82
    {'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'normalize': False}
    f_emma=-326.6/x+173.3å…³äºxæ±‚å¾®åˆ†ï¼š
    326.6
    â”€â”€â”€â”€â”€
       2 
      x  
    


<a href=""><img src="./imgs/7_3.png" height="auto" width="auto" title="caDesign"></a>


#### 1.1.4 çŸ©é˜µ
ä¸€ä¸ª$m \times n$çš„çŸ©é˜µæ˜¯ä¸€ä¸ªç”±$m$è¡Œï¼ˆrowï¼‰$n$åˆ—ï¼ˆcolumnï¼‰å…ƒç´ æ’åˆ—æˆçš„çŸ©å½¢é˜µåˆ—ã€‚çŸ©é˜µé‡Œçš„å…ƒç´ å¯ä»¥æ˜¯æ•°å­—ã€ç¬¦å·æˆ–æ•°å­¦å¼ã€‚ä¾‹å¦‚ï¼š$\begin{bmatrix}1 & 9&-13 \\20 & 5 &-6\end{bmatrix} $ï¼Œå¦‚æœ$\begin{cases} x_{1}+2 x_{2}=-1  \\3 x_{1}+ 4x_{2}=5  \end{cases} $å¯ä»¥å†™ä½œï¼š$\begin{bmatrix}1 & 2 \\3 & 4 \end{bmatrix}  \begin{bmatrix} x_{1}  \\ x_{2} \end{bmatrix} = \begin{bmatrix}-1 \\5 \end{bmatrix} $ï¼Œè€Œå¦‚æœ$\begin{cases} x_{1}+2 x_{2} \\3 x_{1}+ 4x_{2} \end{cases}$ï¼Œå¯ä»¥å†™ä½œï¼š$\begin{bmatrix}1 & 2 \\3 & 4 \end{bmatrix}  \begin{bmatrix} x_{1}  \\ x_{2} \end{bmatrix} $ ã€‚

çŸ©é˜µçš„æ“ä½œå’Œè¿ç®—å¯ä»¥ç›´æ¥åº”ç”¨sympyåº“çš„Matriceséƒ¨åˆ†æ–¹æ³•ï¼Œæˆ–è€…å…¶å®ƒåº“ã€‚æ›´å¤šçš„å†…å®¹éœ€è¦å‚è€ƒå®˜æ–¹æ•™ç¨‹ï¼Œæ­¤å¤„ä¸å†èµ˜è¿°ã€‚


```python
from sympy import Matrix,init_printing,pprint
init_printing()
M_a=Matrix([[1, -1], [3, 4], [0, 2]])
pprint(M_a)
```

    â¡1  -1â¤
    â¢     â¥
    â¢3  4 â¥
    â¢     â¥
    â£0  2 â¦
    

### 1.2 ç®€å•çº¿æ€§å›å½’
åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œçº¿æ€§å›å½’ï¼ˆlinear regressionï¼‰æ˜¯åˆ©ç”¨ç§°ä¸ºçº¿æ€§å›å½’æ–¹ç¨‹çš„æœ€å°å¹³æ–¹å‡½æ•°å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡å’Œå› å˜é‡ä¹‹é—´å…³ç³»è¿›è¡Œå»ºæ¨¡çš„ä¸€ç§å›å½’åˆ†æã€‚è¿™ç§å‡½æ•°æ˜¯ä¸€ä¸ªæˆ–å¤šä¸ªç§°ä¸ºå›å½’ç³»æ•°çš„æ¨¡å‹å‚æ•°çš„çº¿æ€§ç»„åˆã€‚åªæœ‰ä¸€ä¸ªè‡ªå˜é‡çš„æƒ…å†µç§°ä¸ºç®€å•ï¼ˆçº¿æ€§ï¼‰å›å½’ï¼ˆsimple linear regressionï¼‰ï¼Œå¤§äºä¸€ä¸ªè‡ªå˜é‡æƒ…å†µçš„å«å¤šå…ƒå›å½’ï¼ˆmultivariable linear regressionï¼‰ã€‚

* å›å½’åˆ†æçš„æµç¨‹:

1. ä¸ºäº†è®¨è®ºååˆ†å…·æœ‰æ±‚è§£å›å½’æ–¹ç¨‹çš„æ„ä¹‰ï¼Œç”»å‡ºè‡ªå˜é‡å’Œå› å˜é‡çš„æ•£ç‚¹å›¾ï¼›
2. æ±‚è§£å›å½’æ–¹ç¨‹ï¼›
3. ç¡®è®¤å›å½’æ–¹ç¨‹çš„ç²¾åº¦ï¼›
4. è¿›è¡Œå›å½’ç³»æ•°çš„æ£€éªŒï¼›
5. æ€»ä½“å›å½’$Ax+b$çš„ä¼°è®¡ï¼›
6. è¿›è¡Œé¢„æµ‹

#### 1.2.1 å»ºç«‹æ•°æ®é›†
ä½¿ç”¨'æ¼«ç”»ç»Ÿè®¡å­¦ä¹‹å›å½’åˆ†æ'ä¸­æœ€é«˜æ¸©åº¦ï¼ˆ$^{\circ}C$ï¼‰ä¸å†°çº¢èŒ¶é”€å”®é‡(æ¯)çš„æ•°æ®ï¼Œé¦–å…ˆå»ºç«‹åŸºäºDataFrameæ ¼å¼çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä½¿ç”¨äº†æ—¶é—´æˆ³ï¼ˆtimestampï¼‰ä½œä¸ºç´¢å¼•ã€‚


```python
import pandas as pd
import util
from scipy import stats

dt=pd.date_range('2020-07-22', periods=14, freq='D')
dt_temperature_iceTeaSales={"dt":dt,"temperature":[29,28,34,31,25,29,32,31,24,33,25,31,26,30],"iceTeaSales":[77,62,93,84,59,64,80,75,58,91,51,73,65,84]}
iceTea_df=pd.DataFrame(dt_temperature_iceTeaSales).set_index("dt")
util.print_html(iceTea_df,14)
```




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temperature</th>
      <th>iceTeaSales</th>
    </tr>
    <tr>
      <th>dt</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-07-22</th>
      <td>29</td>
      <td>77</td>
    </tr>
    <tr>
      <th>2020-07-23</th>
      <td>28</td>
      <td>62</td>
    </tr>
    <tr>
      <th>2020-07-24</th>
      <td>34</td>
      <td>93</td>
    </tr>
    <tr>
      <th>2020-07-25</th>
      <td>31</td>
      <td>84</td>
    </tr>
    <tr>
      <th>2020-07-26</th>
      <td>25</td>
      <td>59</td>
    </tr>
    <tr>
      <th>2020-07-27</th>
      <td>29</td>
      <td>64</td>
    </tr>
    <tr>
      <th>2020-07-28</th>
      <td>32</td>
      <td>80</td>
    </tr>
    <tr>
      <th>2020-07-29</th>
      <td>31</td>
      <td>75</td>
    </tr>
    <tr>
      <th>2020-07-30</th>
      <td>24</td>
      <td>58</td>
    </tr>
    <tr>
      <th>2020-07-31</th>
      <td>33</td>
      <td>91</td>
    </tr>
    <tr>
      <th>2020-08-01</th>
      <td>25</td>
      <td>51</td>
    </tr>
    <tr>
      <th>2020-08-02</th>
      <td>31</td>
      <td>73</td>
    </tr>
    <tr>
      <th>2020-08-03</th>
      <td>26</td>
      <td>65</td>
    </tr>
    <tr>
      <th>2020-08-04</th>
      <td>30</td>
      <td>84</td>
    </tr>
  </tbody>
</table>



#### 1.2.2æ±‚è§£å›å½’æ–¹ç¨‹
æ±‚è§£å›å½’æ–¹ç¨‹ä½¿ç”¨äº†ä¸¤ç§æ–¹æ³•ï¼Œä¸€ç§æ˜¯é€æ­¥è®¡ç®—çš„æ–¹å¼ï¼›å¦ä¸€ç§æ˜¯ç›´æ¥ä½¿ç”¨sklearnåº“çš„LinearRegressionæ¨¡å‹ã€‚é€æ­¥è®¡ç®—çš„æ–¹å¼å¯ä»¥æ›´ä¸ºæ·±å…¥çš„ç†è§£å›å½’æ¨¡å‹ï¼Œè€Œç†Ÿæ‚‰åŸºæœ¬è®¡ç®—è¿‡ç¨‹ä¹‹åï¼Œç›´æ¥åº”ç”¨sklearnæœºå™¨å­¦ä¹ åº“ä¸­çš„æ¨¡å‹ä¹Ÿä¼šå¯¹å„ç§å‚æ•°çš„é…ç½®æœ‰ä¸ªæ¯”è¾ƒæ¸…æ™°çš„äº†è§£ã€‚é¦–å…ˆè®¡ç®—æ¸©åº¦ä¸é”€é‡ä¹‹é—´çš„ç›¸å…³ç³»æ•°ï¼Œç¡®å®šäºŒè€…ä¹‹é—´å­˜åœ¨å…³è”ï¼Œå…¶p_value=7.661412804450245e-06ï¼Œå°äº0.05çš„æ˜¾è‘—æ€§æ°´å¹³ï¼Œç¡®å®špearson's r=0.90èƒ½å¤Ÿè¡¨æ˜äºŒè€…ä¹‹é—´æ˜¯å¼ºç›¸å…³æ€§ã€‚ 

æ±‚è§£å›å½’æ–¹ç¨‹å³æ˜¯ä½¿æ‰€æœ‰çœŸå®å€¼ä¸é¢„æµ‹å€¼ä¹‹å·®çš„å’Œä¸ºæœ€å°ï¼Œæ±‚å‡ºaå’Œbï¼Œå°±æ˜¯æ‰€æœ‰å˜é‡æ®‹å·®`residual`çš„å¹³æ–¹`s_residual`çš„å’Œ`S_residual`ä¸ºæœ€å°ã€‚å› ä¸ºæ¸©åº¦ä¸é”€é‡ä¸ºçº¿æ€§ç›¸å…³ï¼Œå› æ­¤ä½¿ç”¨ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹å¼ï¼š$y=ax+b$ï¼Œ$x$ä¸ºè‡ªå˜é‡æ¸©åº¦ï¼Œ$y$ä¸ºå› å˜é‡é”€é‡ï¼Œ$a$å’Œ$b$ä¸ºå›å½’ç³»æ•°ï¼ˆå‚æ•°ï¼‰ï¼Œåˆ†åˆ«ç§°ä¸ºæ–œç‡ï¼ˆslopï¼‰å’Œæˆªè·(intercept)ï¼Œæ±‚è§£aå’Œbçš„è¿‡ç¨‹ï¼Œå¯ä»¥ä½¿ç”¨æœ€å°äºŒä¹˜æ³•ï¼ˆleast squares methodï¼‰ï¼Œåˆç§°æœ€å°å¹³æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–è¯¯å·®çš„å¹³æ–¹ï¼ˆæ®‹å·®å¹³æ–¹å’Œï¼‰å¯»æ‰¾æ•°æ®çš„æœ€ä½³å‡½æ•°åŒ¹é…ã€‚ä¸ºæ®‹å·®å¹³æ–¹å’Œï¼š$(âˆ’34ğ‘âˆ’ğ‘+93)^{2} +(âˆ’33ğ‘âˆ’ğ‘+91)^{2}+(âˆ’32ğ‘âˆ’ğ‘+80)^{2}+(âˆ’31ğ‘âˆ’ğ‘+73)^{2}+(âˆ’31ğ‘âˆ’ğ‘+75)^{2}+(âˆ’31ğ‘âˆ’ğ‘+84)^{2}+(âˆ’30ğ‘âˆ’ğ‘+84)^{2}+(âˆ’29ğ‘âˆ’ğ‘+64)^{2}+(âˆ’29ğ‘âˆ’ğ‘+77)^{2}+(âˆ’28ğ‘âˆ’ğ‘+62)^{2}+(âˆ’26ğ‘âˆ’ğ‘+65)^{2}+(âˆ’25ğ‘âˆ’ğ‘+51)^{2}+(âˆ’25ğ‘âˆ’ğ‘+59)^{2}+(âˆ’24ğ‘âˆ’ğ‘+58)^{2}$ï¼Œ 

å…ˆå¯¹$a$å’Œ$b$åˆ†åˆ«æ±‚å¾®åˆ†$\frac{df}{da} $å’Œ$\frac{df}{db} $ï¼Œæ˜¯$\triangle a$å³$a$åœ¨æ¨ªè½´ä¸Šçš„å¢é‡ï¼ŒåŠ$\triangle b$å³$b$åœ¨æ¨ªè½´ä¸Šçš„å¢é‡è¶‹è¿‘äºæ— ç©·å°ï¼Œæ— é™æ¥è¿‘$a$å’Œ$b$æ—¶ï¼Œå› å˜é‡çš„å˜åŒ–é‡ï¼Œè¿™ä¸ªå› å˜é‡å°±æ˜¯æ®‹å·®å¹³æ–¹å’Œçš„å€¼ã€‚æ®‹å·®å¹³æ–¹å’Œçš„å€¼æ˜¯ç”±$a$å’Œ$b$ç¡®å®šçš„ï¼Œå½“$a$å’Œ$b$å–ä¸åŒçš„å€¼æ—¶ï¼Œæ®‹å·®å¹³æ–¹å’Œçš„å€¼éšä¹‹å˜åŒ–ï¼Œå½“æ®‹å·®å¹³æ–¹å’Œçš„å€¼ä¸º0æ—¶ï¼Œè¯´æ˜ç”±è‡ªå˜é‡æ¸©åº¦æ‰€æœ‰å€¼é€šè¿‡å›å½’æ–¹ç¨‹é¢„æµ‹çš„é”€é‡ï¼Œä¸çœŸå®å€¼çš„å·®å€¼ä¹‹å’Œä¸º0ï¼›å•ä¸ªæ¸©åº¦å€¼é€šè¿‡å›å½’æ¨¡å‹é¢„æµ‹çš„é”€é‡ä¸çœŸå®å€¼ä¹‹å·®åˆ™è¶‹äº0ã€‚åœ¨å®é™…è®¡ç®—ä¸­ï¼Œæ‰‹å·¥æ¨ç®—æ—¶ï¼Œå¯¹æ®‹å·®å¹³æ–¹å’Œå…³äº$a$å’Œ$b$æ±‚å¾®åˆ†ï¼Œæ˜¯å¯¹å…¬å¼è¿›è¡Œæ•´ç†ï¼Œæœ€ç»ˆè·å¾—æ±‚è§£å›å½’æ–¹ç¨‹å›å½’ç³»æ•°çš„å…¬å¼ä¸ºï¼š$a= \frac{ S_{xy} }{ S_{xx} } $å…¶ä¸­$S_{xy}$å³å˜é‡`SS_xy`æ˜¯$x$å’Œ$y$çš„ç¦»å·®ç§¯ï¼Œ$S_{xx}$å³å˜é‡`SS_x`æ˜¯$x$çš„ç¦»å·®å¹³æ–¹å’Œã€‚æ±‚å¾—$a$åï¼Œå¯ä»¥æ ¹æ®æ¨å¯¼å…¬å¼ï¼š$b= \overline{y} - \overline{x} a$è®¡ç®—$b$ã€‚

åœ¨pythonè¯­è¨€ä¸­ï¼Œä½¿ç”¨ç›¸å…³åº“åˆ™å¯ä»¥é¿å…ä¸Šè¿°ç¹ççš„æ‰‹å·¥æ¨å¯¼è¿‡ç¨‹ï¼Œåœ¨é€æ­¥è®¡ç®—ä¸­ï¼Œä½¿ç”¨sympyåº“çº¦ç®€æ®‹å·®å¹³æ–¹å’Œå…¬å¼ä¸ºï¼š$12020â‹…a^{2}   + 816â‹…aâ‹…b - 60188â‹…a + 14â‹…b^{2}  - 2032â‹…b + 75936$ï¼Œ å¹¶ç›´æ¥åˆ†åˆ«å¯¹$a$å’Œ$b$å¾®åˆ†ï¼Œè·å¾—ç»“æœä¸ºï¼š$ \frac{df}{da} =24040â‹…a + 816â‹…b - 60188$å’Œ$ \frac{df}{db} =816â‹…a + 28â‹…b - 2032$ï¼Œå¦äºŒè€…åˆ†åˆ«ä¸º0ï¼Œä½¿ç”¨sympyåº“çš„solveæ±‚è§£äºŒå…ƒä¸€æ¬¡æ–¹ç¨‹ç»„ï¼Œè®¡ç®—è·å–$a$å’Œ$b$å€¼ã€‚

æœ€åä½¿ç”¨sklearnåº“çš„LinearRegressionæ¨¡å‹æ±‚è§£å†³å›å½’æ¨¡å‹ï¼Œä»…éœ€è¦å‡ è¡Œä»£ç ï¼Œæ‰€å¾—ç»“æœä¸ä¸Šè¿°åŒã€‚å¯ä»¥ç”¨sklearnè¿”å›çš„å‚æ•°ï¼Œå»ºç«‹å›å½’æ–¹ç¨‹å…¬å¼ï¼Œä½†æ˜¯åœ¨å®é™…çš„åº”ç”¨ä¸­å¹¶ä¸ä¼šè¿™ä¹ˆåšï¼Œè€Œæ˜¯ç›´æ¥åº”ç”¨ä»¥å˜é‡å½¢å¼ä»£è¡¨çš„å›å½’æ¨¡å‹ç›´æ¥é¢„æµ‹å€¼ã€‚


```python
import math
import sympy
from sympy import diff,Eq,solveset,solve,simplify
import matplotlib.pyplot as plt
from matplotlib import cm
import numpy as np

r_=stats.pearsonr(iceTea_df.temperature,iceTea_df.iceTeaSales)
print("_"*50)
print(
    "pearson's r:",r_[0],"\n",
    "p_value:",r_[1]
     )
print("_"*50)

#åŸå§‹æ•°æ®æ•£ç‚¹å›¾
fig, axs=plt.subplots(1,3,figsize=(25,8))
axs[0].plot(iceTea_df.temperature,iceTea_df.iceTeaSales,'o',label='ground truth',color='r')
axs[0].set(xlabel='temperature',ylabel='ice tea sales')


#A - ä½¿ç”¨â€˜æœ€å°äºŒä¹˜æ³•â€™é€æ­¥è®¡ç®—
#1 - æ±‚å‡ºxå’Œyçš„ç¦»å·®åŠç¦»å·®å¹³æ–¹å’Œ
iceTea_df["x_deviation"]=iceTea_df.temperature.apply(lambda row: row-iceTea_df.temperature.mean())
iceTea_df["y_deviation"]=iceTea_df.iceTeaSales.apply(lambda row: row-iceTea_df.iceTeaSales.mean())
iceTea_df["S_x_deviation"]=iceTea_df.temperature.apply(lambda row: math.pow(row-iceTea_df.temperature.mean(),2))
iceTea_df["S_y_deviation"]=iceTea_df.iceTeaSales.apply(lambda row: math.pow(row-iceTea_df.iceTeaSales.mean(),2))
SS_x=iceTea_df["S_x_deviation"].sum()
SS_y=iceTea_df["S_y_deviation"].sum()

#2 - æ±‚å‡ºxå’Œyçš„ç¦»å·®ç§¯åŠå…¶å…¶å’Œ
iceTea_df["S_xy_deviation"]=iceTea_df.apply(lambda row: (row["temperature"]-iceTea_df.temperature.mean())*(row["iceTeaSales"]-iceTea_df.iceTeaSales.mean()),axis=1)
SS_xy=iceTea_df["S_xy_deviation"].sum()

#3 - è¿ç®—è¿‡ç¨‹
a,b=sympy.symbols('a b')
iceTea_df["prediciton"]=iceTea_df.temperature.apply(lambda row: a*row+b)
iceTea_df["residual"]=iceTea_df.apply(lambda row: row.iceTeaSales-(a*row.temperature+b),axis=1)
iceTea_df["s_residual"]=iceTea_df.apply(lambda row: (row.iceTeaSales-(a*row.temperature+b))**2,axis=1)
S_residual=iceTea_df["s_residual"].sum()
S_residual_simplify=simplify(S_residual)
print("S_residual simplification(Binary quadratic equation):")
pprint(S_residual_simplify) #æ®‹å·®å¹³æ–¹å’Œä¸ºä¸€ä¸ªäºŒå…ƒäºŒæ¬¡å‡½æ•°
print("_"*50)

#æ‰“å°æ®‹å·®å¹³æ–¹å’Œå›¾å½¢
S_residual_simplif_=sympy.lambdify([a,b],S_residual_simplify,"numpy")
a_=np.arange(-100,100,5)
a_3d=np.repeat(a_[:,np.newaxis],a_.shape[0],axis=1).T
b_=np.arange(-100,100,5)
b_3d=np.repeat(b_[:,np.newaxis],b_.shape[0],axis=1)
z=S_residual_simplif_(a_3d,b_3d)
from sklearn import preprocessing
z_scaled=preprocessing.scale(z) #æ ‡å‡†åŒ–zå€¼ï¼ŒåŒ from scipy.stats import zscoreæ–¹æ³•

axs[1]=fig.add_subplot(1,3,2, projection='3d')
axs[1].plot_wireframe(a_3d,b_3d,z_scaled)
axs[1].contour(a_3d,b_3d,z_scaled, zdir='z', offset=-2, cmap=cm.coolwarm)
axs[1].contour(a_3d,b_3d,z_scaled, zdir='x', offset=-100, cmap=cm.coolwarm)
axs[1].contour(a_3d,b_3d,z_scaled, zdir='y', offset=100, cmap=cm.coolwarm)

#4 - å¯¹æ®‹å·®å¹³æ–¹å’ŒS_residualå…³äºaå’Œbæ±‚å¾®åˆ†ï¼Œå¹¶ä½¿å…¶ä¸º0
diff_S_residual_a=diff(S_residual,a)
diff_S_residual_b=diff(S_residual,b)
print("diff_S_residual_a=",)
pprint(diff_S_residual_a)
print("\n")
print("diff_S_residual_b=",)
pprint(diff_S_residual_b)

Eq_residual_a=Eq(diff_S_residual_a,0) #è®¾æ‰€æ±‚aå¾®åˆ†ä¸º0
Eq_residual_b=Eq(diff_S_residual_b,0) #è®¾æ‰€æ±‚bå¾®åˆ†ä¸º0
slop_intercept=solve((Eq_residual_a,Eq_residual_b),(a,b)) #è®¡ç®—äºŒå…ƒä¸€æ¬¡æ–¹ç¨‹ç»„
print("_"*50)
print("slop and intercept:\n")
pprint(slop_intercept)
slop=slop_intercept[a]
intercept=slop_intercept[b]

#ç”¨æ±‚è§£å›å½’æ–¹ç¨‹å›å½’ç³»æ•°çš„æ¨å¯¼å…¬å¼ä¹‹é—´è®¡ç®—æ–œç‡slopå’Œæˆªè·intercept
print("_"*50)
slop_=SS_xy/SS_x
print("derivation formula to calculate the slop=",slop_)
intercept_=iceTea_df.iceTeaSales.mean()-iceTea_df.temperature.mean()*slop_
print("derivation formula to calculate the intercept=",intercept_)
print("_"*50)

#5 - å»ºç«‹ç®€å•çº¿æ€§å›å½’æ–¹ç¨‹
x=sympy.Symbol('x')
fx=slop*x+intercept
print("linear regression_fx=:\n")
pprint(fx)
fx_=sympy.lambdify(x,fx,"numpy")

#åœ¨æ®‹å·®å¹³æ–¹å’Œå›¾å½¢ä¸Šæ ‡å‡ºa,bçš„ä½ç½®
axs[1].text(slop,intercept,-1.7,"a/b",color="red",size=20)
axs[1].scatter(slop,intercept,-2,color="red",s=80)
axs[1].view_init(60,340) #å¯ä»¥æ—‹è½¬å›¾å½¢çš„è§’åº¦ï¼Œæ–¹ä¾¿è§‚å¯Ÿ

#6 - ç»˜åˆ¶ç®€å•çº¿æ€§å›å½’æ–¹ç¨‹çš„å›¾å½¢
axs[0].plot(iceTea_df.temperature,fx_(iceTea_df.temperature),'o-',label='prediction',color='blue')

#ç»˜åˆ¶çœŸå®å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„è¿çº¿
i=0
for t in iceTea_df.temperature:
    axs[0].arrow(t, iceTea_df.iceTeaSales[i], t-t, fx_(t)-iceTea_df.iceTeaSales[i], head_width=0.1, head_length=0.1,color="gray",linestyle="--" )
    i+=1

#B - ä½¿ç”¨sklearnåº“sklearn.linear_model.LinearRegression()ï¼ŒOrdinary least squares Linear Regression-æ™®é€šæœ€å°äºŒä¹˜çº¿æ€§å›å½’ï¼Œè·å–å›å½’æ–¹ç¨‹
from sklearn.linear_model import LinearRegression
X,y=iceTea_df.temperature.to_numpy().reshape(-1,1),iceTea_df.iceTeaSales.to_numpy()

#æ‹Ÿåˆæ¨¡å‹
LR=LinearRegression().fit(X,y)
#æ¨¡å‹å‚æ•°
print("_"*50)
print("Sklearn slop:%.2f,intercept:%.2f"%(LR.coef_, LR.intercept_))
#æ¨¡å‹é¢„æµ‹
axs[2].plot(iceTea_df.temperature,iceTea_df.iceTeaSales,'o',label='ground truth',color='r')
axs[2].plot(X,LR.predict(X),'o-',label='linear regression prediction')
axs[2].set(xlabel='temperature',ylabel='ice tea sales')

axs[0].legend(loc='upper left', frameon=False)
axs[2].legend(loc='upper left', frameon=False)

axs[0].set_title('step by step manual calculation')
axs[1].set_title('sum of squares of residuals')
axs[2].set_title('using the Sklearn libray')
plt.show()
util.print_html(iceTea_df,14)
```

    __________________________________________________
    pearson's r: 0.9069229780508894 
     p_value: 7.661412804450245e-06
    __________________________________________________
    S_residual simplification(Binary quadratic equation):
           2                           2                 
    12020â‹…a  + 816â‹…aâ‹…b - 60188â‹…a + 14â‹…b  - 2032â‹…b + 75936
    __________________________________________________
    diff_S_residual_a=
    24040â‹…a + 816â‹…b - 60188
    
    
    diff_S_residual_b=
    816â‹…a + 28â‹…b - 2032
    __________________________________________________
    slop and intercept:
    
    â§   1697     -8254 â«
    â¨a: â”€â”€â”€â”€, b: â”€â”€â”€â”€â”€â”€â¬
    â©   454       227  â­
    __________________________________________________
    derivation formula to calculate the slop= 3.7378854625550666
    derivation formula to calculate the intercept= -36.361233480176224
    __________________________________________________
    linear regression_fx=:
    
    1697â‹…x   8254
    â”€â”€â”€â”€â”€â”€ - â”€â”€â”€â”€
     454     227 
    __________________________________________________
    Sklearn slop:3.74,intercept:-36.36
    


<a href=""><img src="./imgs/7_4.png" height="auto" width="auto" title="caDesign"></a>





<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temperature</th>
      <th>iceTeaSales</th>
      <th>x_deviation</th>
      <th>y_deviation</th>
      <th>S_x_deviation</th>
      <th>S_y_deviation</th>
      <th>S_xy_deviation</th>
      <th>prediciton</th>
      <th>residual</th>
      <th>s_residual</th>
    </tr>
    <tr>
      <th>dt</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-07-22</th>
      <td>29</td>
      <td>77</td>
      <td>-0.142857</td>
      <td>4.428571</td>
      <td>0.020408</td>
      <td>19.612245</td>
      <td>-0.632653</td>
      <td>29*a + b</td>
      <td>-29*a - b + 77</td>
      <td>(-29*a - b + 77)**2</td>
    </tr>
    <tr>
      <th>2020-07-23</th>
      <td>28</td>
      <td>62</td>
      <td>-1.142857</td>
      <td>-10.571429</td>
      <td>1.306122</td>
      <td>111.755102</td>
      <td>12.081633</td>
      <td>28*a + b</td>
      <td>-28*a - b + 62</td>
      <td>(-28*a - b + 62)**2</td>
    </tr>
    <tr>
      <th>2020-07-24</th>
      <td>34</td>
      <td>93</td>
      <td>4.857143</td>
      <td>20.428571</td>
      <td>23.591837</td>
      <td>417.326531</td>
      <td>99.224490</td>
      <td>34*a + b</td>
      <td>-34*a - b + 93</td>
      <td>(-34*a - b + 93)**2</td>
    </tr>
    <tr>
      <th>2020-07-25</th>
      <td>31</td>
      <td>84</td>
      <td>1.857143</td>
      <td>11.428571</td>
      <td>3.448980</td>
      <td>130.612245</td>
      <td>21.224490</td>
      <td>31*a + b</td>
      <td>-31*a - b + 84</td>
      <td>(-31*a - b + 84)**2</td>
    </tr>
    <tr>
      <th>2020-07-26</th>
      <td>25</td>
      <td>59</td>
      <td>-4.142857</td>
      <td>-13.571429</td>
      <td>17.163265</td>
      <td>184.183673</td>
      <td>56.224490</td>
      <td>25*a + b</td>
      <td>-25*a - b + 59</td>
      <td>(-25*a - b + 59)**2</td>
    </tr>
    <tr>
      <th>2020-07-27</th>
      <td>29</td>
      <td>64</td>
      <td>-0.142857</td>
      <td>-8.571429</td>
      <td>0.020408</td>
      <td>73.469388</td>
      <td>1.224490</td>
      <td>29*a + b</td>
      <td>-29*a - b + 64</td>
      <td>(-29*a - b + 64)**2</td>
    </tr>
    <tr>
      <th>2020-07-28</th>
      <td>32</td>
      <td>80</td>
      <td>2.857143</td>
      <td>7.428571</td>
      <td>8.163265</td>
      <td>55.183673</td>
      <td>21.224490</td>
      <td>32*a + b</td>
      <td>-32*a - b + 80</td>
      <td>(-32*a - b + 80)**2</td>
    </tr>
    <tr>
      <th>2020-07-29</th>
      <td>31</td>
      <td>75</td>
      <td>1.857143</td>
      <td>2.428571</td>
      <td>3.448980</td>
      <td>5.897959</td>
      <td>4.510204</td>
      <td>31*a + b</td>
      <td>-31*a - b + 75</td>
      <td>(-31*a - b + 75)**2</td>
    </tr>
    <tr>
      <th>2020-07-30</th>
      <td>24</td>
      <td>58</td>
      <td>-5.142857</td>
      <td>-14.571429</td>
      <td>26.448980</td>
      <td>212.326531</td>
      <td>74.938776</td>
      <td>24*a + b</td>
      <td>-24*a - b + 58</td>
      <td>(-24*a - b + 58)**2</td>
    </tr>
    <tr>
      <th>2020-07-31</th>
      <td>33</td>
      <td>91</td>
      <td>3.857143</td>
      <td>18.428571</td>
      <td>14.877551</td>
      <td>339.612245</td>
      <td>71.081633</td>
      <td>33*a + b</td>
      <td>-33*a - b + 91</td>
      <td>(-33*a - b + 91)**2</td>
    </tr>
    <tr>
      <th>2020-08-01</th>
      <td>25</td>
      <td>51</td>
      <td>-4.142857</td>
      <td>-21.571429</td>
      <td>17.163265</td>
      <td>465.326531</td>
      <td>89.367347</td>
      <td>25*a + b</td>
      <td>-25*a - b + 51</td>
      <td>(-25*a - b + 51)**2</td>
    </tr>
    <tr>
      <th>2020-08-02</th>
      <td>31</td>
      <td>73</td>
      <td>1.857143</td>
      <td>0.428571</td>
      <td>3.448980</td>
      <td>0.183673</td>
      <td>0.795918</td>
      <td>31*a + b</td>
      <td>-31*a - b + 73</td>
      <td>(-31*a - b + 73)**2</td>
    </tr>
    <tr>
      <th>2020-08-03</th>
      <td>26</td>
      <td>65</td>
      <td>-3.142857</td>
      <td>-7.571429</td>
      <td>9.877551</td>
      <td>57.326531</td>
      <td>23.795918</td>
      <td>26*a + b</td>
      <td>-26*a - b + 65</td>
      <td>(-26*a - b + 65)**2</td>
    </tr>
    <tr>
      <th>2020-08-04</th>
      <td>30</td>
      <td>84</td>
      <td>0.857143</td>
      <td>11.428571</td>
      <td>0.734694</td>
      <td>130.612245</td>
      <td>9.795918</td>
      <td>30*a + b</td>
      <td>-30*a - b + 84</td>
      <td>(-30*a - b + 84)**2</td>
    </tr>
  </tbody>
</table>



#### 1.2.3 ç¡®è®¤å›å½’æ–¹ç¨‹çš„ç²¾åº¦
ç¡®è®¤å›å½’æ–¹ç¨‹ï¼ˆæ¨¡å‹ï¼‰çš„ç²¾åº¦æ˜¯è®¡ç®—åˆ¤æ–­ç³»æ•°ï¼ˆå†³å®šç³»æ•°ï¼Œcoefficient of determinationï¼‰ï¼Œè®°ä¸º$R^{2} $æˆ–$r^{2} $ï¼Œç”¨äºè¡¨ç¤ºå®æµ‹å€¼ï¼ˆå›¾è¡¨ä¸­çš„ç‚¹ï¼‰ä¸å›å½’æ–¹ç¨‹æ‹Ÿåˆç¨‹åº¦çš„æŒ‡æ ‡ã€‚å…¶å¤(é‡)ç›¸å…³ç³»æ•°è®¡ç®—å…¬å¼ä¸ºï¼š$R=   \frac{\sum_{i=1}^n  ( y_{i} - \overline{y} )^{2} ( \widehat{y}_{i} - \overline{ \widehat{y} }  )^{2}  }{ \sqrt{(\sum_{i=1}^n (y_{i}- \overline{y} )^{2} )(\sum_{i=1}^n ( \widehat{y}_{i} - \overline{ \widehat{y} })^{2} )} } $ï¼Œå…¶ä¸­$y$ä¸ºè§‚æµ‹å€¼ï¼Œ$\overline{y}$ä¸ºè§‚æµ‹å€¼çš„å‡å€¼ï¼Œ$\widehat{y}$ä¸ºé¢„æµ‹å€¼ï¼Œ$\overline{ \widehat{y} } $ä¸ºé¢„æµ‹å€¼çš„å‡å€¼ã€‚è€Œåˆ¤å®šç³»æ•°$R^{2} $åˆ™ä¸ºé‡ç›¸å…³ç³»æ•°çš„å¹³æ–¹ã€‚åˆ¤å®šç³»æ•°çš„å–å€¼åœ¨0åˆ°1ï¼Œå…¶å€¼è¶Šæ¥è¿‘äº1ï¼Œå›å½’æ–¹ç¨‹çš„ç²¾åº¦è¶Šé«˜ã€‚ç¬¬äºŒç§è®¡ç®—å…¬å¼ä¸ºï¼š$R^{2} =1- \frac{ SS_{res} }{ SS_{tot} }=1- \frac{ \sum_{i=1}^n   e_{i} ^{2}  }{SS_{tot}}  =1- \frac{  \sum_{i=1}^n  (y_{i} -   \widehat{y} _{i} )^{2}  }{ \sum_{i=1}^n  ( y_{i} - \overline{y} )^{2}  } $ï¼Œå…¶ä¸­$SS_{res}$ä¸ºæ®‹å·®å¹³æ–¹å’Œï¼Œ$SS_{tot}$ä¸ºè§‚æµ‹å€¼ç¦»å·®å¹³æ–¹å’Œï¼ˆ(æ€»å¹³æ–¹å’Œï¼Œæˆ–æ€»çš„ç¦»å·®å¹³æ–¹å’Œ)ï¼‰ï¼Œ$e_{i}$ä¸ºæ®‹å·®ï¼Œ$y_{i}$ä¸ºè§‚æµ‹å€¼ï¼Œ$\widehat{y}$ä¸ºé¢„æµ‹å€¼ï¼Œ$\overline{y}$ä¸ºè§‚æµ‹å€¼å‡å€¼ã€‚ç¬¬ä¸‰ç§æ˜¯ç›´æ¥ä½¿ç”¨sklearnåº“æä¾›çš„`r2_score`æ–¹æ³•ç›´æ¥è®¡ç®—ã€‚

æ ¹æ®è®¡ç®—ç»“æœç¬¬1ï¼Œ2ï¼Œ3ç§æ–¹æ³•ç»“æœä¸€è‡´ã€‚åœ¨åç»­çš„å®éªŒä¸­ï¼Œç›´æ¥ä½¿ç”¨sklearnæä¾›çš„æ–¹æ³•è¿›è¡Œè®¡ç®—ã€‚


```python
def coefficient_of_determination(observed_vals,predicted_vals):
    import pandas as pd
    import numpy as np
    import math
    '''
    function - å›å½’æ–¹ç¨‹çš„åˆ¤å®šç³»æ•°
    
    Paras:
    observed_vals - è§‚æµ‹å€¼ï¼ˆå®æµ‹å€¼ï¼‰
    predicted_vals - é¢„æµ‹å€¼
    '''
    vals_df=pd.DataFrame({'obs':observed_vals,'pre':predicted_vals})
    #è§‚æµ‹å€¼çš„ç¦»å·®å¹³æ–¹å’Œ(æ€»å¹³æ–¹å’Œï¼Œæˆ–æ€»çš„ç¦»å·®å¹³æ–¹å’Œ)
    obs_mean=vals_df.obs.mean()
    SS_tot=vals_df.obs.apply(lambda row:(row-obs_mean)**2).sum()
    #é¢„æµ‹å€¼çš„ç¦»å·®å¹³æ–¹å’Œ
    pre_mean=vals_df.pre.mean()
    SS_reg=vals_df.pre.apply(lambda row:(row-pre_mean)**2).sum()
    #è§‚æµ‹å€¼å’Œé¢„æµ‹å€¼çš„ç¦»å·®ç§¯å’Œ
    SS_obs_pre=vals_df.apply(lambda row:(row.obs-obs_mean)*(row.pre-pre_mean), axis=1).sum()
    
    #æ®‹å·®å¹³æ–¹å’Œ
    SS_res=vals_df.apply(lambda row:(row.obs-row.pre)**2,axis=1).sum()
    
    #åˆ¤æ–­ç³»æ•°
    R_square_a=(SS_obs_pre/math.sqrt(SS_tot*SS_reg))**2
    R_square_b=1-SS_res/SS_tot
            
    return R_square_a,R_square_b
    
R_square_a,R_square_b=coefficient_of_determination(iceTea_df.iceTeaSales.to_list(),fx_(iceTea_df.temperature).to_list())   
print("R_square_a=%.5f,R_square_b=%.5f"%(R_square_a,R_square_b))

from sklearn.metrics import r2_score
R_square_=r2_score(iceTea_df.iceTeaSales.to_list(),fx_(iceTea_df.temperature).to_list())
print("using sklearn libray to calculate r2_score=",R_square_)
```

    R_square_a=0.82251,R_square_b=0.82251
    using sklearn libray to calculate r2_score= 0.8225092881166944
    

#### 1.2.4 å›å½’ç³»æ•°çš„æ£€éªŒï¼ˆå›å½’æ˜¾è‘—æ€§æ£€éªŒï¼‰ | Fåˆ†å¸ƒä¸æ–¹å·®åˆ†æ

ä¹‹å‰ç›¸å…³ç« èŠ‚åˆ†åˆ«é˜è¿°äº†æ­£æ€åˆ†å¸ƒå’Œtåˆ†å¸ƒï¼Œè€ŒF-åˆ†å¸ƒï¼ˆF-distributionï¼‰æ˜¯ä¸€ç§è¿ç»­æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¿æ³›åº”ç”¨äºä¼¼ç„¶æ¯”ç‡æ£€éªŒï¼Œç‰¹åˆ«æ˜¯æ–¹å·®åˆ†æï¼ˆAnalysis of variance, ANOVAï¼Œæˆ–å˜å¼‚æ•°åˆ†æï¼‰ä¸­ï¼Œå¯¹äºF-åˆ†å¸ƒçš„é˜é‡Šä½¿ç”¨scipy.stats.fçš„å®˜æ–¹æ¡ˆä¾‹ã€‚å‡½æ•°æ–¹æ³•åŸºæœ¬åŒæ­£æ€åˆ†å¸ƒå’Œtåˆ†å¸ƒã€‚


```python
from scipy.stats import f
import matplotlib.pyplot as plt
fig, ax=plt.subplots(1, 1)

dfn, dfd=29, 18
mean, var, skew, kurt=f.stats(dfn, dfd, moments='mvsk')
print("mean=%f, var=%f, skew=%f, kurt=%f"%(mean, var, skew, kurt))

# æ‰“å°æ¦‚ç‡å¯†åº¦å‡½æ•°(probability density function,pdf)
x=np.linspace(f.ppf(0.01, dfn, dfd),f.ppf(0.99, dfn, dfd), 100) #å–æœä»è‡ªç”±åº¦dfnå’Œdfdï¼Œä½äº1%åˆ°99%çš„100ä¸ªå–å€¼
ax.plot(x, f.pdf(x, dfn, dfd),'-', lw=5, alpha=0.6, label='f pdf')

# å›ºå®šåˆ†å¸ƒå½¢çŠ¶ï¼Œå³å›ºå®šè‡ªç”±åº¦
rv = f(dfn, dfd)
ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

vals = f.ppf([0.001, 0.5, 0.999], dfn, dfd)
print("éªŒè¯ç´¯è®¡åˆ†å¸ƒå‡½æ•°CDFè¿”å›å€¼ä¸å…¶PPFè¿”å›å€¼æ˜¯å¦ç›¸ç­‰æˆ–è¿‘ä¼¼ï¼š",np.allclose([0.001, 0.5, 0.999], f.cdf(vals, dfn, dfd)))

#ç”Ÿæˆæœä»F-åˆ†å¸ƒçš„éšæœºæ•°ï¼Œå¹¶æ‰“å°ç›´æ–¹å›¾
r=f.rvs(dfn, dfd, size=1000)
ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
ax.legend(loc='best', frameon=False)
plt.show()
```

    mean=1.125000, var=0.280557, skew=1.806568, kurt=7.074636
    éªŒè¯ç´¯è®¡åˆ†å¸ƒå‡½æ•°CDFè¿”å›å€¼ä¸å…¶PPFè¿”å›å€¼æ˜¯å¦ç›¸ç­‰æˆ–è¿‘ä¼¼ï¼š True
    


<a href=""><img src="./imgs/7_5.png" height="auto" width="auto" title="caDesign"></a>


* æ€»å¹³æ–¹å’Œ=å›å½’å¹³æ–¹å’Œ+æ®‹å·®å¹³æ–¹å’Œ

å…¬å¼ä¸ºï¼š$SS_{tot}=\sum_{i=1}^n  ( y_{i} - \overline{y} )^{2}=SS_{reg}+SS_{res}= \sum_{i=1}^n  (\widehat{y} _{i} -    \overline{y} )^{2} + \sum_{i=1}^n  (y_{i} -   \widehat{y} _{i} )^{2} $ï¼Œå¼ä¸­$SS_{reg}$å›å½’å¹³æ–¹å’Œï¼Œå…¶å®ƒåŒä¸Šã€‚å›å½’å¹³æ–¹å’Œæ˜¯é¢„æµ‹å€¼ï¼ˆå›å½’å€¼ï¼‰ä¸è§‚æµ‹å€¼ï¼ˆçœŸå®å€¼ã€å®æµ‹å€¼ï¼‰å‡å€¼ä¹‹å·®çš„å¹³æ–¹å’Œï¼Œè¯¥ç»Ÿè®¡é‡åæ˜ äº†è‡ªå˜é‡$x_{1}, x_{2}, \ldots ,x_{m},  $çš„å˜åŒ–å¼•èµ·çš„$y$($y_{k} (k=1,2, \ldots ,n)$)çš„æ³¢åŠ¨ï¼Œå…¶è‡ªç”±åº¦ä¸º$df_{reg}=m $ï¼Œå…¶ä¸­$m$ä¸ºè‡ªå˜é‡çš„ä¸ªæ•°ï¼Œæ¸©åº¦ä¸é”€é‡æ±‚è§£çš„ä¸€å…ƒä¸€æ¬¡çº¿æ€§æ–¹ç¨‹åªæœ‰ä¸€ä¸ªè‡ªå˜é‡ï¼Œå› æ­¤å…¶è‡ªç”±åº¦ä¸º1ï¼Œå³åªæœ‰è¿™ä¸€ä¸ªå› ç´ å¯ä»¥è‡ªç”±å˜åŒ–ï¼›æ®‹å·®å¹³æ–¹å’Œæ˜¯è§‚æµ‹å€¼ä¸é¢„æµ‹å€¼ä¹‹å·®çš„å¹³æ–¹å’Œï¼Œæ®‹å·®çš„å­˜åœ¨æ˜¯ç”±å®éªŒè¯¯å·®åŠå…¶å®ƒå› ç´ å¼•èµ·çš„ï¼Œå…¶è‡ªç”±åº¦ä¸º$df_{res}=n-m-1 $ï¼Œå…¶ä¸­$n$ä¸ºæ ·æœ¬æ•°é‡ï¼Œå³å¯¹åº”çš„$y$çš„å–å€¼æ•°é‡ã€‚æ€»çš„ç¦»å·®å¹³æ–¹å’Œ$SS_{tot}$çš„è‡ªç”±åº¦ä¸º$n-1$ã€‚

è§‚æµ‹å€¼ï¼ˆæ ·æœ¬ï¼‰é€šå¸¸æ˜¯ç»™å®šçš„ï¼Œå› æ­¤æ€»çš„ç¦»å·®å¹³æ–¹å’Œæ˜¯å›ºå®šçš„ï¼Œæ„æˆæ€»çš„ç¦»å·®å¹³æ–¹å’Œçš„å› ç´ ä¸ºå›å½’å¹³æ–¹å’Œå’Œæ®‹å·®å¹³æ–¹å’Œï¼Œåˆ†å¸ƒä»£è¡¨æ‰€æ±‚å¾—çš„å›å½’æ–¹ç¨‹ï¼Œæˆ–å®éªŒè¯¯å·®å’Œå…¶å®ƒå› ç´ å¼•èµ·$y$å€¼å¾—å˜åŒ–ï¼Œå½“æ®‹å·®å¹³æ–¹å’Œè¶Šå°ï¼ˆå°±æ˜¯å®éªŒè¯¯å·®å’Œå…¶å®ƒå› ç´ å½±å“å°ï¼‰ï¼Œåˆ™å›å½’å¹³æ–¹å’Œè¶Šå¤§ï¼Œåˆ™è¯´æ˜æ‰€æ±‚å¾—çš„å›å½’æ–¹ç¨‹çš„é¢„æµ‹å€¼è¶Šå‡†ç¡®ã€‚


> è‡ªç”±åº¦çš„å†è®¨è®ºï¼ˆå‚è€ƒWikipediaï¼‰
åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œè‡ªç”±åº¦ï¼ˆdefree of freedom, dfï¼‰æ˜¯æŒ‡å½“ä»¥æ ·æœ¬çš„ç»Ÿè®¡é‡ä¼°è®¡æ€»ä½“çš„å‚æ•°æ—¶ï¼Œæ ·æœ¬ä¸­ç‹¬ç«‹æˆ–èƒ½è‡ªç”±å˜åŒ–çš„æ•°æ®çš„ä¸ªæ•°ï¼Œç§°ä¸ºè¯¥ç»Ÿè®¡é‡çš„è‡ªç”±åº¦ã€‚èŒƒä¾‹ï¼š
> 1. è‹¥å­˜åœ¨ä¸¤ä¸ªè‡ªå˜é‡$x$å’Œ$y$ï¼Œå¦‚æœ$y=x+c$ï¼Œå…¶ä¸­$c$ä¸ºå¸¸é‡ï¼Œåˆ™å…¶è‡ªç”±åº¦ä¸º1ï¼Œå› ä¸ºå®é™…ä¸Šåªæœ‰$x$æ‰èƒ½çœŸæ­£çš„è‡ªç”±å˜åŒ–ï¼Œ$y$ä¼šè¢«$x$å–å€¼çš„ä¸åŒæ‰€é™åˆ¶ï¼›
2. ä¼°è®¡æ€»ä½“çš„å¹³å‡æ•°$\mu$æ—¶ï¼Œç”±äºæ ·æœ¬ä¸­$n$ä¸ªæ•°éƒ½æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œä»»ä¸€ä¸ªå°šæœªæŠ½å‡ºçš„æ•°éƒ½ä¸å—å·²æŠ½å‡ºä»»ä½•æ•°å€¼çš„å½±å“ï¼Œæ‰€ä»¥è‡ªç”±åº¦ä¸º$n$;
3. ä¼°è®¡æ€»ä½“çš„æ–¹å·®$ \sigma ^{2} $æ—¶æ‰€ä½¿ç”¨çš„ç»Ÿè®¡é‡æ˜¯æ ·æœ¬çš„æ–¹å·®$s^{2} $ï¼Œè€Œ$s^{2} $å¿…é¡»ç”¨åˆ°æ ·æœ¬å¹³å‡æ•°$\overline{x} $æ¥è®¡ç®—ï¼Œ$\overline{x} $åœ¨æŠ½æ ·å®Œæˆåå·²ç¡®å®šï¼Œæ‰€ä»¥å¤§å°ä¸º$n$çš„æ ·æœ¬ä¸­åªè¦$n-1$ä¸ªæ•°ç¡®å®šï¼Œç¬¬$n$ä¸ªæ•°å°±åªæœ‰ä¸€ä¸ªèƒ½ä½¿æ ·æœ¬ç¬¦åˆ$\overline{x} $çš„æ•°å€¼ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ ·æœ¬ä¸­åªæœ‰$n-1$ä¸ªæ•°å¯ä»¥è‡ªç”±å˜åŒ–ï¼Œåªè¦ç¡®å®šäº†è¿™$n-1$ä¸ªæ•°ï¼Œæ–¹å·®ä¹Ÿå°±ç¡®å®šäº†ã€‚è¿™é‡Œï¼Œå¹³å‡æ•°$\overline{x} $å°±ç›¸å½“äºä¸€ä¸ªé™åˆ¶æ¡ä»¶ï¼Œç”±äºåŠ äº†è¿™ä¸ªé™åˆ¶æ¡ä»¶ï¼Œæ ·æœ¬æ–¹å·®$s^{2} $çš„è‡ªç”±åº¦ä¸º$n-1$ï¼›
4. ç»Ÿè®¡æ¨¡å‹çš„è‡ªç”±åº¦ç­‰äºå¯è‡ªç”±å–å€¼çš„è‡ªå˜é‡çš„ä¸ªæ•°ã€‚å¦‚åœ¨å›å½’æ–¹ç¨‹ä¸­ï¼Œå¦‚æœå…±æœ‰$p$ä¸ªå‚æ•°éœ€è¦ä¼°è®¡ï¼Œåˆ™å…¶ä¸­åŒ…æ‹¬äº†$p-1$ä¸ªè‡ªå˜é‡ï¼ˆä¸æˆªè·å¯¹åº”çš„è‡ªå˜é‡æ˜¯å¸¸é‡ï¼‰ï¼Œå› æ­¤è¯¥å›å½’æ–¹ç¨‹çš„è‡ªç”±åº¦ä¸º$p-1$ã€‚

> æ— åä¼°è®¡ï¼ˆunbiased estimatorï¼‰åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œä¸€ä¸ªæ€»ä½“çš„æ ‡å‡†å·®é€šå¸¸æ˜¯ç”±æ€»ä½“ä¸­éšæœºæŠ½å–çš„æ ·æœ¬çš„ä¼°è®¡ï¼Œæ ·æœ¬æ ‡å‡†å·®çš„å®šä¹‰ä¸ºï¼š$s= \sqrt{ \frac{ \sum_{i=1}^n  ( x_{i }- \overline{x} ) ^{2} }{n-1} } $ï¼Œå…¶ä¸­ $x_{1},  x_{2} , \ldots , x_{n} $ä¸ºæ ·æœ¬ï¼Œæ ·æœ¬å®¹é‡ä¸º$n$ï¼Œ$\overline{x}$ä¸ºæ ·æœ¬å‡å€¼ã€‚ä½¿ç”¨$n-1$æ›¿ä»£$n$ï¼Œè¢«ç§°ä¸ºBessel's correctionï¼ˆè´å¡å°”çŸ«æ­£ï¼‰ï¼Œçº æ­£äº†æ€»ä½“æ–¹å·®ä¼°è®¡ä¸­çš„åå·®ï¼ˆæ€»ä½“æ–¹å·®ä¼°è®¡æ˜¯ä½¿ç”¨éšæœºæŠ½å–çš„æ ·æœ¬çš„ä¼°è®¡ï¼Œä¸ç­‰äºæ€»ä½“æ–¹å·®ï¼‰ï¼Œä»¥åŠæ€»ä½“æ ‡å‡†å·®ä¼°è®¡ä¸­çš„éƒ¨åˆ†åå·®ï¼Œä½†ä¸æ˜¯å…¨éƒ¨åå·®ã€‚å› ä¸ºåå·®å–å†³äºç‰¹å®šçš„åˆ†å¸ƒï¼Œä¸å¯èƒ½æ‰¾åˆ°å¯¹æ‰€æœ‰æ€»ä½“åˆ†å¸ƒæ— åçš„æ ‡å‡†åå·®çš„ä¼°è®¡ã€‚


* æ–¹å·®åˆ†æï¼ˆAnalysis of variance, ANOVAï¼Œæˆ–å˜å¼‚æ•°åˆ†æï¼‰

ä¸Šè¿°`æ€»å¹³æ–¹å’Œ=å›å½’å¹³æ–¹å’Œ+æ®‹å·®å¹³æ–¹å’Œ`åˆ†æå®é™…ä¸Šæ˜¯åœ¨åˆ†æå› å˜é‡ï¼ˆæ€»å¹³æ–¹å’Œï¼Œå³æ€»çš„ç¦»å·®å¹³æ–¹å’Œï¼‰ä¸å½±å“å› å˜é‡å˜åŒ–çš„ä¸¤ä¸ªå› ç´ ï¼ˆæˆ–ç§°ä¸ºä¸¤ä¸ªç±»åˆ«ï¼‰ï¼Œå³å›å½’å¹³æ–¹å’ŒåŠæ®‹å·®å¹³æ–¹å’Œçš„å…³ç³»æ¢ç´¢ï¼Œè¿™ä¸ªè¿‡ç¨‹æ—¢ç§°ä¹‹ä¸ºæ–¹å·®åˆ†æã€‚åœ¨æ±‚è§£ä¸Šè¿°å›å½’æ–¹ç¨‹ä¹‹å‰ï¼Œæ¸©åº¦ä¸é”€é‡çš„å…³ç³»ä¸ä¸€å®šæ˜¯çº¿æ€§çš„ï¼Œå¯èƒ½å­˜åœ¨ä¸¤ç§æƒ…å†µï¼Œä¸€ç§æ˜¯ä¸ç®¡æ¸©åº¦ï¼ˆ$x$ï¼‰å–ä»€ä¹ˆå€¼ï¼Œé”€é‡ï¼ˆ$y$ï¼‰éƒ½åœ¨ä¸€æ¡æ°´å¹³çº¿ä¸Šä¸‹æ³¢åŠ¨ï¼›äºŒæ˜¯ï¼Œæ¸©åº¦å’Œé”€é‡å­˜åœ¨é™¤çº¿æ€§å¤–å…¶å®ƒç±»å‹çš„å…³ç³»ï¼Œä¾‹å¦‚éçº¿æ€§ç­‰ã€‚

å¯¹äºä¸Šè¿°æ‰€æ±‚å¾—å›å½’æ–¹ç¨‹$ f_{x} =ax+b= \frac{1697}{ 454} x- \frac{8254}{227 } $ï¼ˆæ ·æœ¬å›å½’æ¨¡å‹ï¼‰ï¼Œå¯¹äºæ€»ä½“è€Œè¨€ï¼Œ$F_{x} =Ax+B$ï¼ˆæ€»ä½“å›å½’æ¨¡å‹ï¼‰ï¼Œæ–œç‡Açº¦ä¸ºa($A \sim a$)ï¼Œæˆªè·Bçº¦ä¸ºb($B \sim b$)ï¼Œ $\sigma ^{2} = \frac{SS_{res} }{n-2}$ï¼ˆæ— åä¼°è®¡é‡ï¼Œæ®‹å·®å¹³æ–¹å’Œæœ‰$n-2$ä¸ªè‡ªç”±åº¦ï¼Œè¿™æ˜¯å› ä¸ºä¸¤ä¸ªè‡ªç”±åº¦ä¸å¾—åˆ°é¢„æµ‹å€¼çš„ä¼°è®¡å€¼$A$å’Œ$B$ç›¸å…³ï¼‰ï¼Œ$\sigma ^{2} $çš„å¹³æ–¹æ ¹æœ‰æ—¶ç§°ä¸ºå›å½’æ ‡å‡†è¯¯å·®ã€‚(ç”±æ®‹å·®å¹³æ–¹å’Œæ±‚å¾—$\sigma ^{2} $çš„æ¨å¯¼è¿‡ç¨‹ï¼Œå¯ä»¥å‚è€ƒ'çº¿æ€§å›å½’åˆ†æå¯¼è®º(Introduction to linear regression analysis)'ï¼Œç®€å•çº¿æ€§å›å½’éƒ¨åˆ†)ã€‚


æ€»ä½“å›å½’æ–¹ç¨‹$F_{x} =Ax+B$éå¸¸é‡è¦çš„ç‰¹ä¾‹æ˜¯ï¼Œ$H_{0} :A=0,  H_{1} :A \neq 0$ï¼ŒåŸå‡è®¾æ„å‘³$x$å’Œ$y$ä¹‹é—´ä¸å­˜åœ¨çº¿æ€§å…³ç³»ï¼Œ$x$å¯¹è§£é‡Š$y$çš„æ–¹å·®å‡ ä¹æ˜¯æ— ç”¨çš„ï¼›å¦‚æœæ‹’ç»åŸå‡è®¾ï¼Œè€Œæ¥å—å¤‡æ‹©å‡è®¾ï¼Œæ„å‘³$x$å¯¹è§£é‡Š$y$çš„æ–¹å·®æ˜¯æœ‰ç”¨çš„ï¼Œå¯èƒ½æ„å‘³çº¿æ€§æ¨¡å‹æ˜¯åˆé€‚çš„ï¼Œä½†æ˜¯ä¹Ÿå¯èƒ½å­˜åœ¨éœ€è¦ç”¨é«˜é˜¶å¤šé¡¹å¼æ‹Ÿåˆçš„éçº¿æ€§æ¨¡å‹ã€‚å¯¹äºå›å½’æ˜¾è‘—æ€§æ£€éªŒå¯ä»¥ä½¿ç”¨tç»Ÿè®¡é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æ–¹å·®åˆ†æã€‚å›å½’ç³»æ•°æ£€éªŒçš„Fç»Ÿè®¡é‡ä¸ºï¼š$F_{0} = \frac{ SS_{reg}/ df_{reg}  }{ SS_{res}/ df_{res}  }$ ;å…¶ä¸­$SS_{reg}$ä¸ºå›å½’å¹³æ–¹å’Œï¼Œè‡ªç”±åº¦$df_{reg} =m$ä¸º1ï¼Œ$SS_{res}$ä¸ºæ®‹å·®å¹³æ–¹å’Œï¼Œå…¶è‡ªç”±åº¦$df_{res}=n-m-1$ä¸º$14-1-1=12$ï¼Œï¼ˆæ¨å¯¼è¿‡ç¨‹å‚è€ƒå‚è€ƒ'çº¿æ€§å›å½’åˆ†æå¯¼è®º(Introduction to linear regression analysis)'ï¼‰ã€‚å¦‚æœåŸå‡è®¾æˆç«‹ï¼Œé‚£ä¹ˆæ£€éªŒç»Ÿè®¡é‡å°±æœä»ç¬¬1è‡ªç”±åº¦$m=1$ï¼Œç¬¬2è‡ªç”±åº¦$n-m-1=12$çš„Fåˆ†å¸ƒã€‚p-value=0.000008ï¼Œå°äºæ˜¾è‘—æ€§æ°´å¹³0.05ï¼Œæ‹’ç»åŸå‡è®¾ï¼Œå¤‡æ‹©å‡è®¾æˆç«‹ã€‚


```python
def ANOVA(observed_vals,predicted_vals,df_reg,df_res):
    import pandas as pd
    import numpy as np
    import math
    from scipy.stats import f
    '''
    function - ç®€å•çº¿æ€§å›å½’æ–¹ç¨‹-å›å½’æ˜¾è‘—æ€§æ£€éªŒï¼ˆå›å½’ç³»æ•°æ£€éªŒï¼‰
    
    Paras:
    observed_vals - è§‚æµ‹å€¼ï¼ˆå®æµ‹å€¼ï¼‰
    predicted_vals - é¢„æµ‹å€¼
    '''
    vals_df=pd.DataFrame({'obs':observed_vals,'pre':predicted_vals})
    #è§‚æµ‹å€¼çš„ç¦»å·®å¹³æ–¹å’Œ(æ€»å¹³æ–¹å’Œï¼Œæˆ–æ€»çš„ç¦»å·®å¹³æ–¹å’Œ)
    obs_mean=vals_df.obs.mean()
    SS_tot=vals_df.obs.apply(lambda row:(row-obs_mean)**2).sum()
    
    #æ®‹å·®å¹³æ–¹å’Œ
    SS_res=vals_df.apply(lambda row:(row.obs-row.pre)**2,axis=1).sum()
   
    #å›å½’å¹³æ–¹å’Œ
    SS_reg=vals_df.pre.apply(lambda row:(row-obs_mean)**2).sum()
    
    print("æ€»å¹³æ–¹å’Œ=%.6f,å›å½’å¹³æ–¹å’Œ=%.6f,æ®‹å·®å¹³æ–¹å’Œ=%.6f"%(SS_tot,SS_reg,SS_res))
    print("æ€»å¹³æ–¹å’Œ=å›å½’å¹³æ–¹å’Œ+æ®‹å·®å¹³æ–¹å’Œï¼šSS_tot=SS_reg+SS_res=%.6f+%.6f=%.6f"%(SS_reg,SS_res,SS_reg+SS_res))
    
    Fz=(SS_reg/df_reg)/(SS_res/df_res)
    print("F-åˆ†å¸ƒç»Ÿè®¡é‡=%.6f;p-value=%.6f"%(Fz,f.sf(Fz,df_reg,df_res)))

ANOVA(iceTea_df.iceTeaSales.to_list(),fx_(iceTea_df.temperature).to_list(),df_reg=1,df_res=12) 
```

    æ€»å¹³æ–¹å’Œ=2203.428571,å›å½’å¹³æ–¹å’Œ=1812.340466,æ®‹å·®å¹³æ–¹å’Œ=391.088106
    æ€»å¹³æ–¹å’Œ=å›å½’å¹³æ–¹å’Œ+æ®‹å·®å¹³æ–¹å’Œï¼šSS_tot=SS_reg+SS_res=1812.340466+391.088106=2203.428571
    F-åˆ†å¸ƒç»Ÿè®¡é‡=55.609172;p-value=0.000008
    

åˆ©ç”¨Fæ£€éªŒå¯¹å›å½’æ–¹ç¨‹è¿›è¡Œæ˜¾è‘—æ€§æ£€éªŒçš„æ–¹æ³•å°±æ˜¯æ–¹ç¨‹åˆ†æï¼Œå°†ä¸Šè¿°è¿‡ç¨‹å¯ä»¥å½’ç»“ä¸ºä¸€ä¸ªæ–¹ç¨‹åˆ†æè¡¨ï¼Œä»è€Œæ›´å®¹æ˜“ç¼•æ¸…è„‰ç»œã€‚

| ç»Ÿè®¡é‡        | å¹³æ–¹å’Œ           | è‡ªç”±åº¦  |æ–¹å·® |æ–¹å·®æ¯”|
| ------------- |:-------------:| -----:| -----:| -----:|
| å›å½’      | $SS_{reg}= \sum_{i=1}^n  (\widehat{y} _{i} -    \overline{y} )^{2}$ | $df_{reg}=m $|$SS_{reg}/df_{reg}$|  $F_{0} = \frac{ SS_{reg}/ df_{reg}  }{ SS_{res}/ df_{res}  }$    |
| æ®‹å·®      |$SS_{res}= \sum_{i=1}^n  (y_{i} -   \widehat{y} _{i} )^{2}   $   |  $df_{res}= n-m-1$ |$SS_{res}/df_{res}$||
| æ€»ä½“ | $SS_{tot}=\sum_{i=1}^n  ( y_{i} - \overline{y} )^{2}$     |  $df_{tot}= n-1$  |||


#### 1.2.5 æ€»ä½“å›å½’$Ax+b$çš„ä¼°è®¡â€”â€”ç½®ä¿¡åŒºé—´ä¼°è®¡
å¯¹äºæ¸©åº¦ä¸é”€é‡çš„å›å½’æ¨¡å‹ï¼Œæ¸©åº¦ä¸ºä»»æ„å€¼æ—¶ï¼Œæ‰€å¯¹åº”çš„é”€é‡ä¸æ˜¯ä¸€ä¸ªå›ºå®šçš„å€¼ï¼Œè€Œæ˜¯æœä»å¹³å‡å€¼ä¸º$Ax+B$ï¼ˆæ€»ä½“å›å½’ï¼‰ï¼Œæ ‡å‡†å·®ä¸º$\sigma$çš„æ­£æ€åˆ†å¸ƒï¼Œå› æ­¤åœ¨ç»™å®šç½®ä¿¡åº¦ï¼ˆ95%ï¼Œ99%ç­‰ï¼‰ï¼Œæ€»ä½“å›å½’$Ax+B$(å³é¢„æµ‹å€¼)ä¸€å®šä¼šåœ¨æŸä¸ªå€¼ä»¥ä¸Šï¼ŒæŸä¸ªå€¼ä»¥ä¸‹çš„åŒºé—´ä¸­ï¼Œè®¡ç®—ä»»æ„æ¸©åº¦æ‰€å¯¹åº”é”€é‡çš„ç½®ä¿¡åŒºé—´ï¼Œæ˜¯ç”±é¢„æµ‹å€¼åŠ å‡ä¸€ä¸ªåŒºé—´ï¼Œè¯¥åŒºé—´çš„è®¡ç®—å…¬å¼ä¸ºï¼š$\sqrt{F(1,n-2;0.05) \times ( \frac{1}{n}+ \frac{ ( x_{i}- \overline{x}  )^{2} }{ S_{xx} }  ) \times   \frac{SS_{res}}{n-2}  } $ï¼Œå…¶ä¸­$n$ä¸ºæ ·æœ¬ä¸ªæ•°ï¼Œ$ x_{i}$ä¸ºè‡ªå˜é‡ï¼ˆæ¸©åº¦ï¼‰æ ·æœ¬å–å€¼ï¼Œ$\overline{x}$ä¸ºæ ·æœ¬å‡å€¼ï¼Œ$S_{xx}$ä¸ºè‡ªå˜é‡$x$ï¼ˆæ¸©åº¦ï¼‰æ ·æœ¬çš„ç¦»å·®å¹³æ–¹å’Œï¼Œ$SS_{res}$ä¸ºæ®‹å·®å¹³æ–¹å’Œã€‚


```python
def confidenceInterval_estimator_LR(x,sample_num,X,y,model,confidence=0.05):
    import numpy as np
    import math
    from scipy.stats import f
    import matplotlib.pyplot as plt
    '''
    function - ç®€å•çº¿æ€§å›å½’ç½®ä¿¡åŒºé—´ä¼°è®¡ï¼Œä»¥åŠé¢„æµ‹åŒºé—´
    
    Paras:
    x - è‡ªå˜é‡å–å€¼
    sample_num - æ ·æœ¬æ•°é‡
    X - æ ·æœ¬æ•°æ®é›†-è‡ªå˜é‡
    y - æ ·æœ¬æ•°æ®é›†-å› å˜é‡
    model -ä½¿ç”¨sklearnè·å–çš„çº¿æ€§å›å½’æ¨¡å‹
    confidence -  ç½®ä¿¡åº¦
    '''
    X_=X.reshape(-1)
    X_mu=X_.mean()
    s_xx=(X_-X_mu)**2
    S_xx=s_xx.sum()
    ss_res=(y-LR.predict(X))**2
    SS_res=ss_res.sum()
    probability_val=f.ppf(q=1-confidence,dfn=1, dfd=sample_num-2) #dfn=1, dfd=sample_num-2
    CI=[math.sqrt(probability_val*(1/sample_num+(x-X_mu)**2/S_xx)*SS_res/(sample_num-2)) for x in X_]
    y_pre=LR.predict(X)
    
    fig, ax=plt.subplots(figsize=(10,10))
    ax.plot(X_,y,'o',label='observations/ground truth',color='r')
    ax.plot(X_,y_pre,'o-',label='linear regression prediction')
    ax.plot(X_,y_pre-CI,'--',label='y_lower')
    ax.plot(X_,y_pre+CI,'--',label='y_upper')
    ax.fill_between(X_, y_pre-CI, y_pre+CI, alpha=0.2,label='95% confidence interval')    
      
    #ç»™å®šå€¼çš„é¢„æµ‹åŒºé—´
    x_ci=math.sqrt(probability_val*(1/sample_num+(x-X_mu)**2/S_xx)*SS_res/(sample_num-2))
    x_pre=LR.predict(np.array([x]).reshape(-1,1))[0]
    x_lower=x_pre-x_ci
    x_upper=x_pre+x_ci
    print("x prediction=%.6f;confidence interval=[%.6f,%.6f]"%(x_pre,x_lower,x_upper))
    ax.plot(x,x_pre,'x',label='x_prediction',color='r',markersize=20)
    ax.arrow(x, x_pre, 0, x_upper-x_pre, head_width=0.3, head_length=2,color="gray",linestyle="--" ,length_includes_head=True)
    ax.arrow(x, x_pre, 0, x_lower-x_pre, head_width=0.3, head_length=2,color="gray",linestyle="--" ,length_includes_head=True)
        
    ax.set(xlabel='temperature',ylabel='ice tea sales')
    ax.legend(loc='upper left', frameon=False)    
    plt.show()                  
    return CI

sample_num=14
confidence=0.05
iceTea_df_sort=iceTea_df.sort_values(by=['temperature'])
X,y=iceTea_df_sort.temperature.to_numpy().reshape(-1,1),iceTea_df_sort.iceTeaSales.to_numpy()
CI=confidenceInterval_estimator_LR(27,sample_num,X,y,LR,confidence)    
```

    x prediction=64.561674;confidence interval=[60.496215,68.627133]
    


<a href=""><img src="./imgs/7_6.png" height="auto" width="auto" title="caDesign"></a>


#### 1.2.6 é¢„æµ‹åŒºé—´
ç»™å®šç‰¹å®šå€¼ä¾‹å¦‚æ¸©åº¦ä¸º31ï¼Œåˆ™é¢„æµ‹å€¼ä¸º79.51ï¼Œä½†æ˜¯å®é™…å€¼ä¸ä¸€å®šä¸ºè¯¥å€¼ï¼Œè€Œæ˜¯åœ¨ç½®ä¿¡åº¦ï¼ˆç½®ä¿¡æ°´å¹³ï¼Œç½®ä¿¡ç³»æ•°ï¼‰ä¸º95%ï¼Œå¯¹åº”çš„ç½®ä¿¡åŒºé—´$[66.060470,92.965962]$å†…æµ®åŠ¨ï¼Œè¿™ä¸ªåŒºé—´ç§°ä¸ºé¢„æµ‹åŒºé—´ï¼Œ

### 1.3 å¤šå…ƒçº¿æ€§å›å½’
åŒ…å«å¤šäºä¸€ä¸ªå›å½’å˜é‡çš„å›å½’æ¨¡å‹ç§°ä¸ºå¤šå…ƒå›å½’æ¨¡å‹ï¼Œå¦‚æœä¸ºçº¿æ€§åˆ™ä¸ºå¤šå…ƒçº¿æ€§å›å½’ï¼ˆmultivariable linear regressionï¼‰ã€‚åœ¨å¾ˆæ˜¯å®é™…é—®é¢˜å¤„ç†ä¸Šï¼Œå°¤å…¶å¤§æ•°æ®ï¼Œä¼šæ¶‰åŠåˆ°å¾ˆå¤šè‡ªå˜é‡ï¼Œä¾‹å¦‚Sklearnæœºå™¨å­¦ä¹ åº“ç»å…¸çš„é¸¢å°¾èŠ±(Iris)æ•°æ®é›†åŒ…å«çš„è‡ªå˜é‡æœ‰èŠ±è¼é•¿åº¦ã€èŠ±è¼å®½åº¦ï¼ŒèŠ±ç“£é•¿åº¦å’ŒèŠ±ç“£å®½åº¦(Sepal Length, Sepal Width, Petal Length and Petal Width)ï¼Œæ€»å…±4ä¸ªï¼Œå…¶å› å˜é‡ä¸ºé¸¢å°¾èŠ±çš„ç§ç±»ï¼Œå¦‚æœè¦æ ¹æ®è‡ªå˜é‡ä¸å› å˜é‡å»ºç«‹å›å½’æ¨¡å‹ï¼Œåˆ™éœ€è¦5ä¸ªå‚æ•°ã€‚

#### 1.3.1 å»ºç«‹æ•°æ®é›†
åœ¨åº”ç”¨pythonè¯­è¨€è§£æè¯¥éƒ¨åˆ†å†…å®¹æ—¶ï¼Œä»ç„¶ä½¿ç”¨æ¯”è¾ƒç®€å•çš„æ•°æ®é›†ï¼Œç”¨'æ¼«ç”»ç»Ÿè®¡å­¦ä¹‹å›å½’åˆ†æ'ä¸­åº—é“ºçš„æ•°æ®é›†ï¼Œè‡ªå˜é‡åŒ…æ‹¬åº—é“ºçš„é¢ç§¯($m^{2} $)ã€ æœ€è¿‘çš„è½¦ç«™è·ç¦»ï¼ˆmï¼‰ï¼Œå› å˜é‡ä¸ºæœˆè¥ä¸šé¢ï¼ˆä¸‡å…ƒï¼‰ã€‚


```python
import pandas as pd
import util
from scipy import stats

store_info={"location":['Ill.','Ky.','Lowa.','Wis.','MIch.','Neb.','Ark.','R.I.','N.H.','N.J.'],"area":[10,8,8,5,7,8,7,9,6,9],"distance_to_nearestStation":[80,0,200,200,300,230,40,0,330,180],"monthly_turnover":[469,366,371,208,246,297,363,436,198,364]}
storeInfo_df=pd.DataFrame(store_info)
util.print_html(storeInfo_df,10)
```




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>location</th>
      <th>area</th>
      <th>distance_to_nearestStation</th>
      <th>monthly_turnover</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Ill.</td>
      <td>10</td>
      <td>80</td>
      <td>469</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Ky.</td>
      <td>8</td>
      <td>0</td>
      <td>366</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Lowa.</td>
      <td>8</td>
      <td>200</td>
      <td>371</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Wis.</td>
      <td>5</td>
      <td>200</td>
      <td>208</td>
    </tr>
    <tr>
      <th>4</th>
      <td>MIch.</td>
      <td>7</td>
      <td>300</td>
      <td>246</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Neb.</td>
      <td>8</td>
      <td>230</td>
      <td>297</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Ark.</td>
      <td>7</td>
      <td>40</td>
      <td>363</td>
    </tr>
    <tr>
      <th>7</th>
      <td>R.I.</td>
      <td>9</td>
      <td>0</td>
      <td>436</td>
    </tr>
    <tr>
      <th>8</th>
      <td>N.H.</td>
      <td>6</td>
      <td>330</td>
      <td>198</td>
    </tr>
    <tr>
      <th>9</th>
      <td>N.J.</td>
      <td>9</td>
      <td>180</td>
      <td>364</td>
    </tr>
  </tbody>
</table>



#### 1.3.2 ç›¸å…³æ€§åˆ†æ
ä¸ºäº†åˆ¤æ–­ä¾æ®ä¸Šè¿°æ•°æ®æ˜¯å¦å…·æœ‰å»ºç«‹å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹çš„æ„ä¹‰ï¼ŒåŒæ ·éœ€è¦è¿›è¡Œç›¸å…³æ€§åˆ†æã€‚å› ä¸ºæ‰€æ¶‰åŠçš„å˜é‡å¢åŠ ï¼Œéœ€è¦è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„ç›¸å…³ç³»æ•°ï¼Œä»¥åŠå¯¹åº”çš„På€¼ï¼Œä¸ºäº†æ–¹ä¾¿æ—¥åå¯¹æ­¤ç§ç±»å‹æ•°æ®çš„ç›¸å…³æ€§åˆ†æï¼Œå»ºç«‹`correlationAnalysis_multivarialbe`å‡½æ•°ã€‚è‡ªå˜é‡ä¸å› å˜é‡ä¹‹é—´çš„ç›¸å…³ç³»æ•°åæ˜ äº†è‡ªå˜é‡æ‰€èƒ½è§£é‡Šå› å˜é‡çš„ç¨‹åº¦ï¼Œå…¶ç›¸å…³ç³»æ•°åˆ†åˆ«ä¸º0.8924,-0.7751ï¼Œä¸¤ä¸ªè‡ªå˜é‡å‡ä¸å› å˜é‡å…·æœ‰è¾ƒå¼ºçš„ç›¸å…³å…³ç³»ï¼Œèƒ½å¤Ÿè§£é‡Šå› å˜é‡ï¼Œå¯ä»¥å»ºç«‹å›å½’æ¨¡å‹ï¼›åŒæ—¶ï¼Œè‡ªå˜é‡ä¹‹é—´çš„ç›¸å…³å…³ç³»ï¼Œå¯ä»¥åˆæ­¥åˆ¤æ–­è‡ªå˜é‡ä¹‹é—´æ˜¯å¦å­˜åœ¨å¤šé‡å…±çº¿æ€§ï¼Œå³è‡ªå˜é‡ä¹‹é—´å­˜åœ¨ç²¾ç¡®ç›¸å…³å…³ç³»æˆ–é«˜åº¦ç›¸å…³å…³ç³»ï¼Œè€Œä½¿å¾—æ¨¡å‹ä¼°è®¡å¤±çœŸï¼Œæˆ–è€…éš¾ä»¥ä¼°è®¡å‡†ç¡®ï¼Œæ ¹æ®è®¡ç®—ç»“æœä¸¤ä¸ªè‡ªå˜é‡ä¹‹é—´çš„ç›¸å…³ç³»æ•°ä¸º-0.4922ï¼Œä½†æ˜¯å¯¹åº”På€¼ä¸º0.1485ï¼Œå³æ‹’ç»åŸå‡è®¾ï¼Œè¯´æ˜ä¸¤ä¸ªè‡ªå˜é‡ä¹‹é—´ä¸å­˜åœ¨çº¿æ€§ç›¸å…³å…³ç³»ï¼Œå› æ­¤åŒæ—¶ä½¿ç”¨è¿™ä¸¤ä¸ªè‡ªå˜é‡è§£é‡Šå› å˜é‡ï¼Œåˆæ­¥åˆ¤æ–­ä¸ä¼šä½¿å›å½’æ¨¡å‹å¤±çœŸã€‚


```python
import seaborn as sns
import matplotlib.pyplot as plt
sns.pairplot(storeInfo_df)
plt.show()

def correlationAnalysis_multivarialbe(df):
    from scipy.stats import pearsonr
    import pandas as pd
    '''
    function - DataFrameæ•°æ®æ ¼å¼ï¼Œæˆç»„è®¡ç®—pearsonrç›¸å…³ç³»æ•°
    
    Paras:
    df - DataFrameæ ¼å¼æ•°æ®é›†
    '''
    df=df.dropna()._get_numeric_data()
    df_cols=pd.DataFrame(columns=df.columns)
    p_values=df_cols.transpose().join(df_cols, how='outer')
    correlation=df_cols.transpose().join(df_cols, how='outer')
    for r in df.columns:
        for c in df.columns:
            p_values[r][c]=round(pearsonr(df[r], df[c])[1], 4)
            correlation[r][c]=round(pearsonr(df[r], df[c])[0], 4)
            
            
    return p_values,correlation
p_values,correlation=correlationAnalysis_multivarialbe(storeInfo_df)

print("p_values:")
print(p_values)
print("_"*78)
print("correlation:")
print(correlation)
```


<a href=""><img src="./imgs/7_7.png" height="auto" width="auto" title="caDesign"></a>


    p_values:
                                  area distance_to_nearestStation monthly_turnover
    area                             0                     0.1485           0.0005
    distance_to_nearestStation  0.1485                          0           0.0084
    monthly_turnover            0.0005                     0.0084                0
    ______________________________________________________________________________
    correlation:
                                  area distance_to_nearestStation monthly_turnover
    area                             1                    -0.4922           0.8924
    distance_to_nearestStation -0.4922                          1          -0.7751
    monthly_turnover            0.8924                    -0.7751                1
    

å› ä¸ºæ€»å…±æ¶‰åŠåˆ°äº†3ä¸ªå˜é‡ï¼Œå¯ä»¥ä½¿ç”¨plotlyåº“æä¾›çš„ä¸‰å…ƒå›¾ï¼ˆTernary Plotï¼‰,æŸ¥çœ‹ä¸¤ä¸ªè‡ªå˜é‡ä¸ä¸€ä¸ªå› å˜é‡ä¹‹é—´çš„åˆ†å¸ƒå…³ç³»ã€‚å¯èƒ½å˜é‡ä¹‹é—´æ•°å€¼çš„å–å€¼èŒƒå›´ç›¸å·®è¾ƒå¤§ï¼Œåœ¨ä¸‰å…ƒå›¾æ‰“å°æ—¶æŸäº›å˜é‡çš„å€¼å¯èƒ½å…¨éƒ¨è´´è¿‘å›¾å½¢è¾¹ç¼˜ï¼Œæ— æ³•æ¸…æ™°è¡¨è¿°å˜é‡é—´çš„å…³ç³»ï¼Œå› æ­¤ä½¿ç”¨ï¼š$\frac{ x_{i} - \overline{x} }{ x_{max}-  x_{min} } $æ–¹æ³•åˆ†åˆ«æ ‡å‡†åŒ–å„ä¸ªå˜é‡ã€‚ä»å›¾ç¤ºå¯å¾—åº—é“ºé¢ç§¯ï¼ˆé¢œè‰²è¡¨ç¤ºé¢ç§¯ï¼‰é€æ¸å¢åŠ ï¼Œæœˆè¥ä¸šé¢é€æ¸å¢åŠ ï¼ˆç‚¹çš„å¤§å°è¡¨ç¤ºæœˆè¥ä¸šé¢æ•°å€¼å¤§å°ï¼‰ï¼›è€Œæœ€è¿‘çš„è½¦ç«™è·ç¦»é€æ­¥å‡å°æ—¶ï¼Œæœˆè¥ä¸šé¢é€æ¸å¢åŠ ã€‚


```python
pd.options.mode.chained_assignment = None

columns=['area','distance_to_nearestStation','monthly_turnover']
storeInfo_plot=storeInfo_df[columns]
normalize_df=storeInfo_plot.T.apply(lambda row:(row-row.min())/(row.max()-row.min()) , axis=1).T
normalize_df["location"]=storeInfo_df.location

import plotly.express as px
fig=px.scatter_ternary(normalize_df,a="monthly_turnover", b="area",c="distance_to_nearestStation",hover_name="location",
                       color="area",size="monthly_turnover", size_max=8) 

fig.show()

```

<a href=""><img src="./imgs/7_8.png" height="auto" width="auto" title="caDesign"></a>

#### 1.3.3 æ±‚è§£å¤šå…ƒå›å½’æ–¹ç¨‹
æ±‚è§£å¤šå…ƒå›å½’æ–¹ç¨‹çš„æ–¹æ³•åŸºæœ¬ç­‰åŒäºç®€å•çº¿æ€§å›å½’æ±‚è§£æ–¹å¼ï¼Œä½¿ç”¨æœ€å°äºŒä¹˜æ³•å¯¹åå›å½’ç³»æ•°è¿›è¡Œæ±‚è§£ã€‚æ±‚è§£è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†ä¸‰ç§æ–¹æ³•ï¼Œä¸€æ˜¯ï¼Œä½¿ç”¨sympyåˆ†åˆ«å¯¹æ®‹å·®å¹³æ–¹å’Œ$SS_res$çš„$a1$ã€$a2$å’Œ$b$æ±‚å¾®åˆ†ï¼Œå½“å„è‡ªå¾®åˆ†çš„å€¼ç­‰äº0æ—¶ï¼Œæ‰€åæ˜ çš„æ®‹å·®å¹³æ–¹å’Œä¸º0ï¼Œå³è§‚æµ‹å€¼å’Œé¢„æµ‹å€¼å·®å€¼çš„å¹³æ–¹å’Œä¸º0ï¼Œè€Œå•ä¸ªè§‚æµ‹å€¼ä¸å¯¹åº”çš„é¢„æµ‹å€¼ä¹‹é—´çš„å·®å€¼è¶‹äº0ï¼›äºŒæ˜¯ï¼Œä½¿ç”¨çŸ©é˜µè®¡ç®—çš„æ–¹å¼æ±‚è§£å‚æ•°ï¼Œå…¶è®¡ç®—å…¬å¼ä¸ºï¼š$\widehat{ \beta } = ( X^{'} X)^{-1} X^{'}y$ï¼Œå…¶ä¸­$X=\left[\begin{matrix}1 & 10 & 80\\1 & 8 & 0\\1 & 8 & 200\\1 & 5 & 200\\1 & 7 & 300\\1 & 8 & 230\\1 & 7 & 40\\1 & 9 & 0\\1 & 6 & 330\\1 & 9 & 180\end{matrix}\right]$ï¼Œ$X^{'}  =\left[\begin{matrix}1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\10 & 8 & 8 & 5 & 7 & 8 & 7 & 9 & 6 & 9\\80 & 0 & 200 & 200 & 300 & 230 & 40 & 0 & 330 & 180\end{matrix}\right]$å³$X$çš„çš„è½¬ç½®ï¼Œä¹Ÿå¯è®°ä½œ$X^{T} ,X^{tr} $ç­‰ï¼Œ$y=\left[\begin{matrix}469\\366\\371\\208\\246\\297\\363\\436\\198\\364\end{matrix}\right]$ã€‚å¯¹äºä¸€ä¸ªçŸ©é˜µ$X$ï¼Œå…¶é€†çŸ©é˜µï¼ˆinverse matrixï¼‰ä¸º$X^{-1} $ã€‚ä½¿ç”¨çŸ©é˜µçš„è®¡ç®—æ–¹æ³•æ—¶ï¼Œä»ç„¶æ˜¯ä½¿ç”¨sympyåº“ï¼Œè¯¥åº“æä¾›äº†å»ºç«‹çŸ©é˜µå’ŒçŸ©é˜µè®¡ç®—çš„åŠŸèƒ½ã€‚æœ€åä¸€ç§æ±‚è§£å¤šå…ƒçº¿æ€§å›å½’æ–¹ç¨‹çš„æ–¹å¼æ˜¯ç›´æ¥ä½¿ç”¨`sklearn.linear_model.LinearRegression`è®¡ç®—ï¼Œå¹¶è·å¾—å›å½’æ¨¡å‹ã€‚

> åå›å½’ç³»æ•°ï¼ˆpartial regression coefficientï¼‰ï¼Œæ˜¯å¤šå…ƒå›å½’é—®é¢˜å‡ºç°çš„ä¸€ä¸ªç‰¹æ®Šæ€§è´¨ã€‚è®¾è‡ªå˜é‡$x_{1}, x_{2}, \ldots ,x_{m},  $ï¼Œä¸å› å˜é‡$y$å…·æœ‰çº¿æ€§å…³ç³»ï¼Œæœ‰$y= a_{1}  x_{1} + a_{2}  x_{2}+ \ldots + a_{n}  x_{n}+b$ï¼Œåˆ™$a_{1} , a_{2} , \ldots , a_{n}  $ä¸ºç›¸å¯¹äºå„è‡ªå˜é‡çš„åå›å½’ç³»æ•°ï¼Œè¡¨ç¤ºå½“å…¶ä»–çš„å„è‡ªå˜é‡éƒ½ä¿æŒä¸€å®šæ—¶ï¼ŒæŒ‡å®šçš„æŸä¸€è‡ªå˜é‡æ¯å˜åŠ¨ä¸€ä¸ªå•ä½ï¼Œå› å˜é‡yå¢åŠ æˆ–å‡å°‘çš„æ•°å€¼ã€‚ 


```python
import sympy,math
from sympy import diff,Eq,solveset,solve,simplify,pprint,Matrix

a1,a2,b=sympy.symbols('a1 a2 b')
#è®¡ç®—æ®‹å·®å¹³æ–¹å’Œ
storeInfo_df["ss_res"]=storeInfo_df.apply(lambda row:(row.monthly_turnover-(row.area*a1+row.distance_to_nearestStation*a2+b))**2,axis=1)
util.print_html(storeInfo_df,10)
SS_res=storeInfo_df["ss_res"].sum()

#A- ä½¿ç”¨sympyæ±‚è§£å¤šå…ƒå›å½’æ–¹ç¨‹
#å¯¹æ®‹å·®å¹³æ–¹å’ŒSS_reså…³äºa1ï¼Œa1å’Œbæ±‚å¾®åˆ†ï¼Œå¹¶ä½¿å¾®åˆ†å€¼ä¸º0
diff_SSres_a1=diff(SS_res,a1)
diff_SSres_a2=diff(SS_res,a2)
diff_SSres_b=diff(SS_res,b)

#å½“å¾®åˆ†å€¼ä¸º0æ—¶ï¼Œè§£æ–¹ç¨‹ç»„ï¼Œè·å¾—a1ï¼Œa2å’Œbçš„å€¼
Eq_residual_a1=Eq(diff_SSres_a1,0) #è®¾æ‰€æ±‚a1å¾®åˆ†ä¸º0
Eq_residual_a2=Eq(diff_SSres_a2,0) #è®¾æ‰€æ±‚a2å¾®åˆ†ä¸º0
Eq_residual_b=Eq(diff_SSres_b,0) #è®¾æ‰€æ±‚a2å¾®åˆ†ä¸º0
slop_intercept=solve((Eq_residual_a1,Eq_residual_a2,Eq_residual_b),(a1,a2,b)) #è®¡ç®—ä¸‰å…ƒä¸€æ¬¡æ–¹ç¨‹ç»„
print("diff_a1,a2 and intercept:\n")
pprint(slop_intercept)
print("_"*50)

#B - ä½¿ç”¨çŸ©é˜µï¼ˆåŸºäºsympyï¼‰æ±‚è§£å¤šå…ƒå›å½’æ–¹ç¨‹
if 'one' not in storeInfo_df.columns:
    X_m=Matrix(storeInfo_df.insert(loc=1,column='one',value=1)[['one','area','distance_to_nearestStation']])
else:
    X_m=Matrix(storeInfo_df[['one','area','distance_to_nearestStation']])
y_m=Matrix(storeInfo_df.monthly_turnover)

parameters_reg=(X_m.T*X_m)**-1*X_m.T*y_m #æ³¨æ„åœ¨çŸ©é˜µè®¡ç®—æ—¶ï¼ŒçŸ©é˜µç›¸ä¹˜ä¸èƒ½ä»»æ„å˜åŒ–ä½ç½®
print("matrix_a1,a2 and intercept:\n")
pprint(parameters_reg)

#C - ä½¿ç”¨sklearnæ±‚è§£å¤šå…ƒå›å½’æ–¹ç¨‹
#B - ä½¿ç”¨sklearnåº“sklearn.linear_model.LinearRegression()ï¼ŒOrdinary least squares Linear Regression-æ™®é€šæœ€å°äºŒä¹˜çº¿æ€§å›å½’ï¼Œè·å–å›å½’æ–¹ç¨‹
from sklearn.linear_model import LinearRegression
X=storeInfo_df[['area','distance_to_nearestStation']].to_numpy()
y=storeInfo_df['monthly_turnover'].to_numpy()

#æ‹Ÿåˆæ¨¡å‹
LR_multivariate=LinearRegression().fit(X,y)
#æ¨¡å‹å‚æ•°
print("_"*50)
print("Sklearn a1=%.2f,a2=%.2f,intercept=%.2f"%(LR_multivariate.coef_[0],LR_multivariate.coef_[1], LR_multivariate.intercept_))

#å»ºç«‹å›å½’æ–¹ç¨‹
x1,x2=sympy.symbols('x1,x2')
fx_m=slop_intercept[a1]*x1+slop_intercept[a2]*x2+slop_intercept[b]
print("linear regression_fx=:\n")
pprint(fx_m)
fx_m=sympy.lambdify([x1,x2],fx_m,"numpy")
```

    diff_a1,a2 and intercept:
    
    â§    4073344      -44597      6409648â«
    â¨aâ‚: â”€â”€â”€â”€â”€â”€â”€, aâ‚‚: â”€â”€â”€â”€â”€â”€â”€, b: â”€â”€â”€â”€â”€â”€â”€â¬
    â©     98121        130828      98121 â­
    __________________________________________________
    matrix_a1,a2 and intercept:
    
    â¡6409648â¤
    â¢â”€â”€â”€â”€â”€â”€â”€â¥
    â¢ 98121 â¥
    â¢       â¥
    â¢4073344â¥
    â¢â”€â”€â”€â”€â”€â”€â”€â¥
    â¢ 98121 â¥
    â¢       â¥
    â¢-44597 â¥
    â¢â”€â”€â”€â”€â”€â”€â”€â¥
    â£ 130828â¦
    __________________________________________________
    Sklearn a1=41.51,a2=-0.34,intercept=65.32
    linear regression_fx=:
    
    4073344â‹…xâ‚   44597â‹…xâ‚‚   6409648
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ - â”€â”€â”€â”€â”€â”€â”€â”€ + â”€â”€â”€â”€â”€â”€â”€
      98121       130828     98121 
    


```python
#å¯ä»¥å°†çŸ©é˜µæ‰“å°ä¸ºLatexæ ¼å¼çš„æ•°å­¦è¡¨è¾¾å¼ï¼Œæ–¹ä¾¿åœ¨markdownä¸­è¡¨è¿°ï¼Œä¸éœ€è¦è‡ªè¡Œè¾“å…¥ã€‚
from sympy import latex
print(latex(X_m.T))
```

    \left[\begin{matrix}1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\10 & 8 & 8 & 5 & 7 & 8 & 7 & 9 & 6 & 9\\80 & 0 & 200 & 200 & 300 & 230 & 40 & 0 & 330 & 180\end{matrix}\right]
    

åŒæ ·ä½¿ç”¨ä¸‰å…ƒå›¾æ‰“å°ä¸¤ä¸ªè‡ªå˜é‡ï¼Œä»¥åŠé¢„æµ‹å€¼ä¹‹é—´çš„å›¾è¡¨ï¼Œè§‚å¯Ÿå˜é‡ä¹‹é—´çš„å…³ç³»ã€‚


```python
pd.options.mode.chained_assignment = None
storeInfo_df['pre']=LR_multivariate.predict(X)
columns=['area','distance_to_nearestStation','monthly_turnover','pre']
storeInfo_plot=storeInfo_df[columns]
normalize_df=storeInfo_plot.T.apply(lambda row:(row-row.min())/(row.max()-row.min()) , axis=1).T
normalize_df["location"]=storeInfo_df.location

import plotly.express as px
fig = px.scatter_ternary(normalize_df, a="pre", b="area",c="distance_to_nearestStation",hover_name="location",
                         color="area",size="pre", size_max=8,
                         ) 

fig.show()
```
<a href=""><img src="./imgs/7_9.png" height="auto" width="auto" title="caDesign"></a>

#### 1.3.4 ç¡®è®¤å¤šå…ƒå›å½’æ–¹ç¨‹çš„ç²¾åº¦
éä¿®æ­£è‡ªç”±åº¦çš„åˆ¤å®šç³»æ•°è®¡ç®—åŒç®€å•çº¿æ€§å›å½’ï¼Œå°†å®šä¹‰çš„è®¡ç®—å‡½æ•°`coefficient_of_determination`æ”¾ç½®äº'util.py'æ–‡ä»¶ä¸­ï¼Œç›´æ¥è°ƒç”¨è®¡ç®—ï¼ŒåŒæ—¶ä¹Ÿä½¿ç”¨Sklearnæä¾›çš„r2_scoreè®¡ç®—ï¼Œå…¶è®¡ç®—ç»“æœçº¦ä¸º0.94ï¼Œè¡¨ç¤ºå®æµ‹å€¼ä¸å›å½’æ–¹ç¨‹çš„é¢„æµ‹å€¼æ‹Ÿåˆç¨‹åº¦çš„æŒ‡æ ‡è¾ƒé«˜ï¼Œèƒ½å¤Ÿæ¯”è¾ƒå¥½çš„æ ¹æ®åº—é“ºé¢ç§¯å’Œæœ€è¿‘è½¦ç«™è·ç¦»é¢„æµ‹æœˆè¥ä¸šé¢ã€‚


```python
#è®¡ç®—å¤ç›¸å…³ç³»æ•°R
import util
R_square_a,R_square_b=util.coefficient_of_determination(storeInfo_df.monthly_turnover.to_list(),storeInfo_df.pre.to_list())   
print("R_square_a=%.5f,R_square_b=%.5f"%(R_square_a,R_square_b))

from sklearn.metrics import r2_score
R_square_=r2_score(storeInfo_df.monthly_turnover.to_list(),storeInfo_df.pre.to_list())
print("using sklearn libray to calculate r2_score=",R_square_)
```

    R_square_a=0.94524,R_square_b=0.94524
    using sklearn libray to calculate r2_score= 0.945235852681711
    

* ä¿®æ­£è‡ªç”±åº¦çš„åˆ¤å®šç³»æ•°
ç›´æ¥ä½¿ç”¨åˆ¤å®šç³»æ•°æ—¶ï¼Œå…¶è‡ªå˜é‡çš„æ•°é‡è¶Šå¤šï¼Œåˆ¤å®šç³»æ•°çš„å€¼è¶Šé«˜ï¼Œä½†æ˜¯å¹¶ä¸æ˜¯æ¯ä¸€ä¸ªè‡ªå˜é‡éƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œå› æ­¤é€šå¸¸ä½¿ç”¨ä¿®æ­£è‡ªç”±åº¦çš„åˆ¤å®šç³»æ•°ï¼Œå…¶å…¬å¼ä¸ºï¼š$R^{2} =1- \frac{  \frac{SS_{res}}{ n_{s} - n_{v} -1}  }{  \frac{SS_{tot}}{n_{s} -1}  }$ï¼Œå…¶ä¸­$n_{s}$ä¸ºæ ·æœ¬ä¸ªæ•°ï¼Œ$n_{v}$ä¸ºè‡ªå˜é‡ä¸ªæ•°ï¼Œ$SS_{res}$ä¸ºæ®‹å·®å¹³æ–¹å’Œï¼Œ$SS_{tot}$ä¸ºæ€»çš„ç¦»å·®å¹³æ–¹å’Œã€‚


```python
def coefficient_of_determination_correction(observed_vals,predicted_vals,independent_variable_n):
    import pandas as pd
    import numpy as np
    import math
    '''
    function - å›å½’æ–¹ç¨‹çš„ä¿®æ­£è‡ªç”±åº¦çš„åˆ¤å®šç³»æ•°
    
    Paras:
    observed_vals - è§‚æµ‹å€¼ï¼ˆå®æµ‹å€¼ï¼‰
    predicted_vals - é¢„æµ‹å€¼
    independent_variable_n - è‡ªå˜é‡ä¸ªæ•°
    '''
    vals_df=pd.DataFrame({'obs':observed_vals,'pre':predicted_vals})
    #è§‚æµ‹å€¼çš„ç¦»å·®å¹³æ–¹å’Œ(æ€»å¹³æ–¹å’Œï¼Œæˆ–æ€»çš„ç¦»å·®å¹³æ–¹å’Œ)
    obs_mean=vals_df.obs.mean()
    SS_tot=vals_df.obs.apply(lambda row:(row-obs_mean)**2).sum()
    
    #æ®‹å·®å¹³æ–¹å’Œ
    SS_res=vals_df.apply(lambda row:(row.obs-row.pre)**2,axis=1).sum()
    
    #åˆ¤æ–­ç³»æ•°
    sample_n=len(observed_vals)
    R_square_correction=1-(SS_res/(sample_n-independent_variable_n-1))/(SS_tot/(sample_n-1))
            
    return R_square_correction
R_square_correction=coefficient_of_determination_correction(storeInfo_df.monthly_turnover.to_list(),storeInfo_df.pre.to_list(),2)
print("ä¿®æ­£è‡ªç”±åº¦çš„åˆ¤å®šç³»æ•°=",R_square_correction)
```

    ä¿®æ­£è‡ªç”±åº¦çš„åˆ¤å®šç³»æ•°= 0.929588953447914
    

#### 1.3.5 å›å½’æ˜¾è‘—æ€§æ£€éªŒ
åœ¨ç®€å•å›å½’æ¨¡å‹ä¸­çš„å›å½’ç³»æ•°æ£€éªŒï¼Œåªéœ€è¦ç»™å®š$H_{0} :A=0,  H_{1} :A \neq 0$ï¼Œä½†æ˜¯åœ¨å¤šå…ƒå›å½’ä¸­ï¼Œå°±æ€»ä½“è€Œè¨€$F_{x} = A_{1}  x_{1} +A_{2}  x_{2}+B$ï¼Œå…¶ä¸­$A_{1} \sim  a_{1} ,A_{2} \sim  a_{2},B \sim b$ï¼Œ$\sim$ä¸ºçº¦ä¸ºã€‚åŒ…æ‹¬$A_{1}$å’Œ $A_{2}$ä¸¤ä¸ªåç›¸å…³ç³»æ•°ï¼Œå› æ­¤å¯ä»¥åˆ†ä¸ºä¸¤ç§æƒ…å†µï¼Œä¸€ç§æ˜¯å…¨é¢è®¨è®ºåå›å½’ç³»æ•°çš„æ£€éªŒï¼ŒåŸå‡è®¾ï¼š$A_{1} =A_{2}=0$ï¼Œå¤‡æ‹©å‡è®¾:$A_{1} =A_{2}=0$ä¸æˆç«‹ï¼Œå³ä»¥ä¸‹ä»»æ„ä¸€ç»„å…³ç³»æˆç«‹ï¼Œ$A_{1}  \neq 0$ä¸”$A_{2}  \neq 0$ï¼Œ$A_{1}  \neq 0$ä¸”$A_{2} = 0$ï¼Œæˆ–$A_{1}  =0$ä¸”$A_{2}  \neq 0$ã€‚å¦ä¸€ç§æ˜¯åˆ†åˆ«è®¨è®ºåå›å½’ç³»æ•°çš„æ£€éªŒï¼Œä¾‹å¦‚åŸå‡è®¾ï¼š$A_{1}  =0$ï¼Œå¤‡æ‹©å‡è®¾ï¼š$A_{1}  \neq 0$ã€‚åœ¨è¿™ä¸¤ç§æ–¹å¼ä¸­ï¼Œæ£€éªŒç»Ÿè®¡é‡æ˜¯ä¸åŒçš„ï¼Œå¯¹äºå…¨é¢æ£€éªŒï¼Œå…¶ç»Ÿè®¡é‡ä¸ºï¼š$F_{0}=  \frac{ SS_{tot}- SS_{res}  }{ n_{v} } / \frac{SS_{res} }{ n_{s}- n_{v}-1} $ï¼Œå…¶ä¸­$SS_{tot}$ä¸ºæ€»å¹³æ–¹å’Œ$SS_{res}$ä¸ºæ®‹å·®å¹³æ–¹å’Œï¼Œ$n_{s}$ä¸ºæ ·æœ¬ä¸ªæ•°ï¼Œ$n_{v}$ä¸ºè‡ªå˜é‡ä¸ªæ•°ï¼Œ;å¯¹äºå•ä¸ªå›å½’ç³»æ•°çš„æ£€éªŒï¼Œå…¶ç»Ÿè®¡é‡ä¸ºï¼š$F_{0}=  \frac{  a_{1} ^{2} }{ C_{jj} } / \frac{ SS_{res} }{ n_{s}- n_{v} -1 } $ï¼Œå…¶ä¸­$ C_{jj}$ä¸º$( X^{'} X)^{-1} $å¯¹è§’çº¿ç›¸äº¤ä½ç½®çš„å€¼ï¼Œå³$( X^{'} X)^{-1} =\left[\begin{matrix}\frac{511351}{98121} & - \frac{55781}{98121} & - \frac{1539}{327070}\\- \frac{55781}{98121} & \frac{6442}{98121} & \frac{66}{163535}\\- \frac{1539}{327070} & \frac{66}{163535} & \frac{67}{6541400}\end{matrix}\right]$ï¼Œå¯¹è§’çº¿çš„å€¼ä¸º$\frac{6442}{98121} $ã€‚

å¯¹äºå…¨éƒ¨å›å½’ç³»æ•°çš„æ€»ä½“æ£€éªŒï¼Œä»¥åŠå•ä¸ªå›å½’ç³»æ•°çš„æ£€éªŒï¼Œå…¶ç»“æœPå€¼å‡å°äº0.05ï¼Œæ„å‘³ç€æ‰€æ±‚å¾—çš„å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹æ˜¯åˆé€‚çš„ã€‚


```python
def ANOVA_multivarialbe(observed_vals,predicted_vals,independent_variable_n,a_i,X):
    import pandas as pd
    import numpy as np
    import math
    from scipy.stats import f
    from sympy import Matrix,pprint
    '''
    function - å¤šå…ƒçº¿æ€§å›å½’æ–¹ç¨‹-å›å½’æ˜¾è‘—æ€§æ£€éªŒï¼ˆå›å½’ç³»æ•°æ£€éªŒï¼‰ï¼Œå…¨éƒ¨å›å½’ç³»æ•°çš„æ€»ä½“æ£€éªŒï¼Œä»¥åŠå•ä¸ªå›å½’ç³»æ•°çš„æ£€éªŒ
    
    Paras:    
    observed_vals - è§‚æµ‹å€¼ï¼ˆå®æµ‹å€¼ï¼‰
    predicted_vals - é¢„æµ‹å€¼
    independent_variable_n - è‡ªå˜é‡ä¸ªæ•°
    a_i - åç›¸å…³ç³»æ•°åˆ—è¡¨
    X - æ ·æœ¬æ•°æ®é›†_è‡ªå˜é‡
    '''
    vals_df=pd.DataFrame({'obs':observed_vals,'pre':predicted_vals})
    #æ€»å¹³æ–¹å’Œï¼Œæˆ–æ€»çš„ç¦»å·®å¹³æ–¹å’Œ
    obs_mean=vals_df.obs.mean()
    SS_tot=vals_df.obs.apply(lambda row:(row-obs_mean)**2).sum()
    
    #æ®‹å·®å¹³æ–¹å’Œ
    SS_res=vals_df.apply(lambda row:(row.obs-row.pre)**2,axis=1).sum()
   
    #å›å½’å¹³æ–¹å’Œ
    SS_reg=vals_df.pre.apply(lambda row:(row-obs_mean)**2).sum()
    
    #æ ·æœ¬ä¸ªæ•°
    n_s=len(observed_vals)
    dfn=independent_variable_n
    dfd=n_s-independent_variable_n-1
    
    #è®¡ç®—å…¨éƒ¨å›å½’ç³»æ•°çš„æ€»ä½“æ£€éªŒç»Ÿè®¡é‡
    F_total=((SS_tot-SS_res)/dfn)/(SS_res/dfd)
    print("F-åˆ†å¸ƒç»Ÿè®¡é‡_total=%.6f;p-value=%.6f"%(F_total,f.sf(F_total,dfn,dfd)))
    
    #é€ä¸ªè®¡ç®—å•ä¸ªå›å½’ç³»æ•°çš„æ£€éªŒç»Ÿè®¡é‡
    X=np.insert(X,0,1,1)
    X_m=Matrix(X)
    M_inverse=(X_m.T*X_m)**-1
    C_jj=M_inverse.row(1).col(1)[0]
    pprint(C_jj)
    
    F_ai_list=[]
    i=0
    for a in a_i:
        F_ai=(a**2/C_jj)/(SS_res/dfd)
        F_ai_list.append(F_ai)
        print("a%d=%.6fæ—¶ï¼ŒF-åˆ†å¸ƒç»Ÿè®¡é‡_=%.6f;p-value=%.6f"%(i,a,F_ai,f.sf(F_total,1,dfd)))
        i+=1
 
a1_,a2_=LR_multivariate.coef_[0],LR_multivariate.coef_[1]
X=storeInfo_df[['area','distance_to_nearestStation']].to_numpy()
ANOVA_multivarialbe(storeInfo_df.monthly_turnover.to_list(),storeInfo_df.pre.to_list(),2,a_i=[a1_,a2_],X=X) 
```

    F-åˆ†å¸ƒç»Ÿè®¡é‡_total=60.410426;p-value=0.000038
     6442
    â”€â”€â”€â”€â”€
    98121
    a0=41.513478æ—¶ï¼ŒF-åˆ†å¸ƒç»Ÿè®¡é‡_=44.032010;p-value=0.000110
    a1=-0.340883æ—¶ï¼ŒF-åˆ†å¸ƒç»Ÿè®¡é‡_=0.002969;p-value=0.000110
    

#### 1.3.6 æ€»ä½“å›å½’$A_{1}  X_{1} + A_{2}  X_{2}+ \ldots + A_{n}  X_{n}+B$çš„ä¼°è®¡â€”â€”ç½®ä¿¡åŒºé—´
å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹çš„é¢„æµ‹å€¼ç½®ä¿¡åŒºé—´ä¼°è®¡ä½¿ç”¨äº†ä¸¤ç§è®¡ç®—æ–¹å¼ï¼Œä¸€æ˜¯ï¼Œè‡ªå®šä¹‰å‡½æ•°é€æ­¥è®¡ç®—ï¼Œå…¶è®¡ç®—å…¬å¼ä¸ºï¼š$\sqrt{F(1,n_s-n_v-1;0.05) \times ( \frac{1}{n_s}+ \frac{ D ^{2} }{ n_s-1 }  ) \times   \frac{SS_{res}}{n_s-n_v-1}  } $ï¼Œå…¶ä¸­$n_s$ä¸ºæ ·æœ¬ä¸ªæ•°ï¼Œ$n_v$ä¸ºè‡ªå˜é‡ä¸ªä½æ•°ï¼Œ$D ^{2}$ä¸ºé©¬æ°è·ç¦»ï¼ˆMahalanobis distanceï¼‰çš„å¹³æ–¹ï¼Œ$SS_{res}$ä¸ºæ®‹å·®å¹³æ–¹å’Œï¼›$D ^{2}$é©¬æ°è·ç¦»çš„å¹³æ–¹è®¡ç®—å…¬å¼ä¸ºï¼šå…ˆæ±‚$S=\begin{bmatrix} S_{11} &S_{12} & \ldots &S_{1p}  \\S_{21}  &S_{22}& \ldots &S_{2p}\\ \vdots & \vdots & \ddots & \vdots \\ S_{p1} &S_{p2}& \ldots &S_{pp}   \end{bmatrix} $çš„é€†çŸ©é˜µ$S^{-1} $ï¼Œå…¶ä¸­ï¼Œ$S_{22}$ä»£è¡¨ç¬¬2ä¸ªè‡ªå˜é‡çš„ç¦»å·®å¹³æ–¹å’Œï¼Œ$S_{25}$ä»£è¡¨ç¬¬2ä¸ªè‡ªå˜é‡å’Œç¬¬5ä¸ªè‡ªå˜é‡çš„ç¦»å·®ç§¯å’Œï¼Œ$S_{25}$ä¸$S_{52}$æ˜¯ç›¸ç­‰çš„ï¼Œä»¥æ­¤ç±»æ¨ï¼›ç„¶åæ ¹æ®$S^{-1}$ï¼Œæ±‚å–é©¬æ°è·ç¦»çš„å¹³æ–¹å…¬å¼ä¸ºï¼š$D^{2} =[( x_{1}- \overline{ x_{1} }  )( x_{1}- \overline{ x_{1} }) S^{11} +( x_{1}- \overline{ x_{1} }  )( x_{2}- \overline{ x_{2} }) S^{12}]+ \ldots +( x_{1}- \overline{ x_{1} }  )( x_{p}- \overline{ x_{p} }) S^{1p}\\+( x_{2}- \overline{ x_{2} }  )( x_{1}- \overline{ x_{1} }) S^{21} +( x_{2}- \overline{ x_{2} }  )( x_{2}- \overline{ x_{2} }) S^{12}]+ \ldots +( x_{2}- \overline{ x_{2} }  )( x_{p}- \overline{ x_{p} }) S^{2p}\\ \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots  \ldots \\+( x_{p}- \overline{ x_{p} }  )( x_{1}- \overline{ x_{1} }) S^{p1} +( x_{p}- \overline{ x_{p} }  )( x_{2}- \overline{ x_{2} }) S^{12}]+ \ldots +( x_{p}- \overline{ x_{p} }  )( x_{p}- \overline{ x_{p} }) S^{pp}(n_s-1)$ï¼Œå…¶ä¸­$n_s$ä¸ºæ ·æœ¬ä¸ªæ•°ã€‚

äºŒæ˜¯ï¼Œä½¿ç”¨[statsmodels](https://www.statsmodels.org/stable/index.html)çš„`statsmodels.regression.linear_model.OLS`æ™®é€šæœ€å°äºŒä¹˜æ³•ï¼ˆOrdinary Least Squaresï¼ŒOLSï¼‰æ±‚å¾—å¤šå…ƒçº¿æ€§å›å½’æ–¹ç¨‹ï¼Œå…¶è¯­æ³•ç»“æ„ä¸SklearnåŸºæœ¬ç›¸åŒã€‚æ‰€æ±‚çš„çš„å›å½’æ¨¡å‹åŒ…å«æœ‰ç½®ä¿¡åŒºé—´çš„å±æ€§ï¼Œå¯ä»¥é€šè¿‡`dt=res.get_prediction(X).summary_frame(alpha=0.05)`çš„æ–¹å¼æå–ã€‚å¯ä»¥æ‰“å°statsmodelsè®¡ç®—æ‰€å¾—å›å½’æ¨¡å‹çš„æ¦‚è¦ï¼ˆsummaryï¼‰ï¼Œæ¯”è¾ƒæ±‚è§£å›å½’æ–¹ç¨‹çš„åå›å½’ç³»æ•°å’Œæˆªè·ï¼ˆcoef_const/area/distance_to_nearestStation ï¼‰ï¼Œä»¥åŠç¡®è®¤å¤šå…ƒå›å½’æ–¹ç¨‹çš„ç²¾åº¦R-squaredï¼ˆ$R^2$ï¼‰å’Œä¿®æ­£è‡ªç”±åº¦çš„åˆ¤å®šç³»æ•°Adj. R-squaredï¼Œå’Œå›å½’æ˜¾è‘—æ€§æ£€éªŒå…¨é¢è®¨è®ºåå›å½’ç³»æ•°çš„æ£€éªŒF-åˆ†å¸ƒç»Ÿè®¡é‡F-statisticï¼Œå¯¹åº”På€¼Prob (F-statistic)ï¼Œå…¨éƒ¨ç›¸ç­‰ï¼Œäº’ç›¸å°è¯äº†æ‰€ä½¿ç”¨çš„æ–¹æ³•æ˜¯å¦ä¿æŒä¸€è‡´ã€‚

å¯¹äºä¸¤ç§æ–¹æ³•åœ¨é¢„æµ‹å˜é‡ç½®ä¿¡åŒºé—´æ¯”è¾ƒä¸Šï¼Œåˆ†åˆ«æ‰“å°äº†å„è‡ªçš„ä¸‰ç»´åˆ†å¸ƒå›¾ï¼Œå…¶ç»“æœæ˜¾ç¤ºäºŒè€…çš„å›¾å½¢ä¿æŒä¸€è‡´ï¼Œå³é€šè¿‡statsmodelsæ±‚è§£å¤šå…ƒå›å½’æ–¹ç¨‹ä¸é€æ­¥è®¡ç®—æ‰€å¾—ç»“æœä¿æŒä¸€è‡´ã€‚

> [statsmodels](https://www.statsmodels.org/stable/index.html) æä¾›äº†ä¸€äº›ç±»å’Œå‡½æ•°ï¼Œç”¨äºä¼°è®¡è®¸å¤šä¸åŒçš„ç»Ÿè®¡æ¨¡å‹ï¼Œä»¥åŠæ‰§è¡Œç»Ÿè®¡æµ‹è¯•å’Œç»Ÿè®¡æ•°æ®ç ”ç©¶ã€‚æ¯ä¸ªä¼°è®¡å™¨éƒ½æœ‰ä¸€ä¸ªå¹¿æ³›çš„ç»“æœç»Ÿè®¡ä¿¡æ¯åˆ—è¡¨ï¼Œå¯ä»¥ç”¨ä»¥æŸ¥çœ‹ç›¸å…³ä¿¡æ¯ï¼Œä»¥ç¡®ä¿æ‰€æ±‚å¾—çš„ä¼°è®¡å™¨ï¼ˆæ¨¡å‹ï¼‰çš„å‡†ç¡®æ€§ã€æ­£ç¡®æ€§ã€‚


* é©¬æ°è·ç¦»ï¼ˆMahalanobis distanceï¼‰

é©¬æ°è·ç¦»è¡¨ç¤ºæ•°æ®çš„åæ–¹å·®çŸ©é˜µï¼Œæœ‰æ•ˆè®¡ç®—ä¸¤ä¸ªæœªçŸ¥æ ·æœ¬é›†ç›¸ä¼¼åº¦çš„æ–¹æ³•ã€‚ä¸æ¬§å¼è·ç¦»ï¼ˆEuclidean distanceï¼‰ä¸åŒçš„æ˜¯å®ƒè€ƒè™‘åˆ°å„ç§ç‰¹æ€§ä¹‹é—´çš„è”ç³»ï¼ˆä¾‹å¦‚èº«é«˜å’Œä½“é‡æ˜¯ç”±å…³è”çš„ï¼‰ï¼Œå¹¶ä¸”æ˜¯å°ºåº¦æ— å…³çš„ï¼ˆscale-invariantï¼Œä¾‹å¦‚å»æ‰å•ä½ï¼‰,ç‹¬ç«‹äºæµ‹é‡å°ºåº¦ã€‚è®¡ç®—å…¬å¼å¦‚ä¸Šæ‰€è¿°ï¼Œä¹Ÿå¯ä»¥ç®€åŒ–è¡¨ç¤ºä¸ºï¼Œå¯¹äºä¸€ä¸ªå‡å€¼ä¸º$ \vec{ \mu }= (  \mu _{1}, \mu _{2}, \mu _{3}, \ldots , \mu _{N} )^{T} $ï¼ˆå³ä¸ºå„ä¸ªè‡ªå˜é‡çš„å‡å€¼ï¼‰çš„å¤šå˜é‡ï¼ˆå¤šä¸ªè‡ªå˜é‡ï¼‰çš„çŸ©é˜µï¼Œ$ \vec{ x }= (  x_{1}, x _{2}, x _{3}, \ldots , x _{N} )^{T}$ï¼Œå…¶é©¬æ°è·ç¦»ä¸º$D_{M} (\vec{ x })= \sqrt{ (\vec{ x }-\vec{ \mu })^{T} S^{-1}  (\vec{ x }-\vec{ \mu })} $ã€‚


```python
import numpy as np
import statsmodels.api as sm

#ä½¿ç”¨statsmodelsåº“æ±‚è§£å›å½’æ–¹ç¨‹ï¼Œä¸è·å¾—é¢„æµ‹å€¼çš„ç½®ä¿¡åŒºé—´
storeInfo_df_sort=storeInfo_df.sort_values(by=['area'])
X=storeInfo_df_sort[['area','distance_to_nearestStation']]
X=sm.add_constant(X) #å› ä¸ºåœ¨ä¸Šè¿°é€æ­¥è®¡ç®—æˆ–è€…ä½¿ç”¨Sklearnæ±‚è§£å›å½’æ–¹ç¨‹è¿‡ç¨‹ä¸­ï¼Œå¤šå…ƒå›å½’æ–¹ç¨‹å‡å¢åŠ äº†å¸¸é‡æˆªè·çš„å‚æ•°ï¼Œå› æ­¤æ­¤å¤„å¢åŠ ä¸€ä¸ªå¸¸é‡ adding a constant
y=storeInfo_df_sort['monthly_turnover']
mod=sm.OLS(y,X) #æ„å»ºæœ€å°äºŒä¹˜æ¨¡å‹ Describe model
res=mod.fit() #æ‹Ÿåˆæ¨¡å‹ Fit model
print(res.summary())   # Summarize model

dt=res.get_prediction(X).summary_frame(alpha=0.05)
y_prd = dt['mean']
yprd_ci_lower = dt['obs_ci_lower']
yprd_ci_upper = dt['obs_ci_upper']
ym_ci_lower = dt['mean_ci_lower'] 
ym_ci_upper = dt['mean_ci_upper']

#é€æ­¥è®¡ç®—
def confidenceInterval_estimator_LR_multivariable(X,y,model,confidence=0.05):
    import pandas as pd
    from sympy import Matrix,pprint
    import numpy as np
    '''
    function - å¤šå…ƒçº¿æ€§å›å½’ç½®ä¿¡åŒºé—´ä¼°è®¡ï¼Œä»¥åŠé¢„æµ‹åŒºé—´
    
    Paras:
    X - æ ·æœ¬è‡ªå˜é‡ DataFrameæ•°æ®æ ¼å¼
    y - æ ·æœ¬å› å˜é‡
    model - å¤šå…ƒå›å½’æ¨¡å‹
    confidence - ç½®ä¿¡åº¦
    
    return:
    CI- é¢„æµ‹å€¼çš„ç½®ä¿¡åŒºé—´
    '''
    #æ ¹æ®æŒ‡å®šæ•°ç›®ï¼Œåˆ’åˆ†åˆ—è¡¨çš„å‡½æ•°
    def chunks(lst, n):
        for i in range(0, len(lst), n):
            yield lst[i:i + n]
    
    X_deepCopy=X.copy(deep=True) #å¦‚æœä¸è¿›è¡Œæ·±åº¦æ‹·è´ï¼Œå¦‚æœä¼ å…¥çš„å‚æ•°å˜é‡Xå‘ç”Ÿäº†æ”¹å˜ï¼Œåˆ™è¯¥å‡½æ•°å¤–éƒ¨çš„å˜é‡å€¼ä¹Ÿä¼šå‘ç”Ÿæ”¹å˜
    columns=X_deepCopy.columns
    n_v=len(columns)
    n_s=len(y)
    
    #æ±‚Sï¼Œç”¨äºé©¬æ°è·ç¦»çš„è®¡ç®—
    SD=[]
    SD_name=[]
    for col_i in columns:
        i=0
        for col_j in columns:
            SD_column_name=col_i+'S'+str(i)
            SD_name.append(SD_column_name)
            if col_i==col_j:
                X_deepCopy[SD_column_name]=X_deepCopy.apply(lambda row: (row[col_i]-X_deepCopy[col_j].mean())**2,axis=1)
                SD.append(X_deepCopy[SD_column_name].sum())
            else:
                X_deepCopy[SD_column_name]=X_deepCopy.apply(lambda row: (row[col_i]-X_deepCopy[col_i].mean())*(row[col_j]-X_deepCopy[col_j].mean()),axis=1)
                SD.append(X_deepCopy[SD_column_name].sum())                
            i+=1
    M=Matrix(list(chunks(SD,n_v)))
    
    #æ±‚Sçš„é€†çŸ©é˜µ
    M_invert=M**-1
    #pprint(M_invert)
    M_invert_list=[M_invert.row(row).col(col)[0] for row in range(n_v) for col in range(n_v)]
    X_mu=[X_deepCopy[col].mean() for col in columns]
    
    #æ±‚é©¬æ°è·ç¦»çš„å¹³æ–¹
    SD_array=X_deepCopy[SD_name].to_numpy()    
    D_square_list=[sum([x*y for x,y in zip(SD_selection,M_invert_list)])*(n_s-1) for SD_selection in SD_array]    
    
    #è®¡ç®—CI-é¢„æµ‹å€¼çš„ç½®ä¿¡åŒºé—´
    print(columns)
    ss_res=(y-model.predict(X_deepCopy[columns]))**2
    SS_res=ss_res.sum()
    print(SS_res)
    probability_val=f.ppf(q=1-confidence,dfn=1, dfd=n_s-n_v-1) 
    CI=[math.sqrt(probability_val*(1/n_s+D_square/(n_s-1))*SS_res/(n_s-n_v-1)) for D_square in D_square_list]

    return CI

X_=storeInfo_df_sort[['area','distance_to_nearestStation']]
y_=storeInfo_df_sort['monthly_turnover']
CI=confidenceInterval_estimator_LR_multivariable(X_,y_,LR_multivariate,confidence=0.05)

#æ‰“å°å›¾è¡¨
fig, axs=plt.subplots(1,2,figsize=(25,11))
x_=X.area
y_=X.distance_to_nearestStation

#ç”±è‡ªå®šä¹‰å‡½æ•°ï¼Œé€æ­¥è®¡ç®—è·å¾—çš„ç½®ä¿¡åŒºé—´
axs[0]=fig.add_subplot(1,2,1, projection='3d')
axs[0].plot(x_,y_, y, linestyle = "None", marker = "o",markerfacecolor = "None", color = "black",label = "actual")
X_array=X_.to_numpy()
LR_pre=LR_multivariate.predict(X_array)
axs[0].plot(x_, y_,LR_pre, color = "red",label = "prediction")
axs[0].plot(x_,y_, LR_pre+CI, color = "darkgreen", linestyle = "--", label = "Confidence Interval")
axs[0].plot(x_,y_, LR_pre-CI, color = "darkgreen", linestyle = "--")

#ç”±statsmodelsåº“è®¡ç®—æ‰€å¾—çš„ç½®ä¿¡åŒºé—´
axs[1]=fig.add_subplot(1,2,2, projection='3d')
axs[1].plot(x_,y_, y, linestyle = "None", marker = "o",markerfacecolor = "None", color = "black",label = "actual")
axs[1].plot(x_, y_,y_prd, color = "red",label = "OLS")
axs[1].plot(x_, y_,yprd_ci_lower, color = "blue", linestyle = "--",label = "Prediction Interval")
axs[1].plot(x_, y_,yprd_ci_upper, color = "blue", linestyle = "--")

axs[1].plot(x_,y_, ym_ci_lower, color = "darkgreen", linestyle = "--", label = "Confidence Interval")
axs[1].plot(x_,y_, ym_ci_upper, color = "darkgreen", linestyle = "--")

axs[1].view_init(210,250) #å¯ä»¥æ—‹è½¬å›¾å½¢çš„è§’åº¦ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
axs[1].set_xlabel('area')
axs[1].set_ylabel('distance_to_nearestStation')
axs[1].set_zlabel('confidence interval')

axs[0].legend()
axs[1].legend()
axs[0].view_init(210,250) #å¯ä»¥æ—‹è½¬å›¾å½¢çš„è§’åº¦ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
axs[1].view_init(210,250) #å¯ä»¥æ—‹è½¬å›¾å½¢çš„è§’åº¦ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
plt.show()
```

    C:\Users\richi\AppData\Roaming\Python\Python37\site-packages\scipy\stats\stats.py:1535: UserWarning:
    
    kurtosistest only valid for n>=20 ... continuing anyway, n=10
    
    

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:       monthly_turnover   R-squared:                       0.945
    Model:                            OLS   Adj. R-squared:                  0.930
    Method:                 Least Squares   F-statistic:                     60.41
    Date:                Sat, 25 Jul 2020   Prob (F-statistic):           3.84e-05
    Time:                        11:20:05   Log-Likelihood:                -44.358
    No. Observations:                  10   AIC:                             94.72
    Df Residuals:                       7   BIC:                             95.62
    Df Model:                           2                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================================
                                     coef    std err          t      P>|t|      [0.025      0.975]
    ----------------------------------------------------------------------------------------------
    const                         65.3239     55.738      1.172      0.280     -66.476     197.124
    area                          41.5135      6.256      6.636      0.000      26.720      56.307
    distance_to_nearestStation    -0.3409      0.078     -4.362      0.003      -0.526      -0.156
    ==============================================================================
    Omnibus:                        0.883   Durbin-Watson:                   3.440
    Prob(Omnibus):                  0.643   Jarque-Bera (JB):                0.448
    Skew:                           0.479   Prob(JB):                        0.799
    Kurtosis:                       2.603   Cond. No.                     1.40e+03
    ==============================================================================
    
    Warnings:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 1.4e+03. This might indicate that there are
    strong multicollinearity or other numerical problems.
    Index(['area', 'distance_to_nearestStation'], dtype='object')
    4173.006119994701
    


<a href=""><img src="./imgs/7_10.png" height="auto" width="auto" title="caDesign"></a>


### 1.4 è¦ç‚¹
#### 1.4.1 æ•°æ®å¤„ç†æŠ€æœ¯

* ä½¿ç”¨sympyåº“å»ºç«‹æ–¹ç¨‹ï¼Œæ±‚è§£æ–¹ç¨‹ç»„ï¼Œä»¥åŠå¾®åˆ†ã€çŸ©é˜µè®¡ç®—ï¼›ä½¿ç”¨sympyçš„pprintæ‰“å°æ–¹ç¨‹åŠå˜é‡

* ç”¨æœºå™¨å­¦ä¹ åº“[Sklearn](https://scikit-learn.org/stable/)ï¼Œä»¥åŠ[statsmodels](https://www.statsmodels.org/stable/index.html)æ±‚è§£å›å½’æ–¹ç¨‹ï¼Œä»¥åŠè®¡ç®—æ¨¡å‹ç²¾åº¦ï¼ˆåˆ¤å®šç³»æ•°ï¼‰ã€å›å½’æ¨¡å‹çš„æ˜¾è‘—æ€§æ£€éªŒã€‚

* ç”¨plotlyåº“çš„`px.scatter_ternary`ï¼Œæˆ–è€…matplotlibåº“çš„`projection='3d'`æ–¹å¼ï¼Œè¡¨è¿°æœ‰ä¸‰ä¸ªå˜é‡çš„å…³ç³»

#### 1.4.2 æ–°å»ºç«‹çš„å‡½æ•°

* function - åœ¨matplotlibçš„å­å›¾ä¸­ç»˜åˆ¶è¿æ¥çº¿ï¼Œ`demo_con_style(a_coordi,b_coordi,ax,connectionstyle)`

* function - å›å½’æ–¹ç¨‹çš„åˆ¤å®šç³»æ•°ï¼Œ `coefficient_of_determination(observed_vals,predicted_vals)`

* function - ç®€å•çº¿æ€§å›å½’æ–¹ç¨‹-å›å½’æ˜¾è‘—æ€§æ£€éªŒï¼ˆå›å½’ç³»æ•°æ£€éªŒï¼‰ï¼Œ `ANOVA(observed_vals,predicted_vals,df_reg,df_res)`

* function - ç®€å•çº¿æ€§å›å½’ç½®ä¿¡åŒºé—´ä¼°è®¡ï¼Œä»¥åŠé¢„æµ‹åŒºé—´ï¼Œ `confidenceInterval_estimator_LR(x,sample_num,X,y,model,confidence=0.05)`

* function - DataFrameæ•°æ®æ ¼å¼ï¼Œæˆç»„è®¡ç®—pearsonrç›¸å…³ç³»æ•°ï¼Œ`correlationAnalysis_multivarialbe(df)`

* function - å›å½’æ–¹ç¨‹çš„ä¿®æ­£è‡ªç”±åº¦çš„åˆ¤å®šç³»æ•°ï¼Œ `coefficient_of_determination_correction(observed_vals,predicted_vals,independent_variable_n)`

* function - å¤šå…ƒçº¿æ€§å›å½’æ–¹ç¨‹-å›å½’æ˜¾è‘—æ€§æ£€éªŒï¼ˆå›å½’ç³»æ•°æ£€éªŒï¼‰ï¼Œå…¨éƒ¨å›å½’ç³»æ•°çš„æ€»ä½“æ£€éªŒï¼Œä»¥åŠå•ä¸ªå›å½’ç³»æ•°çš„æ£€éªŒï¼Œ `ANOVA_multivarialbe(observed_vals,predicted_vals,independent_variable_n,a_i,X)`

* function - å¤šå…ƒçº¿æ€§å›å½’ç½®ä¿¡åŒºé—´ä¼°è®¡ï¼Œä»¥åŠé¢„æµ‹åŒºé—´ï¼Œ `confidenceInterval_estimator_LR_multivariable(X,y,model,confidence=0.05)`

#### 1.4.3 æ‰€è°ƒç”¨çš„åº“


```python
import sympy
from sympy import Symbol
from sympy import diff
from sympy import init_printing,pprint,sqrt
from sympy import ln,log,Eq
from sympy import Matrix
from sympy import Eq,solveset,solve,simplify
from sympy import latex

import numpy as np
import seaborn as sns
import plotly.express as px

import matplotlib.pyplot as plt
from matplotlib import cm

import math
import pandas as pd

from scipy import stats
from scipy.stats import f
from scipy.stats import pearsonr

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn import preprocessing
from sklearn.metrics import r2_score

import statsmodels.api as sm
```

#### 1.4.4 å‚è€ƒæ–‡çŒ®
1. [æ—¥]é«˜æ¡¥ ä¿¡è‘—ä½œ,Inoue Iroha,æ ªå¼ä¼šç¤¾ TREND-PROæ¼«ç”»åˆ¶ä½œ,å¼ ä»²æ’è¯‘.æ¼«ç”»ç»Ÿè®¡å­¦ä¹‹å›å½’åˆ†æ[M].ç§‘å­¦å‡ºç‰ˆç¤¾.åŒ—äº¬.2009.08ï¼›
2. Timothy C.Urdan.Statistics in Plain English(ç™½è¯ç»Ÿè®¡å­¦)[M].ä¸­å›½äººæ°‘å¤§å­¦å‡ºç‰ˆç¤¾.2013,12.ç¬¬3ç‰ˆ.
3. Douglas C.Montgomery,Elizabeth A.Peck,G.Geoffrey Viningè‘—.ç‹è¾°å‹‡è¯‘.çº¿æ€§å›å½’åˆ†æå¯¼è®º(Introduction to linear regression analysis).æœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾.2016.04(ç¬¬5ç‰ˆ)

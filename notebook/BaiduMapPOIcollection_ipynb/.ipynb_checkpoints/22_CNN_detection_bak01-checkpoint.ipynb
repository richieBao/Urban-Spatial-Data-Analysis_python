{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Created on Tue Jan 12 13:48:15 2021 @author: Richie Bao-caDesign设计(cadesign.cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 卷积神经网络，目标检测，人流量监测\n",
    "### 1.1 卷积神经网络(Convolutional neural network, CNN)\n",
    "#### 1.1.1 卷积原理与卷积神经网络\n",
    "在阅读卷积神经网络(CNNs)之前，如果已经阅读‘卷积’，‘计算机视觉，特征提取和尺度空间’等部分章节，可以更好的理解CNNs。其中‘尺度空间’的概念，通过降采样的不同空间分辨率影像（类似池化，pooLing）和不同$\\sigma$值的高斯核卷积(即卷积计算或称为互相关运算)，来提取图像的概貌特征，这与CNNs的多层卷积网络如出一辙，只是CNNs除了卷积层，还可以自由加入其它的数据处理层，提升图像特征捕捉的几率。\n",
    "\n",
    "一幅图像的意义来自于邻近的一组像素，而不是单个像素自身，因为单个像素并不包含关于整个图像的信息。例如下图(引自*PyTorch Artificial Intelligence Fundamentals*)很好的说明了这两者的差异。一个全连接的神经网络（密集层，dense layer），一层的每个节点都连接到下一层的每个节点，并不能很好的反映节点之间的关系，也具有较大的计算量；但是卷积网络利用像素之间的空间结构减少了层之间的连接数量，显著提高了训练的速度，减少了模型的参数，更重要的是反映了图像特征像素之间的关系，即图像内容中各个对象是由自身与邻近像素关系决定的空间结构（即特征）所表述。\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/22_01.jpg\" height='auto' width='700' title=\"caDesign\"></a>\n",
    "\n",
    "二维卷积层的卷积计算，就是表征图像的数组shape(c,h,w)（如果为灰度图像则只有一个颜色通道，如果是彩色图像(RGB)则有三个图像通道），每个通道(h,w)二维数组中每一像素，与卷积核(filter/kernal)的加权和计算。这个过程存在一些可变动的因素：一个是步幅(stride)，卷积窗口（卷积核）从输入数组（二维图像数组）的最左上方开始，按从左到右，从上往下的顺序，依次在输入数组上滑动，每次滑动的行数和列数称为步幅。步幅越小代表扑捉不同内容对象的精度越高。下述图表（参考[*Convolutional Neural Networks*](https://cs231n.github.io/convolutional-networks/)）卷积核的滑动步幅为2，即行列均跨2步后计算；二是填充(padding)，如果不设置填充，并且从左上角滑动，会有一部分卷积核对应空值（没有图像/数据）。同时要保持四周填充的行列数相同，通常使用奇数高宽的卷积核；三是，如果是对图像数据实现卷积，通常包括1个单通道，或3个多通道的情况。对于多通道的计算是可以配置不同的卷积核对应不同的通道，各自通道分别卷积后，计算和（累加）作为结果输出。同时可以增加并行的新的卷积计算，获取多个输出，例如下图的Filter W0，和Filter W1的($3 \\times 3 \\times 3$)卷积核，W0和W1各自包含3个卷积核对应3个通道输入，并各自输出。\n",
    "\n",
    "卷积运算，步幅、填充配置，以及多通道卷积都没有改变图像的空间尺寸（空间分辨率），即尺度空间概念下降采样的表述（可以反映不同对象的尺度大小，或理解为只有在不同的尺度下才可以捕捉到对象的特征）。池化层(pooling)正是降采样在卷积神经网络中的表述，可以降低输入的空间维数，保留输入的深度。在图像卷积过程中，识别出比实际像素信息更多的概念意义，识别保留输入的关键信息，丢弃冗余部分。池化层不仅捕捉尺度空间下的对象特征，同时可以减少训练所需时间，减小模型的参数数量，降低模型复杂度，更好的泛化等。池化层可以用`nn.MaxPool2d`，取最大值；`nn.AvgPool2d`，取均值等方法。\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/22_05.jpg\" height='auto' width='auto' title=\"caDesign\"></a>\n",
    "\n",
    "\n",
    ">  参考文献：\n",
    "1. Jibin Mathew.PyTorch Artificial Intelligence Fundamentals: A recipe-based approach to design, build and deploy your own AI models with PyTorch 1.x[m].UK:Packt Publishing (February 28, 2020)\n",
    "2. Aston Zhang,Zack C. Lipton,Mu Li,etc.Dive into Deep Learning[M].；中文版-阿斯顿.张,李沐,扎卡里.C. 立顿,亚历山大.J. 斯莫拉.动手深度学习[M].人民邮电出版社,北京,2019-06-01\n",
    "3. [Convolutional Neural Networks (CNNs / ConvNets)](https://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "建立图表中的输入数据`t_input`，张量形状为(batchsize, nChannels, Height, Width)=(1,3,7,7)，即只有一幅图像，通道数为3，高度为7，宽度为7。通常查看时，只要确定最后一维内的数据为图像每一行的像素值（由上至下）。PyTorch提供的`nn.Conv2d`卷积方法不需自定义卷积核，其参数为`torch.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], stride: Union[T, Tuple[T, T]] = 1, padding: Union[T, Tuple[T, T]] = 0, dilation: Union[T, Tuple[T, T]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')`。如果需要自定义卷积核，则调用`torch.nn.functional.conv2d`方法，其中`weight`参数即为卷积核，其输入参数为`torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_input.shape=torch.Size([1, 3, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0307, -0.5488, -0.1162],\n",
       "          [-0.5028, -0.9719, -0.9944],\n",
       "          [-0.4138, -0.5809,  0.0107]],\n",
       "\n",
       "         [[-0.0583,  0.4393,  0.1049],\n",
       "          [ 0.2772,  0.4981, -0.4585],\n",
       "          [-0.2310,  0.5467,  0.4913]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#batchsize, nChannels, Height, Width\n",
    "t_input=torch.tensor([[[[0,0,0,0,0,0,0],[0,2,2,2,0,0,0],[0,1,0,2,0,0,0],[0,0,0,1,0,1,0],[0,1,1,1,1,0,0],[0,2,2,0,0,0,0],[0,0,0,0,0,0,0]],\n",
    "                      [[0,0,0,0,0,0,0],[0,1,2,1,1,1,0],[0,2,1,0,1,1,0],[0,0,0,0,0,1,0],[0,2,0,2,1,1,0],[0,0,2,0,1,2,0],[0,0,0,0,0,0,0]],\n",
    "                      [[0,0,0,0,0,0,0],[0,1,0,0,1,1,0],[0,1,0,2,0,1,0],[0,2,0,1,2,1,0],[0,1,2,1,2,2,0],[0,1,1,2,0,0,0],[0,0,0,0,0,0,0]],\n",
    "                     ]],dtype=torch.float)\n",
    "print(\"t_input.shape={}\".format(t_input.shape))\n",
    "conv_2d_3c=nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3,stride=2,padding=0,bias=1)\n",
    "conv_2d_3c(t_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0., -2., -1.],\n",
      "          [-4.,  1., -2.],\n",
      "          [ 2., -4.,  0.]]]])\n",
      "tensor([[[[7., 7., 3.],\n",
      "          [3., 4., 4.],\n",
      "          [1., 2., 2.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "w_0=torch.tensor([[[[-1, 1, 0],\n",
    "                    [ 0,-1, 0],\n",
    "                    [-1,-1, 1]],\n",
    "\n",
    "                   [[0,-1,-1],\n",
    "                    [-1,1,1],\n",
    "                    [-1,-1,1]],\n",
    "\n",
    "                   [[-1,1,0],\n",
    "                    [0,0,1],\n",
    "                    [0,0,-1]]]],dtype=torch.float)\n",
    "\n",
    "w_1=torch.tensor([[[[0, -1, -1],\n",
    "                    [ 1,-1, 1],\n",
    "                    [1,1, 1]],\n",
    "\n",
    "                   [[0,0,1],\n",
    "                    [0,1,1],\n",
    "                    [0,1,0]],\n",
    "\n",
    "                   [[0,0,0],\n",
    "                    [0,0,1],\n",
    "                    [0,1,-1]]]],dtype=torch.float)\n",
    "\n",
    "b_0=torch.tensor([1])\n",
    "b_1=torch.tensor([0])\n",
    "output_0=F.conv2d(input=t_input,weight=w_0,stride=2,padding=0,bias=b_0)\n",
    "output_1=F.conv2d(input=t_input,weight=w_1,stride=2,padding=0,bias=b_1)\n",
    "print(output_0)\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大池化，即设定池化（卷积核）大小，返回覆盖范围内最大值，其参数为`torch.nn.MaxPool2d(kernel_size: Union[T, Tuple[T, ...]], stride: Optional[Union[T, Tuple[T, ...]]] = None, padding: Union[T, Tuple[T, ...]] = 0, dilation: Union[T, Tuple[T, ...]] = 1, return_indices: bool = False, ceil_mode: bool = False)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据形状=torch.Size([1, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [2., 1.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling_input=torch.tensor([[[0,-2,-1],[-4,1,-2],[2,-4,0]]],dtype=torch.float)\n",
    "print(\"输入数据形状={}\".format(pooling_input.shape))\n",
    "\n",
    "maxPool_2d=nn.MaxPool2d(kernel_size=2,stride=1)\n",
    "maxPool_2d(pooling_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 卷积_特征提取器--->_分类器，可视化特征，及tensorboard\n",
    "\n",
    "* 卷积层，池化层输出尺寸（形状）计算，及根据输入输出尺寸反推填充pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2d_output_size_calculation:\n",
    "    '''\n",
    "    class - PyTorch 卷积层，池化层输出尺寸(shape)计算，及根据输入，输出尺寸(shape)反推pad填充大小\n",
    "    \n",
    "    @author:sytelus Shital Shah\n",
    "    Updated on Tue Jan 12 19:17:22 2021 @author: Richie Bao-caDesign设计(cadesign.cn)\n",
    "    '''   \n",
    "    \n",
    "    def num2tuple(self,num):\n",
    "        '''\n",
    "        function - 如果num=2，则返回(2,2)；如果num=(2,2)，则返回(2,2).\n",
    "        '''\n",
    "        return num if isinstance(num, tuple) else (num, num)\n",
    "    \n",
    "    def conv2d_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "        import math\n",
    "        '''\n",
    "        funciton - 计算PyTorch的nn.Conv2d卷积方法的输出尺寸。以卷积核左上角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算\n",
    "        '''\n",
    "        \n",
    "        h_w, kernel_size, stride, pad, dilation = self.num2tuple(h_w), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(pad), self.num2tuple(dilation)\n",
    "        pad = self.num2tuple(pad[0]), self.num2tuple(pad[1])\n",
    "        \n",
    "        h = math.floor((h_w[0] + sum(pad[0]) - dilation[0]*(kernel_size[0]-1) - 1) / stride[0] + 1)\n",
    "        w = math.floor((h_w[1] + sum(pad[1]) - dilation[1]*(kernel_size[1]-1) - 1) / stride[1] + 1)\n",
    "        \n",
    "        return h, w\n",
    "    \n",
    "    def convtransp2d_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1, out_pad=0):\n",
    "        '''\n",
    "        function - 以卷积核右下角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算\n",
    "        '''\n",
    "        h_w, kernel_size, stride, pad, dilation, out_pad = self.num2tuple(h_w), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(pad), self.num2tuple(dilation), self.num2tuple(out_pad)\n",
    "        pad = self.num2tuple(pad[0]), self.num2tuple(pad[1])\n",
    "        \n",
    "        h = (h_w[0] - 1)*stride[0] - sum(pad[0]) + dilation[0]*(kernel_size[0]-1) + out_pad[0] + 1\n",
    "        w = (h_w[1] - 1)*stride[1] - sum(pad[1]) + dilation[1]*(kernel_size[1]-1) + out_pad[1] + 1\n",
    "        \n",
    "        return h, w\n",
    "    \n",
    "    def conv2d_get_padding(self,h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1):\n",
    "        import math\n",
    "        '''\n",
    "        function - conv2d_output_shape 方法的逆，求填充pad\n",
    "        '''\n",
    "        h_w_in, h_w_out, kernel_size, stride, dilation = self.num2tuple(h_w_in), self.num2tuple(h_w_out), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(dilation)\n",
    "        \n",
    "        p_h = ((h_w_out[0] - 1)*stride[0] - h_w_in[0] + dilation[0]*(kernel_size[0]-1) + 1)\n",
    "        p_w = ((h_w_out[1] - 1)*stride[1] - h_w_in[1] + dilation[1]*(kernel_size[1]-1) + 1)\n",
    "        \n",
    "        return (math.floor(p_h/2), math.ceil(p_h/2)), (math.floor(p_w/2), math.ceil(p_w/2))  #((pad_up, pad_bottom)， (pad_left, pad_right))\n",
    "    \n",
    "    def convtransp2d_get_padding(self,h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1, out_pad=0):\n",
    "        import math\n",
    "        '''\n",
    "        function - convtransp2d_output_shape 方法的逆，求填充pad\n",
    "        '''\n",
    "        h_w_in, h_w_out, kernel_size, stride, dilation, out_pad = self.num2tuple(h_w_in), self.num2tuple(h_w_out), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(dilation), self.num2tuple(out_pad)\n",
    "            \n",
    "        p_h = -(h_w_out[0] - 1 - out_pad[0] - dilation[0]*(kernel_size[0]-1) - (h_w_in[0] - 1)*stride[0]) / 2\n",
    "        p_w = -(h_w_out[1] - 1 - out_pad[1] - dilation[1]*(kernel_size[1]-1) - (h_w_in[1] - 1)*stride[1]) / 2\n",
    "        \n",
    "        return (math.floor(p_h/2), math.ceil(p_h/2)), (math.floor(p_w/2), math.ceil(p_w/2))\n",
    "    \n",
    "    def pooling_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "        '''\n",
    "        function - pooling池化层输出尺寸，同conv2d_output_shape\n",
    "        '''\n",
    "        return conv2d_output_shape(h_w, kernel_size=kernel_size, stride=stride, pad=pad, dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_size=(24, 24)\n",
      "size_pool1=(12, 12)\n",
      "size_conv2=(8, 8)\n",
      "size_pool2=(4, 4)\n"
     ]
    }
   ],
   "source": [
    "conv2dSize_cal=conv2d_output_size_calculation()\n",
    "size_conv1=conv2dSize_cal.conv2d_output_shape(28, kernel_size=5, stride=1, pad=0, dilation=1)\n",
    "size_pool1=conv2dSize_cal.pooling_output_shape(24, kernel_size=2, stride=2, pad=0, dilation=1)\n",
    "size_conv2=conv2dSize_cal.conv2d_output_shape(12, kernel_size=5, stride=1, pad=0, dilation=1)\n",
    "size_pool2=conv2dSize_cal.pooling_output_shape(8, kernel_size=2, stride=2, pad=0, dilation=1)\n",
    "\n",
    "print(\"conv1_size={}\\nsize_pool1={}\\nsize_conv2={}\\nsize_pool2={}\".format(conv1_size,size_pool1,size_conv2,size_pool2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Visualizing Models, Data, and Training with TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html?highlight=fashion%20mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#读取（下载）FashionMNIST数据。如果已经下载，则直接读取\n",
    "transform=transforms.Compose(\n",
    "                                [transforms.ToTensor(), #转换PIL图像或numpy.ndarray为tensor张量\n",
    "                                transforms.Normalize((0.5,), (0.5,))] #torchvision.transforms.Normalize(mean, std, inplace=False)，用均值和标准差，标准化张量图像\n",
    "                            )\n",
    "mnist_train=torchvision.datasets.FashionMNIST(root='./datasets/FashionMNIST_norm', train=True, download=True, transform=transform) \n",
    "mnist_test=torchvision.datasets.FashionMNIST(root='./datasets/FashionMNIST_norm', train=False, download=True, transform=transform)\n",
    "\n",
    "#DataLoade-读取小批量\n",
    "import torch.utils.data as data_utils\n",
    "batch_size=4\n",
    "num_workers=2\n",
    "trainloader=data_utils.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "testloader=data_utils.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "def fashionMNIST_show_v2(img,figsize=(5,5),one_channel=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    '''\n",
    "    function - 显示单张图像\n",
    "    '''\n",
    "    if one_channel:\n",
    "        img=img.mean(dim=0)\n",
    "    img=img/2+0.5 #逆标准化, unnormalize\n",
    "    npimg=img.numpy()\n",
    "    print(\"image size={}\".format(npimg.shape))\n",
    "    plt.figure(figsize=figsize)\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg,cmap='Greys')\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "\n",
    "fashionMNIST_show_v2(mnist_test[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class net_fashionMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_fashionMNIST,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,6,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*4*4,120) #torch.nn.Linear(in_features: int, out_features: int, bias: bool = True)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relul(self.conv2(x)))\n",
    "        x=x.view(-1,16*4*4)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "net_fashionMNIST=net_fashionMNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(net_fashionMNIST.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer=SummaryWriter(r'./runs/fashion_mnist_experiment_1')\n",
    "\n",
    "#提取图像（数量为batch_size） get some random training images\n",
    "dataiter=iter(trainloader)\n",
    "images,labels=dataiter.next()\n",
    "#建立图像格网 create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "#显示图像 show images\n",
    "fashionMNIST_show_v2(img_grid, one_channel=True,figsize=(10,10))\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net_fashionMNIST,images)\n",
    "writer.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000015EB64BFBA0>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_fashionMNIST.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

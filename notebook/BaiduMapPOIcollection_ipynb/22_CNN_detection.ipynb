{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Created on Tue Jan 12 13:48:15 2021 @author: Richie Bao-caDesign设计(cadesign.cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 卷积神经网络，可视化卷积层/卷积核,tensorboard,torchvision.models与VGG网络\n",
    "### 1.1 卷积神经网络(Convolutional neural network, CNN)—— 卷积原理与卷积神经网络\n",
    "在阅读卷积神经网络(CNNs)之前，如果已经阅读‘卷积’，‘计算机视觉，特征提取和尺度空间’等部分章节，可以更好的理解CNNs。其中‘尺度空间’的概念，通过降采样的不同空间分辨率影像（类似池化，pooLing）和不同$\\sigma$值的高斯核卷积(即卷积计算或称为互相关运算)，来提取图像的概貌特征，这与CNNs的多层卷积网络如出一辙，只是CNNs除了卷积层，还可以自由加入其它的数据处理层，提升图像特征捕捉的几率。\n",
    "\n",
    "一幅图像的意义来自于邻近的一组像素，而不是单个像素自身，因为单个像素并不包含关于整个图像的信息。例如下图(引自*PyTorch Artificial Intelligence Fundamentals*)很好的说明了这两者的差异。一个全连接的神经网络（密集层，dense layer），一层的每个节点都连接到下一层的每个节点，并不能很好的反映节点之间的关系，也具有较大的计算量；但是卷积网络利用像素之间的空间结构减少了层之间的连接数量，显著提高了训练的速度，减少了模型的参数，更重要的是反映了图像特征像素之间的关系，即图像内容中各个对象是由自身与邻近像素关系决定的空间结构（即特征）所表述。\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/22_01.jpg\" height='auto' width='700' title=\"caDesign\"></a>\n",
    "\n",
    "二维卷积层的卷积计算，就是表征图像的数组shape(c,h,w)（如果为灰度图像则只有一个颜色通道，如果是彩色图像(RGB)则有三个图像通道），每个通道(h,w)二维数组中每一像素，与卷积核(filter/kernal)的加权和计算。这个过程存在一些可变动的因素：一个是步幅(stride)，卷积窗口（卷积核）从输入数组（二维图像数组）的最左上方开始，按从左到右，从上往下的顺序，依次在输入数组上滑动，每次滑动的行数和列数称为步幅。步幅越小代表扑捉不同内容对象的精度越高。下述图表（参考[*Convolutional Neural Networks*](https://cs231n.github.io/convolutional-networks/)）卷积核的滑动步幅为2，即行列均跨2步后计算；二是填充(padding)，如果不设置填充，并且从左上角滑动，会有一部分卷积核对应空值（没有图像/数据）。同时要保持四周填充的行列数相同，通常使用奇数高宽的卷积核；三是，如果是对图像数据实现卷积，通常包括1个单通道，或3个多通道的情况。对于多通道的计算是可以配置不同的卷积核对应不同的通道，各自通道分别卷积后，计算和（累加）作为结果输出。同时可以增加并行的新的卷积计算，获取多个输出，例如下图的Filter W0，和Filter W1的($3 \\times 3$)卷积核，W0和W1各自包含3个卷积核对应3个通道输入，并各自输出。\n",
    "\n",
    "卷积运算，步幅、填充配置，以及多通道卷积都没有改变图像的空间尺寸（空间分辨率），即尺度空间概念下降采样的表述（可以反映不同对象的尺度大小，或理解为只有在不同的尺度下才可以捕捉到对象的特征）。池化层(pooling)正是降采样在卷积神经网络中的表述，可以降低输入的空间维数，保留输入的深度。在图像卷积过程中，识别出比实际像素信息更多的概念意义，识别保留输入的关键信息，丢弃冗余部分。池化层不仅捕捉尺度空间下的对象特征，同时可以减少训练所需时间，减小模型的参数数量，降低模型复杂度，更好的泛化等。池化层可以用`nn.MaxPool2d`，取最大值；`nn.AvgPool2d`，取均值等方法。\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/22_05.jpg\" height='auto' width='auto' title=\"caDesign\"></a>\n",
    "\n",
    "卷积层和池化层都有一个`dilation`参数，可以翻译为膨胀。通过dilation配置，可以调整卷积核与图像的作用域，即感受野（receptive filed）。当$3 \\times 3$的卷积核dilation=1时，即没有膨胀效应；当dilation=2时，感受野扩展至$7 \\times 7$；当dilation=24时，感受野扩展至$15 \\times 15$。可以确定当dilation线性增加时，其感受野时呈指数增加。\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/22_11.jpg\" height='auto' width='1000' title=\"caDesign\"></a>\n",
    "\n",
    "\n",
    ">  参考文献：\n",
    "1. Jibin Mathew.PyTorch Artificial Intelligence Fundamentals: A recipe-based approach to design, build and deploy your own AI models with PyTorch 1.x[m].UK:Packt Publishing (February 28, 2020)\n",
    "2. Aston Zhang,Zack C. Lipton,Mu Li,etc.Dive into Deep Learning[M].；中文版-阿斯顿.张,李沐,扎卡里.C. 立顿,亚历山大.J. 斯莫拉.动手深度学习[M].人民邮电出版社,北京,2019-06-01\n",
    "3. [Convolutional Neural Networks (CNNs / ConvNets)](https://cs231n.github.io/convolutional-networks/)\n",
    "4. [Understanding 2D Dilated Convolution Operation with Examples in Numpy and Tensorflow with Interactive Code](https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25)\n",
    "\n",
    "建立图表中的输入数据`t_input`，张量形状为(batchsize, nChannels, Height, Width)=(1,3,7,7)，即只有一幅图像，通道数为3，高度为7，宽度为7。通常查看时，只要确定最后一维内的数据为图像每一行的像素值（由上至下）。PyTorch提供的`nn.Conv2d`卷积方法不需自定义卷积核，其参数为`torch.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], stride: Union[T, Tuple[T, T]] = 1, padding: Union[T, Tuple[T, T]] = 0, dilation: Union[T, Tuple[T, T]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')`。如果需要自定义卷积核，则调用`torch.nn.functional.conv2d`方法，其中`weight`参数即为卷积核，其输入参数为`torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_input.shape=torch.Size([1, 3, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1740, -0.2426,  0.2305],\n",
       "          [ 0.2810,  0.3236,  0.4322],\n",
       "          [ 0.0234, -0.0917, -0.8094]],\n",
       "\n",
       "         [[-0.1936, -0.0077,  0.0944],\n",
       "          [ 0.1876, -0.2772, -0.5696],\n",
       "          [-0.1700,  0.2354, -0.5344]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#batchsize, nChannels, Height, Width\n",
    "t_input=torch.tensor([[[[0,0,0,0,0,0,0],[0,2,2,2,0,0,0],[0,1,0,2,0,0,0],[0,0,0,1,0,1,0],[0,1,1,1,1,0,0],[0,2,2,0,0,0,0],[0,0,0,0,0,0,0]],\n",
    "                      [[0,0,0,0,0,0,0],[0,1,2,1,1,1,0],[0,2,1,0,1,1,0],[0,0,0,0,0,1,0],[0,2,0,2,1,1,0],[0,0,2,0,1,2,0],[0,0,0,0,0,0,0]],\n",
    "                      [[0,0,0,0,0,0,0],[0,1,0,0,1,1,0],[0,1,0,2,0,1,0],[0,2,0,1,2,1,0],[0,1,2,1,2,2,0],[0,1,1,2,0,0,0],[0,0,0,0,0,0,0]],\n",
    "                     ]],dtype=torch.float)\n",
    "print(\"t_input.shape={}\".format(t_input.shape))\n",
    "conv_2d_3c=nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3,stride=2,padding=0,bias=1)\n",
    "conv_2d_3c(t_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0., -2., -1.],\n",
      "          [-4.,  1., -2.],\n",
      "          [ 2., -4.,  0.]]]])\n",
      "tensor([[[[7., 7., 3.],\n",
      "          [3., 4., 4.],\n",
      "          [1., 2., 2.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "w_0=torch.tensor([[[[-1, 1, 0],\n",
    "                    [ 0,-1, 0],\n",
    "                    [-1,-1, 1]],\n",
    "\n",
    "                   [[0,-1,-1],\n",
    "                    [-1,1,1],\n",
    "                    [-1,-1,1]],\n",
    "\n",
    "                   [[-1,1,0],\n",
    "                    [0,0,1],\n",
    "                    [0,0,-1]]]],dtype=torch.float)\n",
    "\n",
    "w_1=torch.tensor([[[[0, -1, -1],\n",
    "                    [ 1,-1, 1],\n",
    "                    [1,1, 1]],\n",
    "\n",
    "                   [[0,0,1],\n",
    "                    [0,1,1],\n",
    "                    [0,1,0]],\n",
    "\n",
    "                   [[0,0,0],\n",
    "                    [0,0,1],\n",
    "                    [0,1,-1]]]],dtype=torch.float)\n",
    "\n",
    "b_0=torch.tensor([1])\n",
    "b_1=torch.tensor([0])\n",
    "output_0=F.conv2d(input=t_input,weight=w_0,stride=2,padding=0,bias=b_0)\n",
    "output_1=F.conv2d(input=t_input,weight=w_1,stride=2,padding=0,bias=b_1)\n",
    "print(output_0)\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大池化，即设定池化（卷积核）大小，返回覆盖范围内最大值，其参数为`torch.nn.MaxPool2d(kernel_size: Union[T, Tuple[T, ...]], stride: Optional[Union[T, Tuple[T, ...]]] = None, padding: Union[T, Tuple[T, ...]] = 0, dilation: Union[T, Tuple[T, ...]] = 1, return_indices: bool = False, ceil_mode: bool = False)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据形状=torch.Size([1, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [2., 1.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling_input=torch.tensor([[[0,-2,-1],[-4,1,-2],[2,-4,0]]],dtype=torch.float)\n",
    "print(\"输入数据形状={}\".format(pooling_input.shape))\n",
    "\n",
    "maxPool_2d=nn.MaxPool2d(kernel_size=2,stride=1)\n",
    "maxPool_2d(pooling_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 卷积_特征提取器--->_分类器，可视化卷积层/卷积核，及tensorboard\n",
    "\n",
    "#### 1.2.1 卷积层，池化层输出尺寸（形状）计算，及根据输入输出尺寸反推填充pad\n",
    "\n",
    "构建卷积神经网络，很重要的一步是确定图像经过一次或多次卷积后，输出的尺寸，用于分类器（线性模型，全连接层）的输入。在PyTorch的['torch.nn.Conv2d'](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)类方法说明中都会给出卷积输出的尺寸计算公式，但是手工的计算方式容易出错，并且耗时耗力，因此通常将其编写为代码程序。池化层的输出尺寸计算实际上与卷积的输出相同，但是为了区分，仍然在`conv2d_output_size_calculation`类中，增加了` pooling_output_shape`方法（直接调用`conv2d_output_shape`）。卷积的方式除了给定h_w/图像高宽，kernel_size/卷积核（过滤器，filter）,stride/步幅，pad/填充，以及dilation/膨胀参数，卷积核的初始位置可以分为：以卷积核左上角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算，即`conv2d_output_shape`函数； 以卷积核右下角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算，即`convtransp2d_output_shape`函数。\n",
    "\n",
    "同时，也可以根据卷积的输入和输出的尺寸，反推填充的大小，对应卷积核初始位置的不同分别为`conv2d_get_padding`和`convtransp2d_get_padding`函数。\n",
    "\n",
    "对于卷积神经网络'net_fashionMNIST'的网络结构如下：\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/22_06.jpg\" height='auto' width='1000' title=\"caDesign\"></a>\n",
    "\n",
    "输入图像的大小为$28 \\times 28$，经过卷积层conv1-->池化层pool-->卷积层conv2-->池化层pool后，图像尺寸的大小变为$4 \\times 4$，该值（卷积层的输出值）由自定义`conv2d_output_size_calculation`计算。注意两次池化，其参数值相同，因此卷积神经网络结构定义时，可以仅定义一个池化方法，用于生成不同位置的池化层。计算应用卷积提取特征部分的图像输出大小后，因为conv2+pool之后，其输出的'out_channels'输出通道数配置为16，因此到分类器部分（线性函数/全连接层/展平层）的输入大小为$16 \\times 4 \\times 4 = 256$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2d_output_size_calculation:\n",
    "    '''\n",
    "    class - PyTorch 卷积层，池化层输出尺寸(shape)计算，及根据输入，输出尺寸(shape)反推pad填充大小\n",
    "    \n",
    "    @author:sytelus Shital Shah\n",
    "    Updated on Tue Jan 12 19:17:22 2021 @author: Richie Bao-caDesign设计(cadesign.cn)\n",
    "    '''   \n",
    "    \n",
    "    def num2tuple(self,num):\n",
    "        '''\n",
    "        function - 如果num=2，则返回(2,2)；如果num=(2,2)，则返回(2,2).\n",
    "        '''\n",
    "        return num if isinstance(num, tuple) else (num, num)\n",
    "    \n",
    "    def conv2d_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "        import math\n",
    "        '''\n",
    "        funciton - 计算PyTorch的nn.Conv2d卷积方法的输出尺寸。以卷积核左上角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算\n",
    "        '''\n",
    "        \n",
    "        h_w, kernel_size, stride, pad, dilation = self.num2tuple(h_w), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(pad), self.num2tuple(dilation)\n",
    "        pad = self.num2tuple(pad[0]), self.num2tuple(pad[1])\n",
    "        \n",
    "        h = math.floor((h_w[0] + sum(pad[0]) - dilation[0]*(kernel_size[0]-1) - 1) / stride[0] + 1)\n",
    "        w = math.floor((h_w[1] + sum(pad[1]) - dilation[1]*(kernel_size[1]-1) - 1) / stride[1] + 1)\n",
    "        \n",
    "        return h, w\n",
    "    \n",
    "    def convtransp2d_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1, out_pad=0):\n",
    "        '''\n",
    "        function - 以卷积核右下角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算\n",
    "        '''\n",
    "        h_w, kernel_size, stride, pad, dilation, out_pad = self.num2tuple(h_w), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(pad), self.num2tuple(dilation), self.num2tuple(out_pad)\n",
    "        pad = self.num2tuple(pad[0]), self.num2tuple(pad[1])\n",
    "        \n",
    "        h = (h_w[0] - 1)*stride[0] - sum(pad[0]) + dilation[0]*(kernel_size[0]-1) + out_pad[0] + 1\n",
    "        w = (h_w[1] - 1)*stride[1] - sum(pad[1]) + dilation[1]*(kernel_size[1]-1) + out_pad[1] + 1\n",
    "        \n",
    "        return h, w\n",
    "    \n",
    "    def conv2d_get_padding(self,h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1):\n",
    "        import math\n",
    "        '''\n",
    "        function - conv2d_output_shape 方法的逆，求填充pad\n",
    "        '''\n",
    "        h_w_in, h_w_out, kernel_size, stride, dilation = self.num2tuple(h_w_in), self.num2tuple(h_w_out), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(dilation)\n",
    "        \n",
    "        p_h = ((h_w_out[0] - 1)*stride[0] - h_w_in[0] + dilation[0]*(kernel_size[0]-1) + 1)\n",
    "        p_w = ((h_w_out[1] - 1)*stride[1] - h_w_in[1] + dilation[1]*(kernel_size[1]-1) + 1)\n",
    "        \n",
    "        return (math.floor(p_h/2), math.ceil(p_h/2)), (math.floor(p_w/2), math.ceil(p_w/2))  #((pad_up, pad_bottom)， (pad_left, pad_right))\n",
    "    \n",
    "    def convtransp2d_get_padding(self,h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1, out_pad=0):\n",
    "        import math\n",
    "        '''\n",
    "        function - convtransp2d_output_shape 方法的逆，求填充pad\n",
    "        '''\n",
    "        h_w_in, h_w_out, kernel_size, stride, dilation, out_pad = self.num2tuple(h_w_in), self.num2tuple(h_w_out), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(dilation), self.num2tuple(out_pad)\n",
    "            \n",
    "        p_h = -(h_w_out[0] - 1 - out_pad[0] - dilation[0]*(kernel_size[0]-1) - (h_w_in[0] - 1)*stride[0]) / 2\n",
    "        p_w = -(h_w_out[1] - 1 - out_pad[1] - dilation[1]*(kernel_size[1]-1) - (h_w_in[1] - 1)*stride[1]) / 2\n",
    "        \n",
    "        return (math.floor(p_h/2), math.ceil(p_h/2)), (math.floor(p_w/2), math.ceil(p_w/2))\n",
    "    \n",
    "    def pooling_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "        '''\n",
    "        function - pooling池化层输出尺寸，同conv2d_output_shape\n",
    "        '''\n",
    "        return self.conv2d_output_shape(h_w, kernel_size=kernel_size, stride=stride, pad=pad, dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_size=(24, 24)\n",
      "size_pool1=(12, 12)\n",
      "size_conv2=(8, 8)\n",
      "size_pool2=(4, 4)\n"
     ]
    }
   ],
   "source": [
    "conv2dSize_cal=conv2d_output_size_calculation()\n",
    "size_conv1=conv2dSize_cal.conv2d_output_shape(28, kernel_size=5, stride=1, pad=0, dilation=1)\n",
    "size_pool1=conv2dSize_cal.pooling_output_shape(24, kernel_size=2, stride=2, pad=0, dilation=1)\n",
    "size_conv2=conv2dSize_cal.conv2d_output_shape(12, kernel_size=5, stride=1, pad=0, dilation=1)\n",
    "size_pool2=conv2dSize_cal.pooling_output_shape(8, kernel_size=2, stride=2, pad=0, dilation=1)\n",
    "\n",
    "print(\"conv1_size={}\\nsize_pool1={}\\nsize_conv2={}\\nsize_pool2={}\".format(size_conv1,size_pool1,size_conv2,size_pool2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在设计卷积层，或者包含很多卷积层时，上述的方法可以进一步改进，定义一个函数可以一次性计算所有的卷积层。在输入参数设置时，使用了列表和元组的形式，固定输入参数'input'，'conv'和'pool'，值的参数依次为[h_w, kernel_size, stride, pad, dilation]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "卷积层输出尺寸=(4, 4)\n"
     ]
    }
   ],
   "source": [
    "convs_params=[\n",
    "            ('input',(28,28)),\n",
    "            ('conv',[5,1,0,1]),  #h_w, kernel_size, stride, pad, dilation\n",
    "            ('pool',[2,2,0,1]),\n",
    "            ('conv',[5,1,0,1]),\n",
    "            ('pool',[2,2,0,1]),    \n",
    "            ]\n",
    "\n",
    "def conv2d_outputSize_A_oneTime(convs_params):\n",
    "    '''\n",
    "    fucntion - 一次性计算卷积输出尺寸\n",
    "    '''\n",
    "    conv2dSize_cal=conv2d_output_size_calculation()\n",
    "    for v in convs_params:\n",
    "        if v[0]=='input':\n",
    "            h_w=v[1]\n",
    "        elif v[0]=='conv' or v[0]=='pool':            \n",
    "            kernel_size, stride, pad, dilation=v[1]\n",
    "            h_w, kernel_size, stride, pad, dilation=conv2dSize_cal.num2tuple(h_w),conv2dSize_cal.num2tuple(kernel_size), conv2dSize_cal.num2tuple(stride),conv2dSize_cal.num2tuple(pad),conv2dSize_cal.num2tuple(dilation)\n",
    "            h_w=conv2dSize_cal.conv2d_output_shape(h_w, kernel_size, stride, pad, dilation)        \n",
    "    return h_w\n",
    " \n",
    "output_h_w=conv2d_outputSize_A_oneTime(convs_params)    \n",
    "print(\"卷积层输出尺寸={}\".format(output_h_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 构建简单的卷积神经网络识别fashionMNIST数据集，与tensorboard\n",
    "\n",
    "此处构建上图给出的神经网络结构，input-->conv1(relu)-->pool-->conv2(relu)-->pool--->fc1(relu)-->fc2(relu)-->fc3-->output，同时应用tensorboard可以写入并自动的根据写入的内容图示卷积神经网络结构，损失曲线，预测结果，样本信息，以及自定义的内容等。详细内容可以查看[torch.utils.tensorboard](https://pytorch.org/docs/stable/tensorboard.html)。\n",
    "\n",
    "> 参考：[Visualizing Models, Data, and Training with TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html?highlight=fashion%20mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#01-下载/读取fashinMNIST数据，以及构建训练、测试可迭代对象。（如果已经下载，则直接读取）\n",
    "\n",
    "def load_fashionMNIST(root,batchsize=4,num_workers=2,resize=None,n_mean=0.5,n_std=0.5):\n",
    "    import torchvision\n",
    "    import torchvision.transforms as transforms\n",
    "    '''\n",
    "    function - 下载读取fashionMNIST数据集，并建立训练、测试可迭代数据集\n",
    "    '''\n",
    "    trans= [transforms.ToTensor(), #转换PIL图像或numpy.ndarray为tensor张量\n",
    "            transforms.Normalize((0.5,), (0.5,))] #torchvision.transforms.Normalize(mean, std, inplace=False)，用均值和标准差，标准化张量图像       \n",
    "    if resize:\n",
    "        trans.append(transforms.Resize(size=resize))  \n",
    "    transform=transforms.Compose(trans)\n",
    "    mnist_train=torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform) \n",
    "    mnist_test=torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "    #DataLoade-读取小批量\n",
    "    import torch.utils.data as data_utils\n",
    "    batch_size=batchsize\n",
    "    num_workers=num_workers\n",
    "    trainloader=data_utils.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader=data_utils.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return trainloader,testloader\n",
    "\n",
    "trainloader,testloader=load_fashionMNIST(root='./datasets/FashionMNIST_norm')\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_fashionMNIST(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#02-定义网络结构，特征提取层+分类器\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class net_fashionMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_fashionMNIST,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,6,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*4*4,120) #torch.nn.Linear(in_features: int, out_features: int, bias: bool = True)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,16*4*4)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "net_fashionMNIST_=net_fashionMNIST()\n",
    "print(net_fashionMNIST_)\n",
    "\n",
    "#03-定义损失函数核优化算法\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion=nn.CrossEntropyLoss() #定义损失函数\n",
    "optimizer=optim.SGD(net_fashionMNIST_.parameters(), lr=0.001, momentum=0.9) #定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#04-定义显示图像函数（一个batch批次）\n",
    "\n",
    "import torchvision\n",
    "# helper functions\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np    \n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "#05-调入初始化tensorboard.SummaryWriter（指定数据写入的保存路径），并写入训练图像与模型\n",
    "        \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer=SummaryWriter(r'./runs/fashion_mnist_experiment_1')\n",
    "\n",
    "#提取图像（数量为batch_size） get some random training images\n",
    "dataiter=iter(trainloader)\n",
    "images,labels=dataiter.next()\n",
    "#建立图像格网 create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "#显示图像 show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)\n",
    "\n",
    "writer.add_graph(net_fashionMNIST_, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在终端，切换到tensorboard数据保存文件夹runs所在的目录，执行`tensorboard --logdir=runs`后，通常提示在浏览器中打开`http://localhost:6006/`地址，可以查看`writer.add_image('four_fashion_mnist_images', img_grid)`写入的训练图像信息内容(其语法为：`add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')`)，如下：\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/22_07.png\" height='auto' width='1000' title=\"caDesign\"></a>\n",
    "\n",
    "`writer.add_graph(net_fashionMNIST_, images)`（其语法为：`add_graph(model, input_to_model=None, verbose=False)`）,写入的信息内容下，可以查看网络结构及运算的流程，这对模型的构建与调整有所帮助。\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/22_08.png\" height='auto' width='1000' title=\"caDesign\"></a>\n",
    "\n",
    "下述在训练过程中，`riter.add_scalar('training loss',running_loss / 1000,epoch * len(trainloader) + i)`(其语法为：`add_scalar(tag, scalar_value, global_step=None, walltime=None)`)，写入损失数据图表，如下：\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/22_10.png\" height='auto' width='1000' title=\"caDesign\"></a>\n",
    "\n",
    "writer.add_figure('predictions vs. actuals',plot_classes_preds(net_fashionMNIST_, inputs, labels),global_step=epoch * len(trainloader) + i)(其语法为：`add_figure(tag, figure, global_step=None, close=True, walltime=None)`)，写入`plot_classes_preds`返回图表，包括图像，预测值及其概率，如下：\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/22_09.png\" height='auto' width='1000' title=\"caDesign\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#06-定义将图像应用训练的网络（模型）预测及其概率待写入tensorboard文件的函数\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    function - 用训练的网络预测给定的一组图像，并计算相应的概率 Generates predictions and corresponding probabilities from a trainednetwork and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    '''\n",
    "    function - 用训练的网络预测给定的一组图像，并计算概率后，显示图像、预测值以及概率，和实际的标签 Generates matplotlib Figure using a trained network, along with images and labels from a batch, that shows the network's top prediction along with its probability, alongside the actual label, coloring this information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4949ca82542a4ccda6da26080b7f2ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0,running_loss=433.0562955379719\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#07-训练模型，同时向tensorboard文件写入相关信息\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "running_loss = 0.0\n",
    "epochs=1\n",
    "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net_fashionMNIST_(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()                  \n",
    "        if i % 1000==999:    # every 1000 mini-batches...\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',running_loss / 1000,epoch * len(trainloader) + i) #写入损失数值\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('predictions vs. actuals',plot_classes_preds(net_fashionMNIST_, inputs, labels),global_step=epoch * len(trainloader) + i)  #写入自定义函数  plot_classes_preds返回的图表，包括图像，预测值及其概率\n",
    "            loss_temp=running_loss\n",
    "            running_loss=0.0\n",
    "    \n",
    "    print(\"epoch={},running_loss={}\".format(epoch,loss_temp))     \n",
    "    loss_temp=0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 可视化卷积层/卷积核\n",
    "\n",
    "通常一个卷积层的输出通道（output channel）有6，16，32，64，256，等不确定的输出个数及更多的输出个数，即一个卷积层包含输出通道数目的filter/kernal过滤器/卷积核；而卷积核通过$3 \\times 3$，$5 \\times 5$，$7 \\times 7$，等不确定尺寸（通常为奇数）或者更大尺寸来提取图像的特征，不同的卷积核提取的图像特征不同，或者表述为不同的卷积核关注不同的特征提取，这类似于图像的关键点描述子（位于不同的尺度空间下），不过因为卷积核的多样性，卷积提取的特征更加丰富多样。通过一个卷积层的多个卷积核（尺度空间的水平向，表述各个像素值与各周边像素值的差异程度），及多个卷积层（尺度空间的纵深向，表述图像特征所在的（对应的）空间分辨率，例如遥感影像看清建筑轮廓的空间分辨率约为5-15m，看清行人的空间分辨率约为0.3-1m，而若要看清人的五官，空间分辨率率则约为0.01-0.05m，这与对象（特征）的尺寸有关）提取了大量的图像特征，将这些图像特征flatten展平，就构成了该图像的特征集合(feature maps)。\n",
    "\n",
    "下述定义的`conv_retriever`类用于取回卷积神经网络中所有卷积层及其权重值（卷积核），取回的卷积层可以直接输入图像数据计算该卷积。例如上述网络取回的卷积层有两个。函数`visualize_convFilte`则可以打印显示卷积核。函数`visualize_convLaye`则能打印显示指定数目的所有卷积图像结果。\n",
    "\n",
    "> 参考 [Visualizing Filters and Feature Maps in Convolutional Neural Networks using PyTorch](https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV: Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) ====> SHAPE: torch.Size([6, 1, 5, 5])\n",
      "CONV: Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) ====> SHAPE: torch.Size([16, 6, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "class conv_retriever:\n",
    "    '''\n",
    "    class - 取回卷积神经网络中所有卷积层及其权重\n",
    "    '''\n",
    "    def __init__(self,net):\n",
    "        self.net=net\n",
    "        self.model_weights=[] # we will save the conv layer weights in this list\n",
    "        self.conv_layers=[] # we will save the 49 conv layers in this list\n",
    "        self.model_children=list(net.children()) # get all the model children as list\n",
    "        self.counter=0\n",
    "        \n",
    "    def retriever(self):\n",
    "        import torch.nn as nn\n",
    "        for i in range(len(self.model_children)):\n",
    "            if type(self.model_children[i])==nn.Conv2d:                \n",
    "                self.counter+=1\n",
    "                self.model_weights.append(self.model_children[i].weight)\n",
    "                self.conv_layers.append(self.model_children[i])\n",
    "            elif type(self.model_children[i])==nn.Sequential:\n",
    "                for j in range(len(self.model_children[i])):\n",
    "                    for child in self.model_children[i][j].children():\n",
    "                        if type(child)==nn.Conv2d:\n",
    "                            self.counter+=1\n",
    "                            self.model_weights.append(child.weight)\n",
    "                            self.conv_layers.append(child)       \n",
    "         \n",
    "        \n",
    "conv_retriever_=conv_retriever(net_fashionMNIST_)\n",
    "conv_retriever_.retriever()\n",
    "#print(conv_retriever_.conv_layers) #conv_retriever_.model_weights\n",
    "for weight, conv in zip(conv_retriever_.model_weights, conv_retriever_.conv_layers):\n",
    "    print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积核显示中，像素越黑，值越小，趋于0；而像素越亮，值越大，趋于255.0。因此越白的像素，在卷积过程中对应位置图像的像素的权重越大，对特征影响越重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_convFilter(conv_layer,model_weight,output_name,figsize=(10,10)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    '''\n",
    "    function - 可视化卷积核 visualize the conv layer filters\n",
    "    '''\n",
    "    plt.figure(figsize=figsize)\n",
    "    kernel_size=conv_layer.kernel_size\n",
    "    for i,filter in enumerate(model_weight):\n",
    "        plt.subplot(kernel_size[0]+1,kernel_size[1]+1,i+1)\n",
    "        plt.imshow(filter[0,:,:].detach(),cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(r'./results/%s'%output_name)\n",
    "    plt.show()        \n",
    "        \n",
    "visualize_convFilter(conv_retriever_.conv_layers[0],conv_retriever_.model_weights[0],output_name='fashion_MNIST_filter.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积（层）结果的显示，往往可以观察到不同的特征，例如明显的黑色区域轮廓为商标标识，对象的轮廓也能够通过代表不同颜色的值区分开来等。这为观察不同的卷积核提取了图像哪些特征，为相关研究或者网络调试提供参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layer:0,layer.size=torch.Size([6, 24, 24])\n",
      "\n",
      "Saving layer 0 feature maps...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAABcCAYAAABneh+uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWNElEQVR4nO2dR48dRReGi4+cM5hggmwwlsFgwoIgQEiwhC0bL9jzr1iA8BKWyIgFIAtkokGATTIm55z5VtM8/XpOcZnpMaZ5nlWN7+3u6lPhls9b59QRf/zxRxMRERGZI//7pysgIiIisla40BEREZHZ4kJHREREZosLHREREZktLnRERERktrjQERERkdlyVO/Dffv2GXu+AjZs2HDEFPe54447tP8K2Llz5yT2f+CBB7T/Cti+ffuq7b9lyxZtvwL27NkzSd9fv3699l8B+/fvX7X99+7dq+1XwMaNG0vb69ERERGR2eJCR0RERGaLCx0RERGZLS50REREZLa40BEREZHZ4kJHREREZosLHREREZktLnRERERktrjQERERkdniQkdERERmiwsdERERmS0udERERGS2dA/1PNz4448/zzr79ddfh/I333yz7L+31tpJJ500lE844YShfOyxxw7lH3/8sXzmEUf8eU4Yn89yfu+/xtFHHz2UTz/99NFnxxxzzFB+4403hvLXX389lM8777zy3r///vtCdTjyyCMX+t5cYH/77bffhvIvv/wy+t5xxx03lE8++eShzP7L8bPoM7P/zx32Q84XtP2ZZ545uoZzD2Ebpe2reYTP+d//xv8//S/MPXxn9uPjjz9+2e+0NrbLe++995f3TdjH+b3s+717zA3alHNu2qCaH9iPF53bp+C/00IiIiLyn8OFjoiIiMyWw066ossrXVvff//9UP7hhx+G8s8//zyUP/3009E1F1988VCmxHL22WcP5ZS7+DfrQ5f13r17R9fQnbpx48Y2R+iqPOOMM4byaaedNpQ/+uij0TWnnnrqUL788suH8kMPPTSUKSnmNXSJsj9QemyttW3btv1l/f9tpCRRSadsF46R1sa2PfHEE4fyWWedNZRff/310TU//fTTsvWhDPnFF1+MPjuUbuhDQdqAchPlQL43bdraeIxQojrqqD+n3ZQaOcew/d9///2hnONl8+bNxVv8u+D7UpJqrbb5hRdeOJT3798/uoZ24Xyxe/fuocy5q7XxfMP6bNq0aSin3HjFFVe0OcP5hX23KrdWy6ns7zlXVXIXn8/f8Pyshx4dERERmS0udERERGS2/GPSFd1U3In97bffDmVG5uTfdC33dt7zb7opTznllGXv1VprX3311VCuols+/vjj0TVziXzoRVBR7mP7UdJI9z0jrbZu3TqU2WZ0y7c2lq4I5cGMcPk3R12x79Atn67casycc845QzllWNqF7v/169cP5c8++2x0TbbHEhw/b7/99uizXuTi4QZtTNm7kuxaG/dJyhicrzintDa2MeeuL7/8ciinDFXVgddfeumlo882bNhQ1vtwhHMy359Rajn30P6co9i/L7vsstE1zz777FC+5557lv33ntTLelKSzzb74IMP2r+RKoIqZSjKflWkVUrXlczOe3HstXawjLsE25u/NfkOPfToiIiIyGxxoSMiIiKzxYWOiIiIzJY13aND3e67774bfUYtmvo+9TzulWltrFPze9S8U7NjiCc1QWqzvcyW/IwhtbmPIfXGw418R+4nYJl7P9KW1EoZqpwh/eTcc88dyrQ59y9kqHKVOoB7RNatWze6JjX9w4Fq703+TU2c+n/ut+GYqfaZcB9Oa+N2Z5tRaz///PNH1zB8luOM7Z8htrln55+GturtW6Lmz7mil/KANmZoce7b4/4d3o/9IsdlZWNmD//888/L5xwusK9lSDDtzLbh/o20C/fn8TOWGWre2jgbMueeG264YSi/+eabo2tYB/Z9pinhHq3WWnv88cfb4UruXaS92C6cg3oh3Jx32HbZ96u9gfytyTHGNuJzeK9cR+R+ogo9OiIiIjJbXOiIiIjIbFmRdEV5o7Wx65RudIaLZdgk78Fr6JpKlywlLrrA6NbM7JivvPLKUGb4Ye85dA0z5JGu0CeeeGJ0Dd14999/fzuU0LVHeYd1z3eke5LX8N3TFcz7ZWboJTJjMd30lBhJylB8DvsN+0we0pehpWtJSnqsF8sMoc+wyJRTlqCbN9uMtqgklwx1pZ1uvPHGoUw3cbZZFcrb60+Hyn2fYey0I23HNsqUB5Q5OXbYdnkNpVbaq5fygLbnXEhZJ+dFjkW+D8spRxzKvp8HlbIf0q7sN/l7Qa666qqhzLbN57C/8veC8se+ffvKe1OiqjIeZ71ZH0okmY6BUthakvMOZahFshfn9yoZK2X2KuULtxhkRnz2a0rjVXb91sZ9qQo1P3DgwOhv/l5xjCZ6dERERGS2uNARERGR2dKVrugWpisp3ceMjqoyjaY7jG4zusA+/PDDoZzu8ao+dB8ndOUzI+Ytt9wylNN1T5caXaZ0k6V00ovcmgK69fLAO/5NFzcliLymyrJLl3Huvmd7MCKErvjsG/yMdfvkk0+Gci9qi3WjNHDzzTePvpeywdTQXukOrg60pKs5pStC9zvHT2YG53PZTrwm7UB3PiMFefBsymjMgF1lnM5/T5fyaqmiGPPfaQeOW47pzLTNfsiIv2uuuWYo8wDa1sb9ne3N/p5RJJTX+T3WJ+vGuZRtd8EFFyxbz9Zau+SSS9rU0H6cO3KupC3ZxylH53jh3F3NmyldVdFZnB+ybnzu7bffPpR37NgxlDOKh9nbeT1lOc5prU2fFZzzBstpxypDdJXJuLU68zrHVUZQUZaqfutzXPI5nMdou/zd5vvwHarx1tr4QOK77rqrVejRERERkdniQkdERERmS1e6YhIwuvjS3Ucq91O60HgPusooXaUrmPeme52usXSh0eXJAyaraJLWxhIVpS/ag9ERrdUHUa4GviNd12lLSoR0b/LdM8lYlQyL0kfKLbTfrl27hvKVV145lHnAZGtj1zLbqfc+dHvTLc/oknfffXd0Detz9913tymodv6ni5z9tIquSRmwirqiazgjcngP2uKtt94ayin3sg9RCmGfT/mAUgXfh/fOSJ9bb701X2VVUM6mizsPsGRCyupw2RzfdMXzMx6WmVIvxzfrxmienu0J5bJMvEh5ke+2ffv2oZw2eO2115Z9zmqgdMeomezHtBPlaP57/l5QiqC8x7mHc01rrV100UVDmRGf3FqQ8xXHKZ9DGSsjQa+77rpl782InoxsrKJJVwrrzXImA6WsVR2imX2Sf3Nsc27OqDL+dvB7nMNyvPB3iNdXslxr498O9oWePNmL6CN6dERERGS2uNARERGR2eJCR0RERGZLd48OtTXqgxleRy2TGmCVdbS1sc756quvDmXu10k97rbbbhvKVUZM1qW18T4H6ot8Zr4PdWjuD+B+kdRlU9ecAmb2ZDh72qWyOdusd3gg9ylUIYetjTVU2uydd94Zyrl3idmVaX/Wjfp7a+NQTurAjz322FDOQyQzi/MU8NnU+NP+VWbvyq55DccG93zkmLn33nuHMg+TpC2y/1eHhHIPRGYU5buy3tWesdYODsdeLVu2bBnK1Ot7oeLVYZ3VfqjWxv2YexZyXw9twvHGMP08hJhtzLrxXpkVnPvLrr322qHMeSz322WG2SngHgv2r2x3zksct9yHk3bhAaWcU9hOmW148+bNQ3nPnj1Dmf075wD+zfowG3LWjfu3OGc+88wzQzn3B7KdpoDvxHk697Rwrxjfg/0z34/zE+cXfq9Kl9HaeH7nvXKM8R7VobP5m8T3Y3/v9f3MyFyhR0dERERmiwsdERERmS1d6Youp8zKSOiWpauboZPpdqOri+5PutTTfXz11VcPZbrNeJAnXf9JlZUxMzTSFUw3M92IdK3nvaeiOvg0wwyr0EKGsabURpcxQ/opifAQyNZae/jhh4cy5RtKT5mZl+3MkF+6QOmCbW3chmxb9qd0oadNpoB1p10z7Dz79hKUkTIsl/2Kci3HQo45hj9TmqH0lHWrpExenxIl+wozkTJ8OFM/TBXSvwTHE/tKPpdzBO1Ft3i64jkuqkzgmfKAIdacF1mflKGqQzkZNp5yC930lNfZlzKFQ5VFejXk+FoiJdjqINXqMN7Wxu1EGYh2ybmHc/Sdd9657HMynJ99iOOKBz2nfML+QMmOfT/bLEPhV0t1CkHO4ZRxmF2e82RmEma/Zpn3zjB9So1sB9Yz0ySwv7LPcE7N96nSxHC8pmy46JYRPToiIiIyW1zoiIiIyGzpSld091UHarY2lh7oyu1lRqb7kjIUXVOMaGht7Dajm5m7/ZnlN+tG6OZM6YpuQEYU8Zl0Zbc2liKmgi5yykvpjqT9Kkki3cf8jNfQlZuRD08//fRQ5vtT6st25t90MzOba2YaZQRAlRU3paCnnnqqTQ3dr3TfplTAdqoOHMwxw/5C1yxlu4xGY/ZR9gH25ZTR6B6u3Lwp7fC9qyiyStqYiuuvv34oc3znQad0i3Oss40ya2slc7JPZTZfzlesG9uhdw3HAe2bWY35GaUcXs+DDFs72CZTQBtxDsyoPva3qn/lHMzIObYf753S1YMPPjiUmT2akbDZj/n7xT7AdqYk1VprL7744lCmDTjG8jkvv/xymxLOhyxnXassxWyTnCfZJ6tsyDk+ONdXWZt7cyLnN7Zx/o5R/mI0Fa/PLSKLRtvq0REREZHZ4kJHREREZktXuqKMUR3O2No4KoKuSF6Tu/XpjqKbiy64dG3RDc8kTXRL5k5uusMY3UI3ICN7sq50lVFKyCRddDNPBeUBurjTFUyXOdumigZqbdw2jJRiuzz66KOja+jeZJv3koPRVUn5hs/PdqZt+ZwDBw4MZcporR28638KaH9KaOxHrdWueI6flPTYNrQFv5fjbOfOnUOZySSrZGGt1dIK2zIjTwg/oxs825nS6hRQrqJ9sq7se3Sl899TamDyOfYvuugz8SXfne3fi8aj7VmuIuFaG/d9zlGMAMrx0mu/lcKoUs6peZgr2531oESSh9Py/Tm/MhHgjh07RtdQPnv++eeHMqVeysv5XM5DlFlSbqOMzzFPGSz7Og/VnQK+K+e1ahtGa+PfQL5D9i+2C/sxfwPTJqwPpSJK2Vk3bv9guTrAu7Xxbw/HFccl54XW+slAiR4dERERmS0udERERGS2uNARERGR2dLdo1PtSUkttMrkyH/PPToMQ6dOS503wzWffPLJoUzdmPpk7lFg9khqs9wjkRmYueeCmiQzkuaeC+qQU0E9kyGVeQgjbUn78d9z/wD3PTCUj3sTnnvuudE1u3fvHsq5r2mJDNWnJlxlns1st9z/8cILLwxl6tVpg9RupyAP71wi92/Q5tSq2cd6KRl6YejkkUceGcoMO2X/z3HGccI9C9y3khmYqfFX4aRJdXDfSmGfZP/IMNYqK3ivruyjzNbde4cq+3C1J7G1sY25V4JtnIcScl8O+0i1/2Wt4FjnnpicKzmOF60jxwWv5564nHs439Dm3OeRtuRzWJ9emgTWh2OMe0Fyrpk6Kzt/szhn5r5QjlO+a7X/r7X6AGLa96WXXhpdw72CTIvBcs7h7PtcE/D5ubeMY5G/u2yjPNS3tw+V6NERERGR2eJCR0RERGZL1wdK1xTdfZkdk24rXkMXbYaBMdMkXcZ01/ayoDLkjVJCZoKkm5ouxt5Bgfwe3f1VWGlrB8t5U7B3796hTLvmoWt0dVZu/gyxJXT/spyH5FE6ol3omszQREoI7EPVgaVZV4btb926ddnvtHaw23oKGO7Id+9lf65CxTMkmDC8n27xzAJKV2/VN1Iy4Lirwk4zczDbjGOY/SzfJ8fDaqHt6WLvHRBJNzbfNa+pDmGk7bMfU06t2jthv+Zc1pPIaHvOZbRv9v1eHVYKD75ku2cG+Epeo8TQG9+7du0ayjy0luXWxm3GvsF+mDJGdYgz2zZDwytphXNXSlW9sb0SWCc+tycbstyTrmh79imOo5xD+H5VyH72C/4OsS9QnsqULJTmqsNiU+7qSf1Ej46IiIjMFhc6IiIiMlu60hUjiXpZGelOoiuJLqd079G1vGnTpqFMFyMz4bY2jnpat27dUO65FQnd+KxzuoLpWqZbn3JXRt6sRdQPI63ors22qHa4LwptTrtk9lu6QflMuk17ETx0e/J7dGe2Vmf5ZLukrLkWmZFpf8oIvXdkH+tlh6UcwL60bdu2ZZ/f2ridqkMKU1ar5A+S/YmRWvyM9UwX8tTSFWVi2rd3ODDh3JMRWOwr7FN0sedz2PfZxvz3noSU91sipXbatcq6nLanFHPfffeVdfg7MBM75ezsK5xvqijPrG8lNzLLMTN/tza2LeclzkNpf0qenD9Jyl1sD/6usJ9kZvScJ1cLI++qA4NbWyz6Lm1S9anq4M7W6mz71daW1sayIccv7Zv9gnXlGOX3cttG/l2hR0dERERmiwsdERERmS0LR13R5ZTuviragf+ekR10U9EFR9d9ui8rWayqS2u1S507xlO6qt61J8X1pL2Vwnv25KkqEeKiyZQoo7Bd0oXZOyxuiXSnVm7HnnRIN3N1OGNG8a11EjVKT/nsql69tmDb8t6MdMsxQ5tVB+6mLWkXjp+VSH0cM2mDRQ/XW5QqyikloOpAy0UT1lXRiL05rpKhkkWiYvI51bjo9f1MrDoFHN+UJXpSzyLzc2vj+Yr35nzDqK/Wxr9F1b2zL/B7tBmfk23JubWaf3OuXzTyZ1GYqJe2yt+yqn/05LxqywGlKybry+8tKlFXtqetMhqvmkOqea81pSsRERERFzoiIiIyX1zoiIiIyGzpbmzgYVrU5noadXXIXS+UjOVeeDT14EqzTg2RWmEVIpw6Pf+u9OB8TnXo32pguCtDN/PZlf5P8hr+3cukSaosu5U+3NrYLose/Mi+UtWt185TQfvz3XN/UpVttLJxa+O9Cbw3+3+GHlf09uhU+ybY/9OWvB+vYVumnj61/atw4GSRvp9Umn8vA+si1/RYyb6SKhtyttfUB6omzFib/atK2dHbB8XPuBer19+5p6za47Tob0xvT1P1Dr29KZlZf7XcdNNNQ5n74nIvULUXpzcfV9ewHXNsV4cOL3pNNScumrWZZBsveqCqHh0RERGZLS50REREZLZ0pSuGuvZkqF6o8F/9e2u1i7bnHu+FnFX3rtxhPbdkzzVKFpUZ/g5VeGq6/Ko69mxe0ZNb2B7VvXtZUHv2I9W9p86++1cwpJXvvqh026svP6syi/fkxpXIJyup26LpCqaWrvLg4L/73NXKWGvJ1ONg6vDm1ur5PsP2KR1UWZ2zLfheTKHAMZYH2lahx720BouEHmdbVNsWer9/U89LPAGgl3m7knp60tUiv7X5nN7WkCV6462ad3rzW3XvXnqDHnp0REREZLa40BEREZHZ0pWuFs06KGtDLwKKVO67RTO4LspayHOHM72MvGvFf83GFVNnWpa/Ry86hlQyXE/KWCSbdU+eJ1OPl6puh2r8tzZ+J+eDadCjIyIiIrPFhY6IiIjMFhc6IiIiMltc6IiIiMhscaEjIiIis8WFjoiIiMwWFzoiIiIyW1zoiIiIyGxxoSMiIiKz5YhDfVCiiIiIyKFCj46IiIjMFhc6IiIiMltc6IiIiMhscaEjIiIis8WFjoiIiMwWFzoiIiIyW/4Pi9ItPrrn83AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layer:1,layer.size=torch.Size([16, 20, 20])\n",
      "\n",
      "Saving layer 1 feature maps...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_convLayer(imgs_batch,conv_layers,model_weights,num_show=6,figsize=(10,10)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    '''\n",
    "    function - 可视化所有卷积层（卷积结果）\n",
    "    '''\n",
    "    results=[conv_layers[0](imgs_batch)]\n",
    "    for i in range(1,len(conv_layers)):\n",
    "        results.append(conv_layers[i](results[-1]))\n",
    "    outputs=results\n",
    "    \n",
    "    for num_layer in range(len(outputs)):\n",
    "        plt.figure(figsize=figsize)\n",
    "        layer_viz=outputs[num_layer][0,:,:,:]\n",
    "        layer_viz=layer_viz.data\n",
    "        print(\"num_layer:{},layer.size={}\".format(num_layer,layer_viz.size()))\n",
    "        \n",
    "        print()\n",
    "        kernel_size=conv_layers[num_layer].kernel_size\n",
    "        for i,filter in enumerate(layer_viz):\n",
    "            if i==num_show:\n",
    "                break\n",
    "            \n",
    "            plt.subplot(kernel_size[0]+1,kernel_size[1]+1,i+1)\n",
    "            plt.imshow(filter,cmap='gray')\n",
    "            plt.axis('off')\n",
    "        print(f\"Saving layer {num_layer} feature maps...\")\n",
    "        plt.savefig(f'./results/layer_{num_layer}.png')\n",
    "        plt.show()\n",
    "        #plt.close() #如果只保存，不需要显示打印，则可以开启plt.close()，并注释掉plt.show()\n",
    "            \n",
    "    \n",
    "    \n",
    "visualize_convLayer(images,conv_retriever_.conv_layers,conv_retriever_.model_weights,num_show=6)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html)\n",
    "\n",
    "`torchvision.models`库包含有解决不同任务的预先模型定义(通过配置参数pretrained=True，可以下载已经训练模型的参数)，包括：image classification图像分类、pixelwise sementic segmentation像素语义分割、object detection对象检测、instance segmentation实例分割、person keypoint detection 人关键点检测和video classification视频分类等。目前包括的模型如下：\n",
    "\n",
    "| 用途      | 模型/网络 |\n",
    "| ----------- | ----------- |\n",
    "| Classification      | AlexNet, VGG,ResNet, SqueezeNet, DenseNet, Inception v3, GoogLeNet,ShuffleNet v2,MobileNet v2,ResNeXt, Wide ResNet, MNASNet|\n",
    "| Semantic Segmentation   | FCN ResNet50, ResNet101; DeepLabV3 ResNet50, ResNet101       |\n",
    "| Object Detection, Instance Segmentation and Person Keypoint Detection|Faster R-CNN ResNet-50 FPN ,Mask R-CNN ResNet-50 FPN|\n",
    "| Video classification|ResNet 3D,  ResNet Mixed Convolution,ResNet (2+1)D|\n",
    "\n",
    "#### 1.3.1 自定义VGG网络\n",
    "VGG作者研究了在大规模图像识别设置中，卷积深度网络对其精度的影响。其贡献是使用非常小的卷积核($3 \\times 3$)滤波器的架构对深度增加的网络进行全面的评估，表面深度推进到16-19个权重层，可以实现对现有配置的显著改进。同时该模型可以很好的泛化到其它数据集。VGG的网络结构可以看作不断重复的模块，因此在定义模型时可以先定义规律性的模块，然后给定配置的卷积层数(num_convs)，输入通道数(in_channels)和输出通道数(out_channels)，调用模块实现完了的架构，避免代码冗长。其中包括5个卷积模块（block），前2块为单层卷积，后3层为双层卷积。该网络总共8个卷积层和3个全连接层，所以也称为VGG-11。\n",
    "\n",
    "> 参考文献：\n",
    "```\n",
    "1. @misc{simonyan2015deep,\n",
    "      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, \n",
    "      author={Karen Simonyan and Andrew Zisserman},\n",
    "      year={2015},\n",
    "      eprint={1409.1556},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CV}\n",
    "}\n",
    "```\n",
    "\n",
    "> 注： * 收集元组中所有位置参数 ** 收集字典中的所有关键字参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*s_position=('a', 'b', 'c', 'd', 'e')\n",
      "**d_keywords={'param_a': 1, 'param_b': 2, 'param_c': 3}\n"
     ]
    }
   ],
   "source": [
    "def func_asterisk(*s_position,**d_keywords):\n",
    "    print(\"*s_position={}\\n**d_keywords={}\".format(s_position,d_keywords))\n",
    "\n",
    "func_asterisk('a','b','c','d','e',param_a=1,param_b=2,param_c=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG网络结构：\n",
      " Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): flattenLayer()\n",
      "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author:《动手学深度学习》/'Dive into PyTorch'\n",
    "Updated on Thu Jan 14 17:45:17 2021 @author: Richie Bao-caDesign设计(cadesign.cn)\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch  import nn,optim\n",
    "import util\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def vgg_block(num_convs,in_channels,out_channels):\n",
    "    blk=[]\n",
    "    for i in range(num_convs):\n",
    "        if i==0:\n",
    "            blk.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1))\n",
    "        else:\n",
    "            blk.append(nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1))\n",
    "        blk.append(nn.ReLU())\n",
    "    blk.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*blk)  \n",
    "\n",
    "conv_arch=((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\n",
    "fc_features=512*7*7\n",
    "fc_hidden_units=4096\n",
    "\n",
    "def vgg(conv_arch,fc_features,fc_hidden_units=4096):\n",
    "    net=nn.Sequential()\n",
    "    for i,(num_convs,in_channels,out_channels) in enumerate(conv_arch):\n",
    "        net.add_module('vgg_block_'+str(i+1),vgg_block(num_convs,in_channels,out_channels))\n",
    "    net.add_module('fc',nn.Sequential(util.flattenLayer(),\n",
    "                                      nn.Linear(fc_features,fc_hidden_units),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(0.5),\n",
    "                                      nn.Linear(fc_hidden_units,fc_hidden_units),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(0.5),\n",
    "                                      nn.Linear(fc_hidden_units,10)        \n",
    "                                     ))\n",
    "    return net\n",
    "\n",
    "VGG_net=vgg(conv_arch,fc_features,fc_hidden_units)\n",
    "print(\"VGG网络结构：\\n\",VGG_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置完卷积层（特征提取层），可以通过上述定义的`conv2d_outputSize_A_oneTime`函数，计算全连接层的输入尺寸。计算结果为(7,7)，将其与卷积层最后一层的输出通道数相乘就为全连接层的输入尺寸$512 \\times 7 \\times 7=25088$，这与VGG的输入值相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "卷积层输出尺寸=(7, 7)\n"
     ]
    }
   ],
   "source": [
    "VGG_convs_params=[\n",
    "            ('input',(224,224)),\n",
    "            ('conv',[3,1,1,1]),  #kernel_size, stride, pad, dilation\n",
    "            ('pool',[2,2,0,1]),\n",
    "    \n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('pool',[2,2,0,1]),\n",
    "    \n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('pool',[2,2,0,1]),  \n",
    "    \n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('pool',[2,2,0,1]),  \n",
    "    \n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('pool',[2,2,0,1])     \n",
    "            ]\n",
    "\n",
    "VGG_output_h_w=conv2d_outputSize_A_oneTime(VGG_convs_params)    \n",
    "print(\"卷积层输出尺寸={}\".format(VGG_output_h_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以先生成随机的样本数据(batchsize, nChannels, Height, Width)，逐个循环每一模块，即Sequential对象，获取每一模块计算后数据的形状，从而观察、验证并用于辅助调整模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_block_1 output shape:  torch.Size([1, 64, 112, 112])\n",
      "vgg_block_2 output shape:  torch.Size([1, 128, 56, 56])\n",
      "vgg_block_3 output shape:  torch.Size([1, 256, 28, 28])\n",
      "vgg_block_4 output shape:  torch.Size([1, 512, 14, 14])\n",
      "vgg_block_5 output shape:  torch.Size([1, 512, 7, 7])\n",
      "fc output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X=torch.rand(1, 1, 224, 224)\n",
    "for name, blk in VGG_net.named_children(): \n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG的原始输入图像尺寸为224,目前实验的数据为fashionMNIST数据集，一幅图像的尺寸为$28 \\times 28$，因此使用`torchvision.transforms.Resize`的方法调整图像大小，该方法已经包含于自定义`load_fashionMNIST`中，因此只需要配置输入参数`resize=224`就可以修改图像尺寸，满足VGG输入数据尺寸的要求。\n",
    "\n",
    "因为卷积后的图像输出尺寸只与图像的输入尺寸，卷积层、池化层的卷积核大小，填充、步幅等有关，因此可以自由的修改卷积层的输入、输出通道，以及全连接层隐含层的数量。在修改时只需要配置一个缩放比例参数`ratio`，将所有的相关值除以该值完成新的配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG网络结构_减少通道数：\n",
      " Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): flattenLayer()\n",
      "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ratio = 8\n",
    "small_conv_arch = [(1, 1, 64//ratio), (1, 64//ratio, 128//ratio), (2, 128//ratio, 256//ratio), (2, 256//ratio, 512//ratio), (2, 512//ratio, 512//ratio)]\n",
    "VGG_net_= vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)\n",
    "print(\"VGG网络结构_减少通道数：\\n\",VGG_net_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据形状： torch.Size([64, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "trainloader,testloader=load_fashionMNIST(root='./datasets/FashionMNIST_norm',batchsize=batch_size,num_workers=2,resize=224,n_mean=0.5,n_std=0.5)\n",
    "\n",
    "def dataiter_view(dataiter):\n",
    "    '''\n",
    "    function - 查看可迭代数据形状\n",
    "    '''\n",
    "    dataiter_=iter(dataiter)\n",
    "    images_,labels_=dataiter_.next()\n",
    "    print('数据形状：',images_.shape)\n",
    "    return images_,labels_\n",
    "images_,labels_=dataiter_view(testloader)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr,num_epochs=0.001,5\n",
    "optimizer=torch.optim.Adam(VGG_net_.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_V2(data_iter, net, device=None):\n",
    "    '''\n",
    "    function - 模型精度计算\n",
    "    '''    \n",
    "    if device is None and isinstance(net,torch.nn.Module):\n",
    "        device=list(net.parameters())[0].device #如果没指定device就使用net的device\n",
    "    acc_sum,n=0.0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_iter:\n",
    "            if isinstance(net,torch.nn.Module):\n",
    "                net.eval() #评估模式，会关闭dropout\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train() #改回训练模式\n",
    "            n+=y.shape[0]\n",
    "    return acc_sum/n\n",
    "    \n",
    "def train_v2(net, train_iter, test_iter,optimizer, device, num_epochs):\n",
    "    from tqdm.auto import tqdm\n",
    "    import time\n",
    "    '''\n",
    "    function - 训练模型，v2版\n",
    "    '''\n",
    "    net = net.to(device)\n",
    "    print(\"training on-\",device)\n",
    "    loss=torch.nn.CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for X,y in train_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_pred=net(X)\n",
    "            l=loss(y_pred,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_pred.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc=evaluate_accuracy_V2(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'% (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on- cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535c60f4dac2459b854cbb8290790e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.5758, train acc 0.786, test acc 0.879, time 46.5 sec\n",
      "epoch 2, loss 0.3177, train acc 0.885, test acc 0.895, time 45.0 sec\n",
      "epoch 3, loss 0.2715, train acc 0.901, test acc 0.908, time 44.8 sec\n",
      "epoch 4, loss 0.2381, train acc 0.912, test acc 0.906, time 45.1 sec\n",
      "epoch 5, loss 0.2131, train acc 0.922, test acc 0.917, time 45.2 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_v2(net=VGG_net_, train_iter=trainloader, test_iter=testloader,optimizer=optimizer, device=device, num_epochs=num_epochs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 torchvision.models实现VGG网络\n",
    "VGG网络包含于`torchvision.models`库中， 因此无需自行配置网络，直接下载使用。通常模型也包含预先训练的模型参数，可以配置`pretrained=True`下载，下载到本地的位置为`C:\\Users\\<your name>\\.cache\\torch\\hub\\checkpoints`，后缀名为.pth。有时直接下载的网络并不能直接应用到其它不同的数据集，例如fashionMNIST数据集，该数据集的图像为灰色，即只有一个通道；同时，只有10个标签。因此需要对应层修改输入、输出大小。在[Finetuning Torchvision Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)中，包含了模型参数微调的方法，对于VGG而言，修改卷积层，可以通过`.features[idx]`的方式读取；修改全连接层，可以通过`.classifier[idx]`的方式修改。需要修改的层为features[0]和classifier[6]这两个层。其它层不需修改。\n",
    "\n",
    "注意在训练模型之前，需要调整优化函数`optimizer=torch.optim.Adam(VGG_model.parameters(),lr=lr)`的输入模型参数为当前的网络模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "VGG_model=torchvision.models.vgg11(pretrained=False) #如果为真，返回在ImageNet上预先训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(VGG_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "VGG_model.features[0]=nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "VGG_model.classifier[6]=nn.Linear(in_features=4096, out_features=10, bias=True)\n",
    "print(VGG_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on- cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5139009eba443f9a354109412113e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.5578, train acc 0.817, test acc 0.886, time 418.3 sec\n",
      "epoch 2, loss 0.2851, train acc 0.895, test acc 0.894, time 423.2 sec\n",
      "epoch 3, loss 0.2467, train acc 0.909, test acc 0.908, time 422.6 sec\n",
      "epoch 4, loss 0.2115, train acc 0.923, test acc 0.917, time 422.1 sec\n",
      "epoch 5, loss 0.1929, train acc 0.931, test acc 0.924, time 421.0 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr,num_epochs=0.001,5\n",
    "optimizer=torch.optim.Adam(VGG_model.parameters(),lr=lr)\n",
    "train_v2(net=VGG_model, train_iter=trainloader, test_iter=testloader,optimizer=optimizer, device=device, num_epochs=num_epochs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 要点\n",
    "#### 1.5.1 数据处理技术\n",
    "\n",
    "* 卷积层，池化层输出尺寸（形状）计算\n",
    "\n",
    "* tensorboard图表化深度学习参数，辅助调参\n",
    "\n",
    "* 可视化卷积层/卷积核\n",
    "\n",
    "* \\* 收集元组中所有位置参数 ** 收集字典中的所有关键字参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2 新建立的函数\n",
    "\n",
    "* class - PyTorch 卷积层，池化层输出尺寸(shape)计算，及根据输入，输出尺寸(shape)反推pad填充大小. `conv2d_output_size_calculation`\n",
    "\n",
    "包括：\n",
    "\n",
    "* function - 如果num=2，则返回(2,2)；如果num=(2,2)，则返回(2,2). `num2tuple(self,num)`\n",
    "\n",
    "* funciton - 计算PyTorch的nn.Conv2d卷积方法的输出尺寸。以卷积核左上角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算. `conv2d_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1)`\n",
    "\n",
    "* function - 以卷积核右下角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算. `convtransp2d_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1, out_pad=0)`\n",
    "\n",
    "* function - conv2d_output_shape 方法的逆，求填充pad. `conv2d_get_padding(self,h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1)`\n",
    "\n",
    "* function - convtransp2d_output_shape 方法的逆，求填充pad. `convtransp2d_get_padding(self,h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1, out_pad=0)`\n",
    "\n",
    "* function - pooling池化层输出尺寸，同conv2d_output_shape. ` pooling_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1)`\n",
    "\n",
    "---\n",
    "\n",
    "* fucntion - 一次性计算卷积输出尺寸. `conv2d_outputSize_A_oneTime(convs_params)`\n",
    "\n",
    "* function - 下载读取fashionMNIST数据集，并建立训练、测试可迭代数据集. `load_fashionMNIST(root,batchsize=4,num_workers=2,resize=None,n_mean=0.5,n_std=0.5)`\n",
    "\n",
    "* 自定义net_fashionMNIST(nn.Module)网络. `net_fashionMNIST(nn.Module)`\n",
    "\n",
    "* 定义显示图像函数（一个batch批次）. `matplotlib_imshow(img, one_channel=False)`\n",
    "\n",
    "* function - 用训练的网络预测给定的一组图像，并计算相应的概率. `images_to_probs(net, images)`\n",
    "\n",
    "* function - 用训练的网络预测给定的一组图像，并计算概率后，显示图像、预测值以及概率，和实际的标签. `plot_classes_preds(net, images, labels)`\n",
    "\n",
    "* class - 取回卷积神经网络中所有卷积层及其权重. `class conv_retriever`\n",
    "\n",
    "* function - 可视化卷积核 visualize the conv layer filters. `visualize_convFilter(conv_layer,model_weight,output_name,figsize=(10,10))`\n",
    "\n",
    "* function - 可视化所有卷积层（卷积结果）. `visualize_convLayer(imgs_batch,conv_layers,model_weights,num_show=6,figsize=(10,10))`\n",
    "\n",
    "* function - 自定义VGG. `vgg_block(num_convs,in_channels,out_channels)`;`vgg(conv_arch,fc_features,fc_hidden_units=4096)`\n",
    "\n",
    "* function - 查看可迭代数据形状. `dataiter_view(dataiter)`\n",
    "\n",
    "* function - 模型精度计算. `evaluate_accuracy_V2(data_iter, net, device=None)`\n",
    "\n",
    "* function - 训练模型，v2版. `train_v2(net, train_iter, test_iter,optimizer, device, num_epochs)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.3 所调用的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.4 参考文献\n",
    "1. Jibin Mathew.PyTorch Artificial Intelligence Fundamentals: A recipe-based approach to design, build and deploy your own AI models with PyTorch 1.x[m].UK:Packt Publishing (February 28, 2020)\n",
    "2. Aston Zhang,Zack C. Lipton,Mu Li,etc.Dive into Deep Learning[M].；中文版-阿斯顿.张,李沐,扎卡里.C. 立顿,亚历山大.J. 斯莫拉.动手深度学习[M].人民邮电出版社,北京,2019-06-01\n",
    "3. [Convolutional Neural Networks (CNNs / ConvNets)](https://cs231n.github.io/convolutional-networks/)\n",
    "4. [Understanding 2D Dilated Convolution Operation with Examples in Numpy and Tensorflow with Interactive Code](https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25)\n",
    "5. [Visualizing Models, Data, and Training with TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html?highlight=fashion%20mnist)\n",
    "6. [Visualizing Filters and Feature Maps in Convolutional Neural Networks using PyTorch](https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
